{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RTM \u00b6 The Greater Vancouver Regional Travel Model (RTM) is a four-step travel demand model developed and maintained by the South Coast British Columbia Transportation Authority (also known as TransLink). It provides long-range forecast and informs major transportation investments and policy decisions for the Metro Vancouver Regional District (MVRD). Introduction \u00b6 The RTM is a package of software that extends the functionalities of INRO EMME to model travel behaviors in the MVRD. The RTM software, written in python for EMME, handles all aspect of the four-step travel demand model: trip generation, trip distribution, mode choice, and network assignment. The RTM takes advantage of EMME's network editor and network assignment capability to capture user equilibrium network conditions. The latest version of the model is based on: 2011 Trip Diary and Screenline Survey ; Metro Vancouver Regional Growth Projections; and BC Provincial Digital Road Atlas . Get in touch \u00b6 If you require access to the RTM or would like to stay in touch, see Contributing .","title":"RTM"},{"location":"#rtm","text":"The Greater Vancouver Regional Travel Model (RTM) is a four-step travel demand model developed and maintained by the South Coast British Columbia Transportation Authority (also known as TransLink). It provides long-range forecast and informs major transportation investments and policy decisions for the Metro Vancouver Regional District (MVRD).","title":"RTM"},{"location":"#introduction","text":"The RTM is a package of software that extends the functionalities of INRO EMME to model travel behaviors in the MVRD. The RTM software, written in python for EMME, handles all aspect of the four-step travel demand model: trip generation, trip distribution, mode choice, and network assignment. The RTM takes advantage of EMME's network editor and network assignment capability to capture user equilibrium network conditions. The latest version of the model is based on: 2011 Trip Diary and Screenline Survey ; Metro Vancouver Regional Growth Projections; and BC Provincial Digital Road Atlas .","title":"Introduction"},{"location":"#get-in-touch","text":"If you require access to the RTM or would like to stay in touch, see Contributing .","title":"Get in touch"},{"location":"adv_inputs/","text":"Advanced Inputs \u00b6 Advanced inputs are enabled for most of mode choice in the RTM. This allows modelers to update the mode choice model, household segmentations for assignment, and update time slicing definitions. Model Structure \u00b6 The RTM is an advanced four step transportation model comprise of trip generation, trip distribution, mode choice, and trip assignment. Some elements within the RTM framework are programmable through advanced inputs using a set of yaml configuration files and input data. Components labeled in grey - trip attraction, trip production, generalized cost and accessibility calculations, trip assignment, and data export - cannot be modified using advanced inputs. If you need to reprogram those components of the RTM, proficiency with EMME toolbox and Python is required. Configurations \u00b6 Settings to advanced input modules can be modified via the input files inside the RTM/BaseNetworks/Inputs folder. The input folder content copied to the run, then it is loaded at the run level. This makes it possible to modify model specifications that are part of the advanced inputs at the run level for the RTM. Folder Structures \u00b6 RTM/ \u251c\u2500\u2500 BaseNetworks/ \u2502 \u2514\u2500\u2500 Inputs/ \u2502 \u251c\u2500\u2500 BestStop/ \u2502 \u251c\u2500\u2500 ModeChoice/ \u2502 \u2514\u2500\u2500 TimeSlicing/ \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 RTM.emp There are three subfolders relevant to the advanced input modules: BestStop , ModeChoice , and TimeSlicing . Under each subfolder, there are yaml files for major modules such as ComputeBestLot.yaml , Blended_Skim.yaml , TimeSlicing.yaml , UtilityByPurpose/HWRK.yaml , etc. The yaml files require input files in csv or binary data format depending on the procedures relevant in each module. Yaml Files \u00b6 The Yaml configuration file for RTM advanced inputs have three major sections: Files, Steps, and Functional Parameters. The Files section provides input and output files locations. The Steps section provides a list of function calls that are made in python. Finally, the Functional Parameters section provide a list of parameter values relevant to each function call. An Example of yaml file for best lot calculations can be found below: # Compute Best Lot Input Files FILES: INPUTS: COST: BestStop/ComputeBestLot_Cost.csv AVAILABLE_LOT: BestStop/ComputeBestLot_Lot.csv OUTPUTS: BEST_LOT: BestStop/BestLot.fea # Blended Skim Steps: Each Step is a Function STEPS: - get_best_lots get_best_lots: Target_Column: Target Available_Lot_Column: Available_Lot Costij_Column: Costij Costjk_Column: Costjk Note Adding additional steps will require changes be made in python. Changing input and output file values as well as functional parameter values can be done directly with the yaml file. Input Files \u00b6 Flat Files \u00b6 Flat files are used for specifying matrices used in calculations. The matrices specify can be input, output (or intermediate) matrices depending on the context of the flat file being used. For more information on a specific flat input file, please refer to the flat file in the context of the yaml file and relevant step in the model flow. Note Intermediate matrices are generally stored as binary files. It can be the output of an upstream module that becomes the input of a downstream module. Binary Files \u00b6 Binary files are often used for serialized input data or intermediate data storage. Binary files are not designed to be read by users, and they are typically cleaned up after a full model run. The current binary data format being used the the RTM is feather . Since the deserialization of binary file depends on the version of the data format implemented, there is no guarantee that these data files are backward or forward compatible; therefore, binary files are intended only for transient data and are not used for permanent data storage. Compute Best Lot \u00b6 The Compute Best Lot step generates matrices used in evaluating the optimal cost of intermodal trips. From a list of eligible zones (j), the compute best lot algorithm builds a list of total cost from i to j to k, then find the best cost for every i to k pair via the optimal j lot. This procedure requires the following inputs, as identified in ComputeBestLot.yaml : AVAILABLE_LOT (default: Inputs/BestStop/ComputeBestLot_Cost.csv ) - a list of intermediate lots available for each horizon year. Each lot type is delimited by comma which is then further delimited by semicolon, for example for year 2017, gn1;gn3,gn1;gn3,gn1;gn3 corresponds to PNR_Lot of gn1 and gn3 , KNR_Lot of gn1 and gn3 , TNCNR_Lot of gn1 and gn3 . Add new columns if additional lots are needed. COST (default: BestStop/ComputeBestLot_Lot.csv ) - specification of cost for matrix calculation. The cost is calculated by the optimizing the total cost of Costij_Column and Costjk_Column using a convolution algorithm. The minimum cost for an optimal lot is the result of the computation. Variable is assigned to new matrix name defined by Target_Column . The variables are specified by row. The lot type is defined by shared column name for Available_Lot_Column . A specific mode for each leg of the cost need to be specified also with Modeij and Modejk . Add more rows for additional cost that need to be computed. Blended Skim \u00b6 Skim blending further prepares data required for mode choice after best lot computation. Skim blending is the process of producing daily cost from peak hour scenarios, using a set of blending factor. The skim blending process first build daily cost from simple expressions (if applicable). Then, it adds all the peak hour components multiplied by corresponding factors. Finally, adjustments are made for intermodal variables requiring cost from best lot calculations. There are three required file inputs, which can be found in Blended_Skim.yaml : SPEC (default: ModeChoice/Blended_Skim.csv ) - path to csv file containing the list of skims (or expressions) associated with blending factors, segmentations, and associated trip purpose. COEFFICIENTS (default: ModeChoice/Blended_Skim_Coeffs.csv ) - path to csv file containing all blending factors that are referenced by the SPEC file. Typically, the blending factors are time of day and production-attraction (PA) specific. BEST_LOT (default: BestStop/BestLot.fea , leave as default ) - the intermediate binary data file output from the compute best lot calculation. In addition to file inputs that provide the required data for skim blending, a structure of the blending procedure needs to be specifed in the YAML file under the Blending setting: Target_Column - the name of the maxtrix which the result of the skim blending calculation to be stored. Key_Column - the name of the factor that will be applied to perform skim blending. Seg_Column - the list of columns containing segmentation. Factors - outlines the structure of blending factor that corresponds to the data provided in COEFFICIENTS input. The columns in P-A or A-P will be multiplied by columns in Skims_Columns when calculating each skim blending variable. Skims_Columns - the list of columns with time of day scenario skims. Expresstion_Column - instead of Skim Columns by time period, an expression may be specified for all time periods. For example, np.transpose(df['prtrmt']) for transposed vector matrices of park and ride terminal time. Mode Choice By Purpose \u00b6 Trip Purposes \u00b6 The mode choice module computes the trips from production to attraction zones, and prepares the model data for time slicing, and subsequently, trip assignment. There is a total of nine trip purposes modeled, each have a separate mode choice model structure: Home-based Work - settings in ModeChoice/UtilityByPurpose/HWRK.yaml Home-based University - settings in ModeChoice/UtilityByPurpose/HUNI.yaml Home-based School - settings in ModeChoice/UtilityByPurpose/HSCH.yaml Home-based Escorting - settings in ModeChoice/UtilityByPurpose/HESC.yaml Home-based Shopping - settings in ModeChoice/UtilityByPurpose/HSHP.yaml Home-based Social - settings in ModeChoice/UtilityByPurpose/HSOC.yaml Home-based Personal Business - settings in ModeChoice/UtilityByPurpose/HPBS.yaml Non-home-based Work - settings in ModeChoice/UtilityByPurpose/NWRK.yaml Non-home-based Other - settings in ModeChoice/UtilityByPurpose/NOTH.yaml In order to compute trip demand by purpose and by mode, utility, probability, trip distribution, trip demand, and accessibility calculation were performed. These steps are outlined in detail in the following sub sections. Nhb Trip Generations \u00b6 Newer RTM releases use the attraction of home-based trips as production of non-home-based trips. As such, a custom trip generation needs to be performed at the beginning of each non-home-based mode, after all the home-based models are computed. This procedure is called NonHomeBased_TripGen and are appended to STEPS for NWRK.yaml and NOTH.yaml . Utility \u00b6 The utilities of available modes are based on mode availability and cost. The global mode availabilities are defined in AVAILCONDITION and AVAILCHECK . The AVAILCONDITION csv file defines all the availability tests performed to establish the availability conditions. There will often be more than 1 condition per each mode's availability. The AVAILCHECK csv file establish mode availability by segmentation. Availability parameters can vary across trip purposes and are specifried in ComputeUtil:Availability . Segmentations are often different across trip purpose and are specified in ComputeUtil:SEGMENTS . The mode choice utility calculation for unavailable modes are masked with a large negative value (-99999). For available choices, the utility of each sub-mode is calculated based on the mode choice utility functions, specified by a set of csvs: SPEC (i.e.: HWRK.csv ) - a list of cost with expression and coefficients, by default, these costs are grouped into variables and are further defined by COEFFICIENTS and SEGMENTS . COEFFICIENTS (i.e.: HWRK_Coeffs.csv ) - a list of coefficient variable and values. SEGMENTS (i.e.: HWRK_Segments.csv ) - segment specific values and coefficient for utility computation. MATRIXDATA (i.e.: HWRK_MatrixData.csv ) - a list of matrices by mode to be prepared for utility calculations. They should be specified if they are used as a variable, so they are ready to be used. Probability \u00b6 After utility calculations, the choice probability for each mode is calculated based on theta and nesting strcture defined in CalcProb:NESTS within each yaml file. The list of alternatives at the lowest nesting level must match the name of the modes in Util_Data_Mode_Group . Trip Distribution \u00b6 Trip distribution is performed based on balncing type and impedance function defined for the trip purpose. Depending on the tirp purpose, different balancing types such as two_dim_matrix_balancing and one_dim_matrix_balancing can be specified. The impedance is based on calculation with K-factor (Kij) and needs to be specified in the DISTRIBUTIONIMP csv file. The impedance function may vary by segmentation and by modes. Trip Demand \u00b6 Although the mode choice trips can have very detailed segmentation and sub-modes, for the purpose of network assignment, the trips need to be aggregated and separated into different networks, mainly auto network, transit network, and active network. The TNC or Taxi network may be available depending on the availability of those modes. The trip demand calculation varies by trip purpose and can be specifed in TripsByMode within each yaml file. Accessibility \u00b6 The accessibility calculation is a post-processing routine that produce logsum accessibility measures that can be useful for preliminary analysis of user benefits. The ComputeAccessibility:Logsum_Coeff for each trip purpose should correspond to the top level theta for that mode choice model. And the specific modes used in accessibility calculation can be specified within ComputeAccessibility:Accessibility_Modes . The group of modes specified here do not need to reflect mode choice nesting structure and are provided here for reporting purposes. Time Slicing \u00b6 Time slicing is required to split daily trips into multiple time of day so the trip demand results can be feed back into the model for network assignment. Specification for time slicing can be found in Input/TimeSlicing/TimeSlicing.yaml In order to perform time slicing, the production-attraction (PA) trips need to be split into multimodal legs and into specific assignment classes, as specified in the MULTIMODAL_LEGS csv. Using the best lot computation results, the multimodal trip travel times are built. If zero-occupancy vehicles are present, the trip matrices will be adjusted accordingly. For high-occupancy modes like HOV, transit, and TNCs, occupancy adjustments will be made. Finally, the time slicing factors then need to be applied to the adjusted daily trips. Load PA Trips \u00b6 The first step in time slicing, all the PA trip matrices are read and loaded into memory. Matrices are tracked by the mode choice module using a trips_dict data object internally. If certain matrices are missing from utility calculation (for example, it has not been entered into the calculation or didn't exist from previous runs), an error will be raised. Missing matrices must be addressed before time slicing can be completed. Sub Modes Splits \u00b6 Typically, demands for different mode of transport are calculated as part of mode choice; however, certain sub modes are not modeled explicitly in the mode choice model due to some underlying assumptions about the behavior of those sub modes. For example, for home-based escorting trips, we do not explicitly model SOV, HOV2 and HOV3 like we do for home-based work trips. This is due to the joint decision at the household level for escorting trip, which is not modeled in the RTM. So, we establish an assumption that the occupancy for escorting trips is similar to what we observed in our household travel survey. Multimodal Trip Split \u00b6 Trips using multiple modes are modeled as single multimodal trips throughout mode choice. In order to properly assign those trips onto the road and transit network, the multimodal trips need to be split into trip legs, with their appripriate intermediate transit or parking facility assigned. Definition for multimodal trips is specified as rows in a CSV file from the MULTIMODAL_LEGS setting under FILES/INPUTS . This step also requires BEST_LOT results to be specified as an input in the TimeSlicing.yaml . Assignment Modes \u00b6 Modes used by mode choice often occupany the same network and belong to the same assignment class (most notably the HOV modes). These matrix names of each of the mode choice demand are updated to include information regarding the mode's assignment class. This allows groups of matrices belonging to the same assignment class be retrieved during time slicing. Factors \u00b6 Time slicing factors and their corresponding ensembles (i.e.: ga, gb, gc ensembles) are read and processed. A min_val should be specified to fill in any missing value after merging the ensemble and factor tables. ZOV Mask \u00b6 If Connected and Autonomous Vehicle (CAVs) is enabled in the RTM, Zero Occupancy Vehicles (ZOVs) will need to be generated in the network for trips performed by the CAVs without any occupants. The RTM models the decision to park versus drive home the CAVs as a trade off between parking cost and driving cost. Users may specify the cost matrices and parking cost information under CalculateZOVMask in TimeSlicing.yaml . Occupancy \u00b6 In RTM, PA trips in trip generation, distribution and mode choice are modeled as person trips while assignment trips are modeled as vehicle trips for the auto network and person trips for the transit network. Therefore, occupancy is an important prerequisite to time slicing calculations. This step allows users to define scalar or matrices for occupancy and generate occupancy_dict that will be used in time slicing to process trips with HOV assignment class ( assign_mode ) and corresponding Mode Choice modes ( mc_mode ). Time Slice \u00b6 The TimeSlice function is the main step in producing time slicing results for assignment. The time slicing procedure first split the daily trips into time of day trips (i.e. AM, MD, PM) using time_of_day_factors . Then, it converts person trip to driver trip using occupancy by mode choice modes, which is then stored as matrices by demand segments. To generate the ZOV trips, ZOV mask by TAZs, which represent the propensity for the CAV to drive home instead of parking, is applied to the ZOV demand and are assigned to the driver segment. To get OD trips, the time slicing factors is applied to all PA trips to get OD demand by time of day, by trip purpose, by mode, and by assignment class. Finally, all trips are aggregated and generated by assignment class. If TNC is being modeled, it will be added as a separate assignment class. Segment disaggregation \u00b6 The RTM uses a paramterized naming convention throughout time slicing. To specify this, users may modify the segment_disaggregation definition under TimeSlice . To define a segment, the start and end character index, and disaggreation criteria ( char_start, char_end, disaggregation ) need to be entered. For example, assign_mode: 19, 22, True means the segment assign_mode need to be aggregated over the matrix name character index [19, 22) , 19th character to 21st character. Then, the segment is to remain disaggregated. If the disaggreation criteria is False , then the segment would become aggregated for time slicing. Users may also filter by segments. For example, mat_type: 0, 3, AMT, MDT, PMT will keep mat_type segment disaggregated but only process segment AMT, MDT, PMT (processed as a list). If daily trips or other time period trips are present, they will be ignored. For more detail on naming conventions related to segment_disaggregation , refer to the figure below. Demand Summary \u00b6 With the introduction of more flexible segment disaggregation, users now have control over how to aggregate demand matrices by adjusting settings in TimeSlice step ( segment_daily_demand , segment_tod_demand ) within TimeSlicing.yaml . The Demand Summary yaml allows user to have additional control over how the aggregated demand matrices are summarized, as well as different format of export. Multiple Demand Summary yaml can be implemented using DemandSummary_*.yaml file pattern. For example, user may add a project specific Demand Summary yaml like DemandSummary_prjectA.yaml in addition to DemandSummary_Default.yaml within the Inputs/TimeSlice input data folder. At model run time, all of the Demand Summary yaml will be executed. To specify custom Demand Summary, refer to default Demand Summary yaml and comments for instruction. Here are the fields you may modify to generate custom Demand Summary outputs: Data type: matrix or data_table Export format sql: save to trip_summaries.db csv: comma separated text file fea: Dataframe as feather emx: emme matrices \u2013 matrix only Group by: segmentations to keep, i.e.: [mat_type, purpose, assign_mode] Filter by: List of attribute values to include Geography: gy, TAZ, i or j \u2013 data_table only Post eval: col or df operations \u2013 data_table only Example of snippet of default Demand Summary yaml: Example of snippet of a project based Demand Summary yaml: Example Use Case: Export Daily Demands at the PA level \u00b6 By default, daily demand at the TAZ level from production to attraction zones is not being saved at the end of the model run. In order to create summary of this data, users can use the demand summary function to export this data to feather file: # Demand Summary (Time Slicing Sub Routine) - RTS Toolbox # - Sub routine is executed at the end of Time Slicing. # - If multiple DemandSummary_*.yaml exist, table names # must not be identical across yaml configurations. # - Please refer to TimeSlice-segment_disagg_demand # within TimeSlicing.yaml for available segments. STEPS: - DemandSummary DemandSummary: # steps to export demand summaries, method of summary is always sum of trips export_demand_steps: # name of step will be used as table or file name RTS_Demand: # result data type: # data_table - long data table with segments # matrix - named array/column with full OD type: matrix # data export format: # ['sql', # trip_summaries sqlite db # 'csv', # comma separated # 'fea', # feather file # 'emx'] # emme matrix output (matrix only) export_format: ['fea'] # group data by columns: # should match segment_disaggregation naming # variable not specified in group by will be summed # mat_type (for TOD/Daily) should always be included group_by: [mat_type, purpose, income, assign_mode, vot] # filter segment: filter_by: mat_type: ['DPA'] pr_dr: ['PR'] # directory output folder relative to db_xx output_directory: Outputs # export matrix addition spec, default False export_mat_spec: False # export every cycle, default False export_every_cycle: False You can save this example using this link: DemandSummary_RTS.yaml Note Please double check the /BaseNetworks/Inputs/TimeSlicing/TimeSlicing.yaml file to make sure the existing or new data summary files are not being included in FileCleanup function near the end of the yaml file.","title":"Advanced Inputs"},{"location":"adv_inputs/#advanced-inputs","text":"Advanced inputs are enabled for most of mode choice in the RTM. This allows modelers to update the mode choice model, household segmentations for assignment, and update time slicing definitions.","title":"Advanced Inputs"},{"location":"adv_inputs/#model-structure","text":"The RTM is an advanced four step transportation model comprise of trip generation, trip distribution, mode choice, and trip assignment. Some elements within the RTM framework are programmable through advanced inputs using a set of yaml configuration files and input data. Components labeled in grey - trip attraction, trip production, generalized cost and accessibility calculations, trip assignment, and data export - cannot be modified using advanced inputs. If you need to reprogram those components of the RTM, proficiency with EMME toolbox and Python is required.","title":"Model Structure"},{"location":"adv_inputs/#configurations","text":"Settings to advanced input modules can be modified via the input files inside the RTM/BaseNetworks/Inputs folder. The input folder content copied to the run, then it is loaded at the run level. This makes it possible to modify model specifications that are part of the advanced inputs at the run level for the RTM.","title":"Configurations"},{"location":"adv_inputs/#folder-structures","text":"RTM/ \u251c\u2500\u2500 BaseNetworks/ \u2502 \u2514\u2500\u2500 Inputs/ \u2502 \u251c\u2500\u2500 BestStop/ \u2502 \u251c\u2500\u2500 ModeChoice/ \u2502 \u2514\u2500\u2500 TimeSlicing/ \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 RTM.emp There are three subfolders relevant to the advanced input modules: BestStop , ModeChoice , and TimeSlicing . Under each subfolder, there are yaml files for major modules such as ComputeBestLot.yaml , Blended_Skim.yaml , TimeSlicing.yaml , UtilityByPurpose/HWRK.yaml , etc. The yaml files require input files in csv or binary data format depending on the procedures relevant in each module.","title":"Folder Structures"},{"location":"adv_inputs/#yaml-files","text":"The Yaml configuration file for RTM advanced inputs have three major sections: Files, Steps, and Functional Parameters. The Files section provides input and output files locations. The Steps section provides a list of function calls that are made in python. Finally, the Functional Parameters section provide a list of parameter values relevant to each function call. An Example of yaml file for best lot calculations can be found below: # Compute Best Lot Input Files FILES: INPUTS: COST: BestStop/ComputeBestLot_Cost.csv AVAILABLE_LOT: BestStop/ComputeBestLot_Lot.csv OUTPUTS: BEST_LOT: BestStop/BestLot.fea # Blended Skim Steps: Each Step is a Function STEPS: - get_best_lots get_best_lots: Target_Column: Target Available_Lot_Column: Available_Lot Costij_Column: Costij Costjk_Column: Costjk Note Adding additional steps will require changes be made in python. Changing input and output file values as well as functional parameter values can be done directly with the yaml file.","title":"Yaml Files"},{"location":"adv_inputs/#input-files","text":"","title":"Input Files"},{"location":"adv_inputs/#flat-files","text":"Flat files are used for specifying matrices used in calculations. The matrices specify can be input, output (or intermediate) matrices depending on the context of the flat file being used. For more information on a specific flat input file, please refer to the flat file in the context of the yaml file and relevant step in the model flow. Note Intermediate matrices are generally stored as binary files. It can be the output of an upstream module that becomes the input of a downstream module.","title":"Flat Files"},{"location":"adv_inputs/#binary-files","text":"Binary files are often used for serialized input data or intermediate data storage. Binary files are not designed to be read by users, and they are typically cleaned up after a full model run. The current binary data format being used the the RTM is feather . Since the deserialization of binary file depends on the version of the data format implemented, there is no guarantee that these data files are backward or forward compatible; therefore, binary files are intended only for transient data and are not used for permanent data storage.","title":"Binary Files"},{"location":"adv_inputs/#compute-best-lot","text":"The Compute Best Lot step generates matrices used in evaluating the optimal cost of intermodal trips. From a list of eligible zones (j), the compute best lot algorithm builds a list of total cost from i to j to k, then find the best cost for every i to k pair via the optimal j lot. This procedure requires the following inputs, as identified in ComputeBestLot.yaml : AVAILABLE_LOT (default: Inputs/BestStop/ComputeBestLot_Cost.csv ) - a list of intermediate lots available for each horizon year. Each lot type is delimited by comma which is then further delimited by semicolon, for example for year 2017, gn1;gn3,gn1;gn3,gn1;gn3 corresponds to PNR_Lot of gn1 and gn3 , KNR_Lot of gn1 and gn3 , TNCNR_Lot of gn1 and gn3 . Add new columns if additional lots are needed. COST (default: BestStop/ComputeBestLot_Lot.csv ) - specification of cost for matrix calculation. The cost is calculated by the optimizing the total cost of Costij_Column and Costjk_Column using a convolution algorithm. The minimum cost for an optimal lot is the result of the computation. Variable is assigned to new matrix name defined by Target_Column . The variables are specified by row. The lot type is defined by shared column name for Available_Lot_Column . A specific mode for each leg of the cost need to be specified also with Modeij and Modejk . Add more rows for additional cost that need to be computed.","title":"Compute Best Lot"},{"location":"adv_inputs/#blended-skim","text":"Skim blending further prepares data required for mode choice after best lot computation. Skim blending is the process of producing daily cost from peak hour scenarios, using a set of blending factor. The skim blending process first build daily cost from simple expressions (if applicable). Then, it adds all the peak hour components multiplied by corresponding factors. Finally, adjustments are made for intermodal variables requiring cost from best lot calculations. There are three required file inputs, which can be found in Blended_Skim.yaml : SPEC (default: ModeChoice/Blended_Skim.csv ) - path to csv file containing the list of skims (or expressions) associated with blending factors, segmentations, and associated trip purpose. COEFFICIENTS (default: ModeChoice/Blended_Skim_Coeffs.csv ) - path to csv file containing all blending factors that are referenced by the SPEC file. Typically, the blending factors are time of day and production-attraction (PA) specific. BEST_LOT (default: BestStop/BestLot.fea , leave as default ) - the intermediate binary data file output from the compute best lot calculation. In addition to file inputs that provide the required data for skim blending, a structure of the blending procedure needs to be specifed in the YAML file under the Blending setting: Target_Column - the name of the maxtrix which the result of the skim blending calculation to be stored. Key_Column - the name of the factor that will be applied to perform skim blending. Seg_Column - the list of columns containing segmentation. Factors - outlines the structure of blending factor that corresponds to the data provided in COEFFICIENTS input. The columns in P-A or A-P will be multiplied by columns in Skims_Columns when calculating each skim blending variable. Skims_Columns - the list of columns with time of day scenario skims. Expresstion_Column - instead of Skim Columns by time period, an expression may be specified for all time periods. For example, np.transpose(df['prtrmt']) for transposed vector matrices of park and ride terminal time.","title":"Blended Skim"},{"location":"adv_inputs/#mode-choice-by-purpose","text":"","title":"Mode Choice By Purpose"},{"location":"adv_inputs/#trip-purposes","text":"The mode choice module computes the trips from production to attraction zones, and prepares the model data for time slicing, and subsequently, trip assignment. There is a total of nine trip purposes modeled, each have a separate mode choice model structure: Home-based Work - settings in ModeChoice/UtilityByPurpose/HWRK.yaml Home-based University - settings in ModeChoice/UtilityByPurpose/HUNI.yaml Home-based School - settings in ModeChoice/UtilityByPurpose/HSCH.yaml Home-based Escorting - settings in ModeChoice/UtilityByPurpose/HESC.yaml Home-based Shopping - settings in ModeChoice/UtilityByPurpose/HSHP.yaml Home-based Social - settings in ModeChoice/UtilityByPurpose/HSOC.yaml Home-based Personal Business - settings in ModeChoice/UtilityByPurpose/HPBS.yaml Non-home-based Work - settings in ModeChoice/UtilityByPurpose/NWRK.yaml Non-home-based Other - settings in ModeChoice/UtilityByPurpose/NOTH.yaml In order to compute trip demand by purpose and by mode, utility, probability, trip distribution, trip demand, and accessibility calculation were performed. These steps are outlined in detail in the following sub sections.","title":"Trip Purposes"},{"location":"adv_inputs/#nhb-trip-generations","text":"Newer RTM releases use the attraction of home-based trips as production of non-home-based trips. As such, a custom trip generation needs to be performed at the beginning of each non-home-based mode, after all the home-based models are computed. This procedure is called NonHomeBased_TripGen and are appended to STEPS for NWRK.yaml and NOTH.yaml .","title":"Nhb Trip Generations"},{"location":"adv_inputs/#utility","text":"The utilities of available modes are based on mode availability and cost. The global mode availabilities are defined in AVAILCONDITION and AVAILCHECK . The AVAILCONDITION csv file defines all the availability tests performed to establish the availability conditions. There will often be more than 1 condition per each mode's availability. The AVAILCHECK csv file establish mode availability by segmentation. Availability parameters can vary across trip purposes and are specifried in ComputeUtil:Availability . Segmentations are often different across trip purpose and are specified in ComputeUtil:SEGMENTS . The mode choice utility calculation for unavailable modes are masked with a large negative value (-99999). For available choices, the utility of each sub-mode is calculated based on the mode choice utility functions, specified by a set of csvs: SPEC (i.e.: HWRK.csv ) - a list of cost with expression and coefficients, by default, these costs are grouped into variables and are further defined by COEFFICIENTS and SEGMENTS . COEFFICIENTS (i.e.: HWRK_Coeffs.csv ) - a list of coefficient variable and values. SEGMENTS (i.e.: HWRK_Segments.csv ) - segment specific values and coefficient for utility computation. MATRIXDATA (i.e.: HWRK_MatrixData.csv ) - a list of matrices by mode to be prepared for utility calculations. They should be specified if they are used as a variable, so they are ready to be used.","title":"Utility"},{"location":"adv_inputs/#probability","text":"After utility calculations, the choice probability for each mode is calculated based on theta and nesting strcture defined in CalcProb:NESTS within each yaml file. The list of alternatives at the lowest nesting level must match the name of the modes in Util_Data_Mode_Group .","title":"Probability"},{"location":"adv_inputs/#trip-distribution","text":"Trip distribution is performed based on balncing type and impedance function defined for the trip purpose. Depending on the tirp purpose, different balancing types such as two_dim_matrix_balancing and one_dim_matrix_balancing can be specified. The impedance is based on calculation with K-factor (Kij) and needs to be specified in the DISTRIBUTIONIMP csv file. The impedance function may vary by segmentation and by modes.","title":"Trip Distribution"},{"location":"adv_inputs/#trip-demand","text":"Although the mode choice trips can have very detailed segmentation and sub-modes, for the purpose of network assignment, the trips need to be aggregated and separated into different networks, mainly auto network, transit network, and active network. The TNC or Taxi network may be available depending on the availability of those modes. The trip demand calculation varies by trip purpose and can be specifed in TripsByMode within each yaml file.","title":"Trip Demand"},{"location":"adv_inputs/#accessibility","text":"The accessibility calculation is a post-processing routine that produce logsum accessibility measures that can be useful for preliminary analysis of user benefits. The ComputeAccessibility:Logsum_Coeff for each trip purpose should correspond to the top level theta for that mode choice model. And the specific modes used in accessibility calculation can be specified within ComputeAccessibility:Accessibility_Modes . The group of modes specified here do not need to reflect mode choice nesting structure and are provided here for reporting purposes.","title":"Accessibility"},{"location":"adv_inputs/#time-slicing","text":"Time slicing is required to split daily trips into multiple time of day so the trip demand results can be feed back into the model for network assignment. Specification for time slicing can be found in Input/TimeSlicing/TimeSlicing.yaml In order to perform time slicing, the production-attraction (PA) trips need to be split into multimodal legs and into specific assignment classes, as specified in the MULTIMODAL_LEGS csv. Using the best lot computation results, the multimodal trip travel times are built. If zero-occupancy vehicles are present, the trip matrices will be adjusted accordingly. For high-occupancy modes like HOV, transit, and TNCs, occupancy adjustments will be made. Finally, the time slicing factors then need to be applied to the adjusted daily trips.","title":"Time Slicing"},{"location":"adv_inputs/#load-pa-trips","text":"The first step in time slicing, all the PA trip matrices are read and loaded into memory. Matrices are tracked by the mode choice module using a trips_dict data object internally. If certain matrices are missing from utility calculation (for example, it has not been entered into the calculation or didn't exist from previous runs), an error will be raised. Missing matrices must be addressed before time slicing can be completed.","title":"Load PA Trips"},{"location":"adv_inputs/#sub-modes-splits","text":"Typically, demands for different mode of transport are calculated as part of mode choice; however, certain sub modes are not modeled explicitly in the mode choice model due to some underlying assumptions about the behavior of those sub modes. For example, for home-based escorting trips, we do not explicitly model SOV, HOV2 and HOV3 like we do for home-based work trips. This is due to the joint decision at the household level for escorting trip, which is not modeled in the RTM. So, we establish an assumption that the occupancy for escorting trips is similar to what we observed in our household travel survey.","title":"Sub Modes Splits"},{"location":"adv_inputs/#multimodal-trip-split","text":"Trips using multiple modes are modeled as single multimodal trips throughout mode choice. In order to properly assign those trips onto the road and transit network, the multimodal trips need to be split into trip legs, with their appripriate intermediate transit or parking facility assigned. Definition for multimodal trips is specified as rows in a CSV file from the MULTIMODAL_LEGS setting under FILES/INPUTS . This step also requires BEST_LOT results to be specified as an input in the TimeSlicing.yaml .","title":"Multimodal Trip Split"},{"location":"adv_inputs/#assignment-modes","text":"Modes used by mode choice often occupany the same network and belong to the same assignment class (most notably the HOV modes). These matrix names of each of the mode choice demand are updated to include information regarding the mode's assignment class. This allows groups of matrices belonging to the same assignment class be retrieved during time slicing.","title":"Assignment Modes"},{"location":"adv_inputs/#factors","text":"Time slicing factors and their corresponding ensembles (i.e.: ga, gb, gc ensembles) are read and processed. A min_val should be specified to fill in any missing value after merging the ensemble and factor tables.","title":"Factors"},{"location":"adv_inputs/#zov-mask","text":"If Connected and Autonomous Vehicle (CAVs) is enabled in the RTM, Zero Occupancy Vehicles (ZOVs) will need to be generated in the network for trips performed by the CAVs without any occupants. The RTM models the decision to park versus drive home the CAVs as a trade off between parking cost and driving cost. Users may specify the cost matrices and parking cost information under CalculateZOVMask in TimeSlicing.yaml .","title":"ZOV Mask"},{"location":"adv_inputs/#occupancy","text":"In RTM, PA trips in trip generation, distribution and mode choice are modeled as person trips while assignment trips are modeled as vehicle trips for the auto network and person trips for the transit network. Therefore, occupancy is an important prerequisite to time slicing calculations. This step allows users to define scalar or matrices for occupancy and generate occupancy_dict that will be used in time slicing to process trips with HOV assignment class ( assign_mode ) and corresponding Mode Choice modes ( mc_mode ).","title":"Occupancy"},{"location":"adv_inputs/#time-slice","text":"The TimeSlice function is the main step in producing time slicing results for assignment. The time slicing procedure first split the daily trips into time of day trips (i.e. AM, MD, PM) using time_of_day_factors . Then, it converts person trip to driver trip using occupancy by mode choice modes, which is then stored as matrices by demand segments. To generate the ZOV trips, ZOV mask by TAZs, which represent the propensity for the CAV to drive home instead of parking, is applied to the ZOV demand and are assigned to the driver segment. To get OD trips, the time slicing factors is applied to all PA trips to get OD demand by time of day, by trip purpose, by mode, and by assignment class. Finally, all trips are aggregated and generated by assignment class. If TNC is being modeled, it will be added as a separate assignment class.","title":"Time Slice"},{"location":"adv_inputs/#segment-disaggregation","text":"The RTM uses a paramterized naming convention throughout time slicing. To specify this, users may modify the segment_disaggregation definition under TimeSlice . To define a segment, the start and end character index, and disaggreation criteria ( char_start, char_end, disaggregation ) need to be entered. For example, assign_mode: 19, 22, True means the segment assign_mode need to be aggregated over the matrix name character index [19, 22) , 19th character to 21st character. Then, the segment is to remain disaggregated. If the disaggreation criteria is False , then the segment would become aggregated for time slicing. Users may also filter by segments. For example, mat_type: 0, 3, AMT, MDT, PMT will keep mat_type segment disaggregated but only process segment AMT, MDT, PMT (processed as a list). If daily trips or other time period trips are present, they will be ignored. For more detail on naming conventions related to segment_disaggregation , refer to the figure below.","title":"Segment disaggregation"},{"location":"adv_inputs/#demand-summary","text":"With the introduction of more flexible segment disaggregation, users now have control over how to aggregate demand matrices by adjusting settings in TimeSlice step ( segment_daily_demand , segment_tod_demand ) within TimeSlicing.yaml . The Demand Summary yaml allows user to have additional control over how the aggregated demand matrices are summarized, as well as different format of export. Multiple Demand Summary yaml can be implemented using DemandSummary_*.yaml file pattern. For example, user may add a project specific Demand Summary yaml like DemandSummary_prjectA.yaml in addition to DemandSummary_Default.yaml within the Inputs/TimeSlice input data folder. At model run time, all of the Demand Summary yaml will be executed. To specify custom Demand Summary, refer to default Demand Summary yaml and comments for instruction. Here are the fields you may modify to generate custom Demand Summary outputs: Data type: matrix or data_table Export format sql: save to trip_summaries.db csv: comma separated text file fea: Dataframe as feather emx: emme matrices \u2013 matrix only Group by: segmentations to keep, i.e.: [mat_type, purpose, assign_mode] Filter by: List of attribute values to include Geography: gy, TAZ, i or j \u2013 data_table only Post eval: col or df operations \u2013 data_table only Example of snippet of default Demand Summary yaml: Example of snippet of a project based Demand Summary yaml:","title":"Demand Summary"},{"location":"adv_inputs/#example-use-case-export-daily-demands-at-the-pa-level","text":"By default, daily demand at the TAZ level from production to attraction zones is not being saved at the end of the model run. In order to create summary of this data, users can use the demand summary function to export this data to feather file: # Demand Summary (Time Slicing Sub Routine) - RTS Toolbox # - Sub routine is executed at the end of Time Slicing. # - If multiple DemandSummary_*.yaml exist, table names # must not be identical across yaml configurations. # - Please refer to TimeSlice-segment_disagg_demand # within TimeSlicing.yaml for available segments. STEPS: - DemandSummary DemandSummary: # steps to export demand summaries, method of summary is always sum of trips export_demand_steps: # name of step will be used as table or file name RTS_Demand: # result data type: # data_table - long data table with segments # matrix - named array/column with full OD type: matrix # data export format: # ['sql', # trip_summaries sqlite db # 'csv', # comma separated # 'fea', # feather file # 'emx'] # emme matrix output (matrix only) export_format: ['fea'] # group data by columns: # should match segment_disaggregation naming # variable not specified in group by will be summed # mat_type (for TOD/Daily) should always be included group_by: [mat_type, purpose, income, assign_mode, vot] # filter segment: filter_by: mat_type: ['DPA'] pr_dr: ['PR'] # directory output folder relative to db_xx output_directory: Outputs # export matrix addition spec, default False export_mat_spec: False # export every cycle, default False export_every_cycle: False You can save this example using this link: DemandSummary_RTS.yaml Note Please double check the /BaseNetworks/Inputs/TimeSlicing/TimeSlicing.yaml file to make sure the existing or new data summary files are not being included in FileCleanup function near the end of the yaml file.","title":"Example Use Case: Export Daily Demands at the PA level"},{"location":"code_review/","text":"Code Review \u00b6 Code review is an important part of model development to ensure consistency of general coding and modeling practices. A thorough code review should be conducted when changes are introduced to the main codebase via pull requests. Code reviews are required whenever a new pull request has been created in the main RTM repository. Typically, a member of the RTM development team will review the requested code change and work with the contributor that created the pull request. Overall, this is the general procedure for code review: Create a pull request Conduct model testing Assign a reviewer Create a pull request \u00b6 Pull request is the recommended channel to introduce model changes and start the code review process. We encourage contributors to provides relevant information regarding the changes introduced, outline and assess the impact of the changes on model results and provide model testing and comparison results. This information allows the RTM development team to efficiently process pull requests. If you have never created a pull request before, please refer to GitHub Docs - Creating a pull request for more information. Conduct model testing \u00b6 Model testing is a critical part of the quality control and assurance process in maintaining the RTM. The model testing ensure new versions of RTM is free from software bugs and errors, and the impact of the changes on the RTM results is well understood. Model testing should be performed for all base years. These are the steps to perform model testing: Step 1 : Complete a set of reference model runs (for each base year) from a previous version of RTM, use the branch you are trying to pull your changes into as reference. Step 2 : Complete a set of updated model runs (for each base year) from the new version of RTM Step 3 : Run scenario comparison tool between the reference and updated model runs Step 4 : Package scenario comparison result excel files, and archive databank files (with input folder, rtm.db, and trip_summaries.db) Using validate_many.ipynb for model testing \u00b6 If you are familiar with using EMME notebook with Python, you may use the validate_many.ipynb notebook within the Scripts/Tools folder to conduct reference and updated model runs, perform scenario comparison, and generate archive databank files. Step 1 : After downloading a previous version of RTM, run validate_many.ipynb with the default config of the notebook, this will run all of the base year scenarios. The archive version of your databanks can be found in the Media/ScenarioComparison folder Step 2 : With the new version of RTM, open validate_many.ipynb , update the prev_version_databank_folder option for each of the scenario specified within params to point to the databank folders of each of the corresponding base year. Then run validate_many.ipynb . Note that the last 2 blocks of the code in the validate_many.ipynb notebook will package an archive version of your databanks, and generate scenario comparison result excel files. Everything you need to share with the reviewer will be with the Media/ScenarioComparison folder. Assign a reviewer \u00b6 Once you have all the relevant information included in your pull request and conducted the model testing procedure. Please reach out to your contact at the RTM development team via email. They will provide you an ETA and give you updates on your pull request. If additional changes are requested, you may need to conduct additional model testing.","title":"Code Review"},{"location":"code_review/#code-review","text":"Code review is an important part of model development to ensure consistency of general coding and modeling practices. A thorough code review should be conducted when changes are introduced to the main codebase via pull requests. Code reviews are required whenever a new pull request has been created in the main RTM repository. Typically, a member of the RTM development team will review the requested code change and work with the contributor that created the pull request. Overall, this is the general procedure for code review: Create a pull request Conduct model testing Assign a reviewer","title":"Code Review"},{"location":"code_review/#create-a-pull-request","text":"Pull request is the recommended channel to introduce model changes and start the code review process. We encourage contributors to provides relevant information regarding the changes introduced, outline and assess the impact of the changes on model results and provide model testing and comparison results. This information allows the RTM development team to efficiently process pull requests. If you have never created a pull request before, please refer to GitHub Docs - Creating a pull request for more information.","title":"Create a pull request"},{"location":"code_review/#conduct-model-testing","text":"Model testing is a critical part of the quality control and assurance process in maintaining the RTM. The model testing ensure new versions of RTM is free from software bugs and errors, and the impact of the changes on the RTM results is well understood. Model testing should be performed for all base years. These are the steps to perform model testing: Step 1 : Complete a set of reference model runs (for each base year) from a previous version of RTM, use the branch you are trying to pull your changes into as reference. Step 2 : Complete a set of updated model runs (for each base year) from the new version of RTM Step 3 : Run scenario comparison tool between the reference and updated model runs Step 4 : Package scenario comparison result excel files, and archive databank files (with input folder, rtm.db, and trip_summaries.db)","title":"Conduct model testing"},{"location":"code_review/#using-validate_manyipynb-for-model-testing","text":"If you are familiar with using EMME notebook with Python, you may use the validate_many.ipynb notebook within the Scripts/Tools folder to conduct reference and updated model runs, perform scenario comparison, and generate archive databank files. Step 1 : After downloading a previous version of RTM, run validate_many.ipynb with the default config of the notebook, this will run all of the base year scenarios. The archive version of your databanks can be found in the Media/ScenarioComparison folder Step 2 : With the new version of RTM, open validate_many.ipynb , update the prev_version_databank_folder option for each of the scenario specified within params to point to the databank folders of each of the corresponding base year. Then run validate_many.ipynb . Note that the last 2 blocks of the code in the validate_many.ipynb notebook will package an archive version of your databanks, and generate scenario comparison result excel files. Everything you need to share with the reviewer will be with the Media/ScenarioComparison folder.","title":"Using validate_many.ipynb for model testing"},{"location":"code_review/#assign-a-reviewer","text":"Once you have all the relevant information included in your pull request and conducted the model testing procedure. Please reach out to your contact at the RTM development team via email. They will provide you an ETA and give you updates on your pull request. If additional changes are requested, you may need to conduct additional model testing.","title":"Assign a reviewer"},{"location":"coding_standard/","text":"Coding Standard \u00b6 The coding standard for the RTM has two major components: guideline and compliance. Coding Standard Guideline spell out the best practices around the design of the RTM code. It is comprised of 3 elements: styles, function abstraction, data management. Following the guideline ensures the programming paradigm used by all developers of the RTM remain the consistent. Code Standard Compliance refers to the set of tools and procedures employed to ensure that the guideline has been met. The code review standards will be enforced through our code review process. General Guideline \u00b6 Styles \u00b6 Recommended Code Formator: Black from Python Software Foundation Compliant with the recent changes in the PEP 8 style guide For integration with your favourite IDE, see Editor Integration for Black Key requirements taken from the Python PEP8 Standard: Indentation : spaces are strongly preferred (4 spaces in place of 1 tab) Maximum Line Length : no less than 80 and no more than 100 characters, note that Black defaults to 88 Line Break : occur before binary operators Blank lines : surround top-level function and class definitions with two blank lines. Method definitions inside a class are surrounded by a single blank line Imports : separate lines of import starting with standard library, related third party, then local application imports Comments : block comments should explain following code and what and why they do or may be used to section the code Docstring (Documentation String) : docstring is required for all modules , functions , classes and methods . It should contain short description of what, how, and why the code exists. When applicable, describe the input and output types of the function. For more specific conventions on docstring, see PEP 257 You may use extension/tools to generate docstring. For example, Python Docstring Generator is available for VSCode Python Naming Conventions: Naming of classes : CapitalizedWords Naming of functions and variables : lower_case_with_underscores You may maintain matrix and constant naming conventions when declaring class level variables. For instant variables, lower_case_with_underscores is still preferred. Data Naming Conventions Naming of matrices : DLTHWRKI1A1 see Matrix Naming Conventions for more Naming of constants : C_HWRKBIAS Function Abstraction \u00b6 If the similar (or identical) routine needs to be done more than once, the routine should be written into a function with appropriate arguments. If a set of functions is part of a high-order concept, they should be organized into a class, thus, becoming a modular component that can be instantiated. An example of a concept that can be organized into a class is the Mode Choice Model. Different function within a class object for Mode Choice can read specification, evaluate mode availabilities, and calculate choice probabilities. Once instantiated, the Mode Choice Model object can be applied to different trips by household segments. Data Management \u00b6 Intermediate data must not be persisted into EMME databank. They may be persisted using fast and lightweight data storage such as arrow for numpy and feather for pandas . should have descriptive names, and when possible, use the fixed length naming convention established in Matrix Naming Conventions . should be removed after a model run (unless if running in test/debug mode). Final output data must be stored as EMME matrices, or into the omx data format (see osPlanning/omx ). must follow fixed length naming conventions based on attribute category, see Matrix Naming Conventions . Do not use of legacy EMME matrix numbering system. Definition of naming of array, matrices, and data tables may be specified with metadata configurations. This is especially useful when a large amount of intermediate data are carried across the modeling process. Managing Logging and Exceptions \u00b6 Logging level may be modified during development as per need. Minimal logging should be used for commits pulled into master or main branch of the RTM. Error messages and exception handling need to always be reported to logs. Compliance and Procedure \u00b6 All code submissions shall be done using GitHub pull request feature. The code checking tool and code review procedure are used to enforce our coding standards. The code checking tool, if applicable, enforces standard rules using automated tools such as pylint for GitHub . Then, the review procedure is carried out to ensure area of compliance not covered by the code checking tool. The intention of the code checking tool is to aid in the submission process and is not intended to replace the standard review procedure. If a commit does not meet our standard, RTM code maintainers will request revision on your pull request. Code Checking Tool \u00b6 Rules covered by PEP8 will be automatically checked upon any commit to the main RTM repository (feature to be launched on the main RTM repository at a future date). Code Review Procedure \u00b6 The code review procedure is to be conducted after the initiation of a Pull Request into any protected branches. For a commit to a non-protected branch, developers do not need to conform to the guideline for ease of development, testing and debugging. However, protected branches such as \u201cmaster\u201d will be checked for compliance. At least 1 approver review by an RTM maintainer (typically the Project Manager) is needed. The RTM maintainer shall provide specific comments and suggestions if revisions are needed. No comments are required for any approved commits.","title":"Coding Standard"},{"location":"coding_standard/#coding-standard","text":"The coding standard for the RTM has two major components: guideline and compliance. Coding Standard Guideline spell out the best practices around the design of the RTM code. It is comprised of 3 elements: styles, function abstraction, data management. Following the guideline ensures the programming paradigm used by all developers of the RTM remain the consistent. Code Standard Compliance refers to the set of tools and procedures employed to ensure that the guideline has been met. The code review standards will be enforced through our code review process.","title":"Coding Standard"},{"location":"coding_standard/#general-guideline","text":"","title":"General Guideline"},{"location":"coding_standard/#styles","text":"Recommended Code Formator: Black from Python Software Foundation Compliant with the recent changes in the PEP 8 style guide For integration with your favourite IDE, see Editor Integration for Black Key requirements taken from the Python PEP8 Standard: Indentation : spaces are strongly preferred (4 spaces in place of 1 tab) Maximum Line Length : no less than 80 and no more than 100 characters, note that Black defaults to 88 Line Break : occur before binary operators Blank lines : surround top-level function and class definitions with two blank lines. Method definitions inside a class are surrounded by a single blank line Imports : separate lines of import starting with standard library, related third party, then local application imports Comments : block comments should explain following code and what and why they do or may be used to section the code Docstring (Documentation String) : docstring is required for all modules , functions , classes and methods . It should contain short description of what, how, and why the code exists. When applicable, describe the input and output types of the function. For more specific conventions on docstring, see PEP 257 You may use extension/tools to generate docstring. For example, Python Docstring Generator is available for VSCode Python Naming Conventions: Naming of classes : CapitalizedWords Naming of functions and variables : lower_case_with_underscores You may maintain matrix and constant naming conventions when declaring class level variables. For instant variables, lower_case_with_underscores is still preferred. Data Naming Conventions Naming of matrices : DLTHWRKI1A1 see Matrix Naming Conventions for more Naming of constants : C_HWRKBIAS","title":"Styles"},{"location":"coding_standard/#function-abstraction","text":"If the similar (or identical) routine needs to be done more than once, the routine should be written into a function with appropriate arguments. If a set of functions is part of a high-order concept, they should be organized into a class, thus, becoming a modular component that can be instantiated. An example of a concept that can be organized into a class is the Mode Choice Model. Different function within a class object for Mode Choice can read specification, evaluate mode availabilities, and calculate choice probabilities. Once instantiated, the Mode Choice Model object can be applied to different trips by household segments.","title":"Function Abstraction"},{"location":"coding_standard/#data-management","text":"Intermediate data must not be persisted into EMME databank. They may be persisted using fast and lightweight data storage such as arrow for numpy and feather for pandas . should have descriptive names, and when possible, use the fixed length naming convention established in Matrix Naming Conventions . should be removed after a model run (unless if running in test/debug mode). Final output data must be stored as EMME matrices, or into the omx data format (see osPlanning/omx ). must follow fixed length naming conventions based on attribute category, see Matrix Naming Conventions . Do not use of legacy EMME matrix numbering system. Definition of naming of array, matrices, and data tables may be specified with metadata configurations. This is especially useful when a large amount of intermediate data are carried across the modeling process.","title":"Data Management"},{"location":"coding_standard/#managing-logging-and-exceptions","text":"Logging level may be modified during development as per need. Minimal logging should be used for commits pulled into master or main branch of the RTM. Error messages and exception handling need to always be reported to logs.","title":"Managing Logging and Exceptions"},{"location":"coding_standard/#compliance-and-procedure","text":"All code submissions shall be done using GitHub pull request feature. The code checking tool and code review procedure are used to enforce our coding standards. The code checking tool, if applicable, enforces standard rules using automated tools such as pylint for GitHub . Then, the review procedure is carried out to ensure area of compliance not covered by the code checking tool. The intention of the code checking tool is to aid in the submission process and is not intended to replace the standard review procedure. If a commit does not meet our standard, RTM code maintainers will request revision on your pull request.","title":"Compliance and Procedure"},{"location":"coding_standard/#code-checking-tool","text":"Rules covered by PEP8 will be automatically checked upon any commit to the main RTM repository (feature to be launched on the main RTM repository at a future date).","title":"Code Checking Tool"},{"location":"coding_standard/#code-review-procedure","text":"The code review procedure is to be conducted after the initiation of a Pull Request into any protected branches. For a commit to a non-protected branch, developers do not need to conform to the guideline for ease of development, testing and debugging. However, protected branches such as \u201cmaster\u201d will be checked for compliance. At least 1 approver review by an RTM maintainer (typically the Project Manager) is needed. The RTM maintainer shall provide specific comments and suggestions if revisions are needed. No comments are required for any approved commits.","title":"Code Review Procedure"},{"location":"data_analysis/","text":"Model Data Analysis \u00b6 Python libraries and the EMME python \u00b6 In general the RTM uses the default python libraries installed with the EMME python. In some cases this means foregoing features that have been recently introduced or having code that is not compatible with python installations outside EMME. However, it ensures that the RTM will run without further configuration providing the user has installed the correct version of EMME. Python + util \u00b6 The RTM scripting employs a utility toolbox that creates functions for repeated operations. It is worth familiarizing yourself with the functions here as they are often employed outside the RTM run itself to post-process data. For example, util can be used to create a pandas dataframe for full matrices and append RTM matrices to it for further analysis or plotting in python. import inro.modeller as _m util = _m.Modeller().tool(\"translink.util\") eb = _m.Modeller().emmebank df = util.get_pd_ij_df(eb) # create a longform pandas dataframe for full matrices # add columns for light and heavy truck OD tables to the data frame df['amLGVDemand'] = util.get_matrix_numpy(eb, \"mfLGVAM\").flatten() df['amHGVDemand'] = util.get_matrix_numpy(eb, \"mfHGVAM\").flatten() Which yields the following results: In the above example matrix names or numbers can be used to access the data in util.get_matrix_numpy() . Data extraction using variables of interest tool \u00b6 In this example we use a spreadsheet to build a sqlite database. This tools allows the user to create tables and fields within a table using EMME matrix calculations. The Variables_of_Interest.xlsx spreadsheet is located in the RTM/Scripts/Phase3Analytics folder. Setting up the spreadsheet \u00b6 The Datamap tab describes what each field does in the Sheet1 tab. The Sheet1 tab is used to build the database. A simple matrix calculation can be used to extract data. Entering the following in Sheet1 will create a table in the output sqlite database with AM demand for light and heavy goods vehicles equivalent to the pandas data frame example above: In addition to simple data extraction, new data can be generated based on RTM outputs using EMME matrix calculator expressions. For example, we can create a matrix of the minimum time on transit between all zones considering both the bus and rail modes. Note that we can reference EMME matrices by name or number. Warning All Attributes (fields) associated with the same Category (table) must be of the same dimension. For example, full matrices (MF) and vectors (MO/MD) must be placed in separate tables. Running the Modeller Tool \u00b6 Once you have specified the data you wish to output in in Sheet1 , the modeller tool Variables of Interest Extraction located in the Phase3Analytics Toolbox can be run to produce the outputs. Note This tool requires the installation of one additional python library. Installation instructions can be found in the tool documentation at RTM/Documentation/ToolDoc_VariablesOfInterest.pdf . To create the database with your specified tables and fields simply run the tool with the appropriate sheet name in the field. If you have not changed the name from Sheet1 there is nothing to update and you can simply run the tool. Note The name Sheet1 can be changed to any Excel allowable name. The chosen name needs to be passed to the tool either in the modeller interface or the function call. Viewing results \u00b6 The requested results will be output to a sqlite database named Variables_of_Interest_Results.db . The output can be viewed in the same manner as the trip_summaries.db and rtm.db discussed here Post-processing tools \u00b6 RTM provides several standard tools to help users quickly analyze the transportation results after the model run. The common-use pro-processing tools are select link analysis for traffic and select transit analysis for public transit. These two tools are also located in Phase3Analytics Toolbox. Auto Select Link tool \u00b6 The auto select link tool is for analyzing the auto traffic on a select link. The tool provides a simple capability to let user perform a quick select assignment. Set @selectlink=1 to select the link that to be evaluated. Transit Select link/line tool \u00b6 The transit select tool provides larger capability to perform more complicated post analysis. It executes the transit assignment based on the select link/ line. Upon the request, the outputs can be: transit segment volumes, boardings, alightings for select transit link/line link auxiliary transit volume for select walk link matrices by transit sub-mode Before using the tool There are some procedures needs to be done prior to applying the select transit tool: - Ensure the databank dimension has enough room for additional extra attributes. The recommended number is 2,500,000 - Strategies generated by extended transit assignments. (Finish full model run or transit assignment) - Recommend copy AM, MD, and PM scenario to prevent from overwriting the original scenario - Create the extra attribute(s) and set the appropriate value in all three scenarios (following the below instruction) After using the tool The tool performs post-analyses on paths extracted from strategies generated by extended transit assignments, and it creates: 1. new matrices and save the O-D results: ID Name Descritption mf110 AMBUSsline AM Bus Select transit O-D mf111 AMRALsline AM Rail Select transit O-D mf112 AMWCEsline AM WCE Select transit O-D mf113 AMTOTsline AM TOT Select transit O-D mf130 MDBUSsline MD Bus Select transit O-D mf131 MDRALsline MD Rail Select transit O-D mf132 MDWCEsline MD WCE Select transit O-D mf133 MDTOTsline MD TOT Select transit O-D mf150 PMBUSsline PM Bus Select transit O-D mf151 PMRALsline PM Rail Select transit O-D mf152 PMWCEsline PM WCE Select transit O-D mf153 PMTOTsline PM TOT Select transit O-D 2.new extra attributes and save network results: Name Descritption Name Descritption @voltr_bussline Transit Volume BUS @board_bussline Boardings BUS @voltr_ralsline Transit Volume RAL @board_ralsline Boardings RAL @voltr_wcesline Transit Volume WCE @board_wcesline Boardings WCE @voltr_totsline Transit Volume TOT @board_totsline Boardings TOT @volax_bussline Aux Transit Volume BUS @alight_bussline Alightings BUS @volax_ralsline Aux Transit Volume RAL @alight_ralsline Alightings RAL @volax_wcesline Aux Transit Volume WCE @alight_wcesline Alightings WCE @volax_totsline Aux Transit Volume TOT @alight_totsline Alightings TOT 3.Aggregate O-D result matrices and export to CSV: \\rtm\\RTM\\ \\Outputs\\transit_sline_gy.csv) transit link/line selection The transit link/line can be selected by setting up the trip components. There are multiple ways to select the trip component. One or more trip components can be taken into account: For in-vehicle trip component, the selections can be link (@selectsegment), transit line (@selectline), or segment (@selectlink). Check the box if the trip component is an auxiliary transit link For boarding or alighting trip component, the selections is node attribute(@selectinboanode,@selectfialinode @selecttrboanode @selecttralinode ). Check one box from the node component upon the purpose. Examples These are some examples related to trip component selection, and path selection threshold. Case 1: The people who are in-vehicle in segment A Case 2: The people who in-vehicle in segment A OR walk on link B Case 3: The people who in-vehicle in segment A AND walk on link B Case 4: The people who in-vehicle in segment A AND walk on link B AND transfer boarding at node C Case 5: The people who take any segment D of a transit line Case 6: The people who initially board at node C AND finally alight at node E","title":"Model Data Analysis"},{"location":"data_analysis/#model-data-analysis","text":"","title":"Model Data Analysis"},{"location":"data_analysis/#python-libraries-and-the-emme-python","text":"In general the RTM uses the default python libraries installed with the EMME python. In some cases this means foregoing features that have been recently introduced or having code that is not compatible with python installations outside EMME. However, it ensures that the RTM will run without further configuration providing the user has installed the correct version of EMME.","title":"Python libraries and the EMME python"},{"location":"data_analysis/#python-util","text":"The RTM scripting employs a utility toolbox that creates functions for repeated operations. It is worth familiarizing yourself with the functions here as they are often employed outside the RTM run itself to post-process data. For example, util can be used to create a pandas dataframe for full matrices and append RTM matrices to it for further analysis or plotting in python. import inro.modeller as _m util = _m.Modeller().tool(\"translink.util\") eb = _m.Modeller().emmebank df = util.get_pd_ij_df(eb) # create a longform pandas dataframe for full matrices # add columns for light and heavy truck OD tables to the data frame df['amLGVDemand'] = util.get_matrix_numpy(eb, \"mfLGVAM\").flatten() df['amHGVDemand'] = util.get_matrix_numpy(eb, \"mfHGVAM\").flatten() Which yields the following results: In the above example matrix names or numbers can be used to access the data in util.get_matrix_numpy() .","title":"Python + util"},{"location":"data_analysis/#data-extraction-using-variables-of-interest-tool","text":"In this example we use a spreadsheet to build a sqlite database. This tools allows the user to create tables and fields within a table using EMME matrix calculations. The Variables_of_Interest.xlsx spreadsheet is located in the RTM/Scripts/Phase3Analytics folder.","title":"Data extraction using variables of interest tool"},{"location":"data_analysis/#setting-up-the-spreadsheet","text":"The Datamap tab describes what each field does in the Sheet1 tab. The Sheet1 tab is used to build the database. A simple matrix calculation can be used to extract data. Entering the following in Sheet1 will create a table in the output sqlite database with AM demand for light and heavy goods vehicles equivalent to the pandas data frame example above: In addition to simple data extraction, new data can be generated based on RTM outputs using EMME matrix calculator expressions. For example, we can create a matrix of the minimum time on transit between all zones considering both the bus and rail modes. Note that we can reference EMME matrices by name or number. Warning All Attributes (fields) associated with the same Category (table) must be of the same dimension. For example, full matrices (MF) and vectors (MO/MD) must be placed in separate tables.","title":"Setting up the spreadsheet"},{"location":"data_analysis/#running-the-modeller-tool","text":"Once you have specified the data you wish to output in in Sheet1 , the modeller tool Variables of Interest Extraction located in the Phase3Analytics Toolbox can be run to produce the outputs. Note This tool requires the installation of one additional python library. Installation instructions can be found in the tool documentation at RTM/Documentation/ToolDoc_VariablesOfInterest.pdf . To create the database with your specified tables and fields simply run the tool with the appropriate sheet name in the field. If you have not changed the name from Sheet1 there is nothing to update and you can simply run the tool. Note The name Sheet1 can be changed to any Excel allowable name. The chosen name needs to be passed to the tool either in the modeller interface or the function call.","title":"Running the Modeller Tool"},{"location":"data_analysis/#viewing-results","text":"The requested results will be output to a sqlite database named Variables_of_Interest_Results.db . The output can be viewed in the same manner as the trip_summaries.db and rtm.db discussed here","title":"Viewing results"},{"location":"data_analysis/#post-processing-tools","text":"RTM provides several standard tools to help users quickly analyze the transportation results after the model run. The common-use pro-processing tools are select link analysis for traffic and select transit analysis for public transit. These two tools are also located in Phase3Analytics Toolbox.","title":"Post-processing tools"},{"location":"data_analysis/#auto-select-link-tool","text":"The auto select link tool is for analyzing the auto traffic on a select link. The tool provides a simple capability to let user perform a quick select assignment. Set @selectlink=1 to select the link that to be evaluated.","title":"Auto Select Link tool"},{"location":"data_analysis/#transit-select-linkline-tool","text":"The transit select tool provides larger capability to perform more complicated post analysis. It executes the transit assignment based on the select link/ line. Upon the request, the outputs can be: transit segment volumes, boardings, alightings for select transit link/line link auxiliary transit volume for select walk link matrices by transit sub-mode Before using the tool There are some procedures needs to be done prior to applying the select transit tool: - Ensure the databank dimension has enough room for additional extra attributes. The recommended number is 2,500,000 - Strategies generated by extended transit assignments. (Finish full model run or transit assignment) - Recommend copy AM, MD, and PM scenario to prevent from overwriting the original scenario - Create the extra attribute(s) and set the appropriate value in all three scenarios (following the below instruction) After using the tool The tool performs post-analyses on paths extracted from strategies generated by extended transit assignments, and it creates: 1. new matrices and save the O-D results: ID Name Descritption mf110 AMBUSsline AM Bus Select transit O-D mf111 AMRALsline AM Rail Select transit O-D mf112 AMWCEsline AM WCE Select transit O-D mf113 AMTOTsline AM TOT Select transit O-D mf130 MDBUSsline MD Bus Select transit O-D mf131 MDRALsline MD Rail Select transit O-D mf132 MDWCEsline MD WCE Select transit O-D mf133 MDTOTsline MD TOT Select transit O-D mf150 PMBUSsline PM Bus Select transit O-D mf151 PMRALsline PM Rail Select transit O-D mf152 PMWCEsline PM WCE Select transit O-D mf153 PMTOTsline PM TOT Select transit O-D 2.new extra attributes and save network results: Name Descritption Name Descritption @voltr_bussline Transit Volume BUS @board_bussline Boardings BUS @voltr_ralsline Transit Volume RAL @board_ralsline Boardings RAL @voltr_wcesline Transit Volume WCE @board_wcesline Boardings WCE @voltr_totsline Transit Volume TOT @board_totsline Boardings TOT @volax_bussline Aux Transit Volume BUS @alight_bussline Alightings BUS @volax_ralsline Aux Transit Volume RAL @alight_ralsline Alightings RAL @volax_wcesline Aux Transit Volume WCE @alight_wcesline Alightings WCE @volax_totsline Aux Transit Volume TOT @alight_totsline Alightings TOT 3.Aggregate O-D result matrices and export to CSV: \\rtm\\RTM\\ \\Outputs\\transit_sline_gy.csv) transit link/line selection The transit link/line can be selected by setting up the trip components. There are multiple ways to select the trip component. One or more trip components can be taken into account: For in-vehicle trip component, the selections can be link (@selectsegment), transit line (@selectline), or segment (@selectlink). Check the box if the trip component is an auxiliary transit link For boarding or alighting trip component, the selections is node attribute(@selectinboanode,@selectfialinode @selecttrboanode @selecttralinode ). Check one box from the node component upon the purpose. Examples These are some examples related to trip component selection, and path selection threshold. Case 1: The people who are in-vehicle in segment A Case 2: The people who in-vehicle in segment A OR walk on link B Case 3: The people who in-vehicle in segment A AND walk on link B Case 4: The people who in-vehicle in segment A AND walk on link B AND transfer boarding at node C Case 5: The people who take any segment D of a transit line Case 6: The people who initially board at node C AND finally alight at node E","title":"Transit Select link/line tool"},{"location":"data_assumptions/","text":"Data and Assumptions \u00b6 In this section, you will find some useful infomation you may need to use in your RTM application project. Some of them are RTM assumptions and some of them are the calibrated factors. This document could be a dictionary of the model components. Modes and Vehicles \u00b6 This section shows the link and transit mode, and transit vehicle type Link mode \u00b6 Mode Description v Vehicles (Primary Auto Mode) c High Occupancy Vehicles (HOV) d Single Occupancy Vehicles (SOV) x Light Trucks t Heavy Trucks n Discourage Heavy Vehicle Traffic b Bus p Pedestrian a Auxillary Transit Access l SkyTrain r Commuter Rail (WCE) s SeaBus h Gondola Transit mode \u00b6 Mode Description b Bus l SkyTrain r Commuter Rail (WCE) s SeaBus g BRT f LRT h Gondola Transit Vehicle Capacity and Cost \u00b6 The table below gives a transit vehicle capacity and cost catalog. The value is used in the capacited transit assignment. Transit Vehicle Mode Fleet Size Capacity(S/T) OC($/h) OC($/km) Auto. equ 1.Motor-Bus b 747 35/50 65 0.55 2.5 2.Trolley b 224 31/47 65 0.55 2.5 3.Blue-Bus b 48 31/47 65 0.55 2.5 4.Sea-Bus s 2 300/400 0 0 0 5.Sky-E l 999 88/287 0 3 0 6.pcl-bus b 20 50/50 30 0.55 2.5 7.Abbotsford b 12 48/60 30 0.55 2.5 8.Comrail-5 r 5 1500/1650 0 0 0 9.Chilliwack b 12 48/60 30 0.55 2.5 10.Mot-Artics b 42 48/90 65 0.55 3.75 11.Tro-Artics b 1 49/70 65 0.55 3.75 12.HwyCoach b 66 49/49 65 0.55 2.5 13.MiniBus b 40 20/24 65 0.55 2.5 14.Sky-M l 999 140/300 0 0 0 15.Sky-Sh l 999 150/350 0 0 0 16.BRT g 42 60/105 0 0 3.75 17.LRT f 1 160/300 0 3 0 18.Gondola h 20 28/35 0 0 0 20.Sky-ML2 l 999 127/218 0 0 0 31.EL-2017 l 999 174/424 0 3 0 32.EL-2030 l 999 143/509 0 3 0 33.EL-2045 l 999 143/509 0 3 0 34.ML-2017 l 999 66/222 0 3 0 35.ML-2030 l 999 132/444 0 3 0 35.ML-2030 l 999 132/444 0 3 0 36.ML-2045 l 999 132/444 0 3 0 37.CL-2017 l 999 88/288 0 3 0 38.CL-2030 l 999 88/288 0 3 0 39.CL-2045 l 999 110/360 0 3 0 Volume Delay Function \u00b6 VDF Description Expression 11 Centroid Connector length x 60/40 12 Bowen Island Service 40 + (volume-100) x 60/volume x (volume>=100) 13 Highway Merge Lane length x 60/posted_speed + 0.85 x (volume/(capacity x lanes))^5 14 Stop Sign and Signal signal_delay + length x 60/posted_speed + 0.85 x (volume/(capacity x lanes))^4 15 Free Flow length x 60/posted_speed x (1+0.6 x 0.85 x (volume/(capacity x lanes^1.05))^5) 16 Free Flow length x 60/(posted_speed x 1.1) x (1+0.6 x 0.43 x (volume/(capacity x lanes^1.05))^5.25) Time of Assginment \u00b6 The RTM model only handles 3 scenarios in each databank. Each scenario stores one-hour assignment result, and it is used to represent various periods throughout the day. Peak Period Hour Hours of the day represented AM 07:30 - 08:30 06:00 - 10:00 MD 12:00 - 13:00 10:00 - 15:00, 18:00 - 06:00 PM 16:30 - 17:30 15:00 - 18:00 Expansion Factor \u00b6 The traffic expansion factor is to convert the model assigned hourly traffic volume to daily volume. The factor set has a generic set and categorized set. Both sets can be used according to the study purpose. All Class \u00b6 AM MD PM 3.68 9.20 3.11 By Category \u00b6 Class AM MD PM SOV 3.44 8.41 3.95 HOV 1.51 8.58 5.32 SOV+HOV 3.22 8.63 4.05 LGV 3.59 5.63 6.17 HGV 4.88 5.43 6.36 LGV+HGV 3.83 5.81 6.63 Transit Ridership \u00b6 The transit expansion factor converts the hourly transit ridership to daily value. Like auto traffic, the factor set is classed by different types of service. The value shows below. (*SeaBus has the same factor with Rail) Transit mode AM MD PM Bus 2.54 9.44 2.57 SkyTrain 2.53 9.54 2.92 WCE 3.34 - 2.02 Annual Factor \u00b6 The annual expansion factor is to expend the daily volume on typical fall weekday to yearly volume. The generic expansion factor, including all vehicle classes, is 333. And the factors for each class are shown below: Class Factor All Classes 333 Auto 335 LGV 313 HGV 276 Transit 300 Bus 299 Skytrain 331 WCE 224 Transit Perception Factor \u00b6 Transit perception is a factor that can reflect the user\u2019s feeling about taking transit in each stage. It is used in the mode choice model. The values are cited from the literature with some minor adjustment based on local knowledge. Note that the boarding perception factor used for transfer boarding only. The model assumed there is no penalty for initial boarding. Current model only account perception impact for work purpose only. Bus/Rail/WCE Time Component Matrix EMME Name Value Bus In-vehicle ms300 busIVTprcpWk 1.25 -- Wait ms301 busWAITprcpWk 2.50 -- Walk ms302 busWALKprcpWk 2.00 -- Boarding ms303 busBOARDSprcpWk 10.00 Rail In-vehicle ms310 railIVTprcpWk 1.00 -- Wait ms311 railWAITprcpWk 2.50 -- Walk ms312 railWALKprcpWk 2.00 -- Boarding ms313 railBOARDSprcpWk 10.00 WCE In-vehicle ms320 wceIVTprcpWk 1.00 -- Wait ms321 wceWAITprcpWk 2.50 -- Walk ms322 wceWALKprcpWk 2.00 -- Boarding ms323 wceBOARDSprcpWk 10.00 Matrix \u00b6 Over thousand matrices are used in the RTM simulation. Here is a list showing the matrices grouped by type and function: For the full list, see matrix list page . Ensembles \u00b6 The ensemble is a useful tool to group the zones by different purposes, either for geographical mapping or model calculating. There are various ensembles in the RTM model. Some of them are under implementation. Some of them are historically used and not functional at this moment, but we keep them as a placeholder for future usage. Here is a list of ensembles that the latest RTM is using: Download Ensemble_Index.pdf","title":"Data and Assumptions"},{"location":"data_assumptions/#data-and-assumptions","text":"In this section, you will find some useful infomation you may need to use in your RTM application project. Some of them are RTM assumptions and some of them are the calibrated factors. This document could be a dictionary of the model components.","title":"Data and Assumptions"},{"location":"data_assumptions/#modes-and-vehicles","text":"This section shows the link and transit mode, and transit vehicle type","title":"Modes and Vehicles"},{"location":"data_assumptions/#link-mode","text":"Mode Description v Vehicles (Primary Auto Mode) c High Occupancy Vehicles (HOV) d Single Occupancy Vehicles (SOV) x Light Trucks t Heavy Trucks n Discourage Heavy Vehicle Traffic b Bus p Pedestrian a Auxillary Transit Access l SkyTrain r Commuter Rail (WCE) s SeaBus h Gondola","title":"Link mode"},{"location":"data_assumptions/#transit-mode","text":"Mode Description b Bus l SkyTrain r Commuter Rail (WCE) s SeaBus g BRT f LRT h Gondola","title":"Transit mode"},{"location":"data_assumptions/#transit-vehicle-capacity-and-cost","text":"The table below gives a transit vehicle capacity and cost catalog. The value is used in the capacited transit assignment. Transit Vehicle Mode Fleet Size Capacity(S/T) OC($/h) OC($/km) Auto. equ 1.Motor-Bus b 747 35/50 65 0.55 2.5 2.Trolley b 224 31/47 65 0.55 2.5 3.Blue-Bus b 48 31/47 65 0.55 2.5 4.Sea-Bus s 2 300/400 0 0 0 5.Sky-E l 999 88/287 0 3 0 6.pcl-bus b 20 50/50 30 0.55 2.5 7.Abbotsford b 12 48/60 30 0.55 2.5 8.Comrail-5 r 5 1500/1650 0 0 0 9.Chilliwack b 12 48/60 30 0.55 2.5 10.Mot-Artics b 42 48/90 65 0.55 3.75 11.Tro-Artics b 1 49/70 65 0.55 3.75 12.HwyCoach b 66 49/49 65 0.55 2.5 13.MiniBus b 40 20/24 65 0.55 2.5 14.Sky-M l 999 140/300 0 0 0 15.Sky-Sh l 999 150/350 0 0 0 16.BRT g 42 60/105 0 0 3.75 17.LRT f 1 160/300 0 3 0 18.Gondola h 20 28/35 0 0 0 20.Sky-ML2 l 999 127/218 0 0 0 31.EL-2017 l 999 174/424 0 3 0 32.EL-2030 l 999 143/509 0 3 0 33.EL-2045 l 999 143/509 0 3 0 34.ML-2017 l 999 66/222 0 3 0 35.ML-2030 l 999 132/444 0 3 0 35.ML-2030 l 999 132/444 0 3 0 36.ML-2045 l 999 132/444 0 3 0 37.CL-2017 l 999 88/288 0 3 0 38.CL-2030 l 999 88/288 0 3 0 39.CL-2045 l 999 110/360 0 3 0","title":"Transit Vehicle Capacity and Cost"},{"location":"data_assumptions/#volume-delay-function","text":"VDF Description Expression 11 Centroid Connector length x 60/40 12 Bowen Island Service 40 + (volume-100) x 60/volume x (volume>=100) 13 Highway Merge Lane length x 60/posted_speed + 0.85 x (volume/(capacity x lanes))^5 14 Stop Sign and Signal signal_delay + length x 60/posted_speed + 0.85 x (volume/(capacity x lanes))^4 15 Free Flow length x 60/posted_speed x (1+0.6 x 0.85 x (volume/(capacity x lanes^1.05))^5) 16 Free Flow length x 60/(posted_speed x 1.1) x (1+0.6 x 0.43 x (volume/(capacity x lanes^1.05))^5.25)","title":"Volume Delay Function"},{"location":"data_assumptions/#time-of-assginment","text":"The RTM model only handles 3 scenarios in each databank. Each scenario stores one-hour assignment result, and it is used to represent various periods throughout the day. Peak Period Hour Hours of the day represented AM 07:30 - 08:30 06:00 - 10:00 MD 12:00 - 13:00 10:00 - 15:00, 18:00 - 06:00 PM 16:30 - 17:30 15:00 - 18:00","title":"Time of Assginment"},{"location":"data_assumptions/#expansion-factor","text":"The traffic expansion factor is to convert the model assigned hourly traffic volume to daily volume. The factor set has a generic set and categorized set. Both sets can be used according to the study purpose.","title":"Expansion Factor"},{"location":"data_assumptions/#all-class","text":"AM MD PM 3.68 9.20 3.11","title":"All Class"},{"location":"data_assumptions/#by-category","text":"Class AM MD PM SOV 3.44 8.41 3.95 HOV 1.51 8.58 5.32 SOV+HOV 3.22 8.63 4.05 LGV 3.59 5.63 6.17 HGV 4.88 5.43 6.36 LGV+HGV 3.83 5.81 6.63","title":"By Category"},{"location":"data_assumptions/#transit-ridership","text":"The transit expansion factor converts the hourly transit ridership to daily value. Like auto traffic, the factor set is classed by different types of service. The value shows below. (*SeaBus has the same factor with Rail) Transit mode AM MD PM Bus 2.54 9.44 2.57 SkyTrain 2.53 9.54 2.92 WCE 3.34 - 2.02","title":"Transit Ridership"},{"location":"data_assumptions/#annual-factor","text":"The annual expansion factor is to expend the daily volume on typical fall weekday to yearly volume. The generic expansion factor, including all vehicle classes, is 333. And the factors for each class are shown below: Class Factor All Classes 333 Auto 335 LGV 313 HGV 276 Transit 300 Bus 299 Skytrain 331 WCE 224","title":"Annual Factor"},{"location":"data_assumptions/#transit-perception-factor","text":"Transit perception is a factor that can reflect the user\u2019s feeling about taking transit in each stage. It is used in the mode choice model. The values are cited from the literature with some minor adjustment based on local knowledge. Note that the boarding perception factor used for transfer boarding only. The model assumed there is no penalty for initial boarding. Current model only account perception impact for work purpose only. Bus/Rail/WCE Time Component Matrix EMME Name Value Bus In-vehicle ms300 busIVTprcpWk 1.25 -- Wait ms301 busWAITprcpWk 2.50 -- Walk ms302 busWALKprcpWk 2.00 -- Boarding ms303 busBOARDSprcpWk 10.00 Rail In-vehicle ms310 railIVTprcpWk 1.00 -- Wait ms311 railWAITprcpWk 2.50 -- Walk ms312 railWALKprcpWk 2.00 -- Boarding ms313 railBOARDSprcpWk 10.00 WCE In-vehicle ms320 wceIVTprcpWk 1.00 -- Wait ms321 wceWAITprcpWk 2.50 -- Walk ms322 wceWALKprcpWk 2.00 -- Boarding ms323 wceBOARDSprcpWk 10.00","title":"Transit Perception Factor"},{"location":"data_assumptions/#matrix","text":"Over thousand matrices are used in the RTM simulation. Here is a list showing the matrices grouped by type and function: For the full list, see matrix list page .","title":"Matrix"},{"location":"data_assumptions/#ensembles","text":"The ensemble is a useful tool to group the zones by different purposes, either for geographical mapping or model calculating. There are various ensembles in the RTM model. Some of them are under implementation. Some of them are historically used and not functional at this moment, but we keep them as a placeholder for future usage. Here is a list of ensembles that the latest RTM is using: Download Ensemble_Index.pdf","title":"Ensembles"},{"location":"data_generate/","text":"Custom Data Generation \u00b6 Python script vs modeller tool \u00b6 In general Modeller tools more polished and can provide a GUI, but take more time and effort to prepare. They can still be called from other places and imported into other files. We generally use these for production grade, stable solutions, that we intend to use repeatedly. Object oriented. It is expected that everyone here knows how to use a modeller tool Python script can be built easily and often is enough to quickly complete a task at hand. Maybe fragile, can break with simple model changes. These may not be maintained. These can work will with the notebooks. They can just work with basic functions, easy to put together a basic but reproducible analysis. Executing a python script from the EMME iPython shell \u00b6 Open the ipython modeller shell in EMME desktop Once in the shell, navigate to location of the subject script. In this case the script in question is located in the RTM/Scripts/Tools directory Check the files in this directory to ensure we have the one we want, then we can use the iPython magic %run <filename.py> to execute our script Example Tool for PA to OD \u00b6 As a general rule in trip based modeling production/attraction (PA) trips can be converted to origin/destination (OD) trips roughly by 0.5 * (mat + mat') . However, for a variety of reasons, most notably non-home-based (NHB) trips, not everything is completely symmetrical. Production to attraction blending factors have been calculated from the trip diary for the RTM skims. The example runs the PA2OD.py python script with the output directed to the trip_summaries.db database. Once you have run the script, navigate to RTM/<your_db_folder> and open the trip_summaries.db in your sqlite viewer. Once there, we will see a new table named od_daily_<your ensemble if any> From there we can query and view the results Why does it run? \u00b6 if __name__ == '__main__': eb = _m.Modeller().emmebank # run with ensemble name and ensem_agg = True to aggregate # run with no ensemble name and ensem_agg = False to get TAZ level results # note, TAZ level results create very large file main(eb, ensem='gy', ensem_agg=True) And why does this work? See this video on YouTube Name and Main Using the notebook \u00b6 The EMME Notebook is a powerful tool to generate results and summary data across many model runs. To demonstrate this, we took the base case model and varied the park and ride price on a particular park and ride lot. We run a separate model run for each park and ride price point. Then, we summarized the result across all of these model runs. Connect to EMME Modeler \u00b6 When working with Jupyter Notebook within EMME, you need to import some basic libraries and toolboxes first, then connect to EMME desktop and databank. import inro.modeller as _m import inro.emme.desktop as _d import csv import os import multiprocessing import numpy as np import pandas as pd import sqlite3 import traceback as _traceback import shutil # connect to EMME dt = _m.Modeller().desktop de = dt.data_explorer() db = de.active_database() ebs = de.databases() # load toolbox util = _m.Modeller().tool(\"translink.util\") Listing databanks \u00b6 Within the Notebook scripting environment, you have access to all of the databank that is loaded in your project, you can list them all to verify. # make sure modeller is closed or it will print to the python console in there counter = 0 for eb in ebs: print counter, eb.title() counter += 1 0 Minimal Base Databank 1 pnr_bp_2016_250 2 pnr_bp_2016_300 Build functions \u00b6 To retrieve results from multiple runs, you should build a function to systematically get the results. In our example, we will get the lot usage at all park and ride lot, and export the results to csv. def get_all_pnr_usage(eb): scenario_tag = str(util.get_eb_path(eb).split('\\\\')[-1].split('_')[-1]) df = util.get_pd_ij_df(eb) df['price'] = scenario_tag df['pnr_usage'] = (util.get_matrix_numpy(eb, \"mf3000\").flatten() + util.get_matrix_numpy(eb, \"mf3001\").flatten() + util.get_matrix_numpy(eb, \"mf3002\").flatten()) / 2 df = df[(df['j'] >99) & (df['j'] < 999)] return (df) # initialize dataframe, ij from any eb all_pnr_usage = pd.DataFrame() # loop to get result for eb in ebs: title = eb.title() if title == 'Minimal Base Databank': continue eb.open() eb = _m.Modeller().emmebank if all_pnr_usage.empty: all_pnr_usage = get_all_pnr_usage(eb) else: all_pnr_usage = pd.concat([all_pnr_usage, get_all_pnr_usage(eb)], axis=0) all_pnr_usage_summary = all_pnr_usage.groupby( ['price', 'j']).sum()[['pnr_usage']].reset_index().pivot_table( values='pnr_usage', index='j', columns='price').fillna(0) # export result for all pnr lot usage import datetime all_pnr_usage_summary.to_csv( 'Bridgeport_pnr_all_lot_usage_result_' + datetime.datetime.today().strftime('%Y-%m-%d') + '.csv', index=True) Here is a snippet of the output data: j 300 350 ... 101 0 0 ... 102 235.7 235.2 ... 103 2.6 2.6 ... 104 341.4 342.4 ... 105 0 0 ... ... ... ... ...","title":"Custom Data Generation"},{"location":"data_generate/#custom-data-generation","text":"","title":"Custom Data Generation"},{"location":"data_generate/#python-script-vs-modeller-tool","text":"In general Modeller tools more polished and can provide a GUI, but take more time and effort to prepare. They can still be called from other places and imported into other files. We generally use these for production grade, stable solutions, that we intend to use repeatedly. Object oriented. It is expected that everyone here knows how to use a modeller tool Python script can be built easily and often is enough to quickly complete a task at hand. Maybe fragile, can break with simple model changes. These may not be maintained. These can work will with the notebooks. They can just work with basic functions, easy to put together a basic but reproducible analysis.","title":"Python script vs modeller tool"},{"location":"data_generate/#executing-a-python-script-from-the-emme-ipython-shell","text":"Open the ipython modeller shell in EMME desktop Once in the shell, navigate to location of the subject script. In this case the script in question is located in the RTM/Scripts/Tools directory Check the files in this directory to ensure we have the one we want, then we can use the iPython magic %run <filename.py> to execute our script","title":"Executing a python script from the EMME iPython shell"},{"location":"data_generate/#example-tool-for-pa-to-od","text":"As a general rule in trip based modeling production/attraction (PA) trips can be converted to origin/destination (OD) trips roughly by 0.5 * (mat + mat') . However, for a variety of reasons, most notably non-home-based (NHB) trips, not everything is completely symmetrical. Production to attraction blending factors have been calculated from the trip diary for the RTM skims. The example runs the PA2OD.py python script with the output directed to the trip_summaries.db database. Once you have run the script, navigate to RTM/<your_db_folder> and open the trip_summaries.db in your sqlite viewer. Once there, we will see a new table named od_daily_<your ensemble if any> From there we can query and view the results","title":"Example Tool for PA to OD"},{"location":"data_generate/#why-does-it-run","text":"if __name__ == '__main__': eb = _m.Modeller().emmebank # run with ensemble name and ensem_agg = True to aggregate # run with no ensemble name and ensem_agg = False to get TAZ level results # note, TAZ level results create very large file main(eb, ensem='gy', ensem_agg=True) And why does this work? See this video on YouTube Name and Main","title":"Why does it run?"},{"location":"data_generate/#using-the-notebook","text":"The EMME Notebook is a powerful tool to generate results and summary data across many model runs. To demonstrate this, we took the base case model and varied the park and ride price on a particular park and ride lot. We run a separate model run for each park and ride price point. Then, we summarized the result across all of these model runs.","title":"Using the notebook"},{"location":"data_generate/#connect-to-emme-modeler","text":"When working with Jupyter Notebook within EMME, you need to import some basic libraries and toolboxes first, then connect to EMME desktop and databank. import inro.modeller as _m import inro.emme.desktop as _d import csv import os import multiprocessing import numpy as np import pandas as pd import sqlite3 import traceback as _traceback import shutil # connect to EMME dt = _m.Modeller().desktop de = dt.data_explorer() db = de.active_database() ebs = de.databases() # load toolbox util = _m.Modeller().tool(\"translink.util\")","title":"Connect to EMME Modeler"},{"location":"data_generate/#listing-databanks","text":"Within the Notebook scripting environment, you have access to all of the databank that is loaded in your project, you can list them all to verify. # make sure modeller is closed or it will print to the python console in there counter = 0 for eb in ebs: print counter, eb.title() counter += 1 0 Minimal Base Databank 1 pnr_bp_2016_250 2 pnr_bp_2016_300","title":"Listing databanks"},{"location":"data_generate/#build-functions","text":"To retrieve results from multiple runs, you should build a function to systematically get the results. In our example, we will get the lot usage at all park and ride lot, and export the results to csv. def get_all_pnr_usage(eb): scenario_tag = str(util.get_eb_path(eb).split('\\\\')[-1].split('_')[-1]) df = util.get_pd_ij_df(eb) df['price'] = scenario_tag df['pnr_usage'] = (util.get_matrix_numpy(eb, \"mf3000\").flatten() + util.get_matrix_numpy(eb, \"mf3001\").flatten() + util.get_matrix_numpy(eb, \"mf3002\").flatten()) / 2 df = df[(df['j'] >99) & (df['j'] < 999)] return (df) # initialize dataframe, ij from any eb all_pnr_usage = pd.DataFrame() # loop to get result for eb in ebs: title = eb.title() if title == 'Minimal Base Databank': continue eb.open() eb = _m.Modeller().emmebank if all_pnr_usage.empty: all_pnr_usage = get_all_pnr_usage(eb) else: all_pnr_usage = pd.concat([all_pnr_usage, get_all_pnr_usage(eb)], axis=0) all_pnr_usage_summary = all_pnr_usage.groupby( ['price', 'j']).sum()[['pnr_usage']].reset_index().pivot_table( values='pnr_usage', index='j', columns='price').fillna(0) # export result for all pnr lot usage import datetime all_pnr_usage_summary.to_csv( 'Bridgeport_pnr_all_lot_usage_result_' + datetime.datetime.today().strftime('%Y-%m-%d') + '.csv', index=True) Here is a snippet of the output data: j 300 350 ... 101 0 0 ... 102 235.7 235.2 ... 103 2.6 2.6 ... 104 341.4 342.4 ... 105 0 0 ... ... ... ... ...","title":"Build functions"},{"location":"data_outputs/","text":"Model Data Output \u00b6 Exporting full matrix directory \u00b6 The RTM produces a lot of data for each run, there are more than 1,000 full matrices at the end of a run. It's sometimes difficult to know what data is available afterwards. EMME desktop provides a full matrix directory, but it is not directly searchable. However if we open the worksheet located here: Worksheets/Tables/EMME Standard - GENERAL/General/Matrices/Matrix directory - Full matrices (mf) Clicking on the icon that looks like a database table opens the worksheet as a datatable. A datatable is effectively a sqlite database table. Click the save as button Removing spaces in the name simplifies later querying. The matrix directory will be saved at the project level in the RTM\\data_tables.db sqlite database, which we will query in the next section. Searching for matrices \u00b6 Now that we have the full matrix directory saved at the project level in the RTM\\data_tables.db sqlite database, we can use SQL to search for matrices that are of interest. Although there are a number of ways to access the database we recommend a sqlite viewer such as SQLite Studio Suppose we are interested in commute trips and want to find home-based work production attraction (PA) matrices. We can open the database in the sqlite viewer and execute the following query SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE '%HBW%' or UPPER([Name]) LIKE '%WORK%' or UPPER([Description]) LIKE '%HBW%' or UPPER([Description]) LIKE '%WORK%' Which yields the following results We get 164 records, but can see the matrices we want highlighted in the red box. Changing the WHERE clause returns only the results we want: SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE 'HBWP-A%' Now we get only 9 results, 3 income categories by 3 auto ownership levels Changing the WHERE clause as follows let's us see the PA tables for all home-based purposes SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE 'HB%P-A%' Note that we get 55 records. There are 7 home-based purposes, but home-based university does not have income and auto-ownership distinction so 6 x 9 + 1 = 55. Finally, we can add an 'N' to the WHERE clause to see the non-home-based purposes. SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE 'NH%P-A%' And here we get 2 records because there are 2 non-home-based purposes and they do not have income and auto-ownership distinction. Similar methods can be used to find other data of interest Using the rtm and trip summaries databases \u00b6 In addition to the matrices generated by EMME and stored in the databank during full model run, the RTM saves various input and output data related to the model run in two sqlite databases. The rtm.db database primarily store input data, intermediate values, or high-level model results useful for diagnosing model issues. You can view all the tables within this database with SQLite Studio . The trip_summaries.db database contains high-level results reported by GY (large multi-municipal groups for Metro Vancouver) and disaggregate-level results by network link and transit lines. Database queries \u00b6 SQLite databases are packaged into a single database file, so they are easy to create and share. Like other databases, users can run simple SQL queries to get quick answers about the data. To start writing queries, open query editor in SQLite Studio, then select the correct database to run the query from: Query example: Traffic zone with highest employment density \u00b6 If you are trying to get the max value from a table, use the max(colname) builtin function. SELECT max(empdens) FROM densities Results of the query would look something like this: max(empdens) 2821.884521484375 If you are trying to get the max value for a certain set of rows, for instance, a certain range from taz, add WHERE statement SELECT max(empdens) FROM densities WHERE TAZ1700>=10000 and TAZ1700<20000 Sample results of query: max(empdens) 222.649658203125 Notice how traffic zones in the 10000 series have lower maximum employment density than the entire region. Query example: Total vehicle-kilometers (VKT) by peak period and origin GY \u00b6 SQL is great at creating pivot tables from large dataset. The sum function computes total of values, and the group by statement define the variable which was used to aggregates the table values. SELECT gy_i, peak, sum(vkt) FROM autoVktGy GROUP BY gy_i, peak Sample results of query: gy_i peak sum(vkt) 1 am 106675.75589299179 1 md 65820.09947714038 1 pm 97089.90104259636 2 am 229276.9490799977 2 md 134214.06760361523 2 pm 212243.29537474972 3 am 71116.98403721345 3 md 84972.73319700224 ... ... ... Learn more about SQL .","title":"Model Data Output"},{"location":"data_outputs/#model-data-output","text":"","title":"Model Data Output"},{"location":"data_outputs/#exporting-full-matrix-directory","text":"The RTM produces a lot of data for each run, there are more than 1,000 full matrices at the end of a run. It's sometimes difficult to know what data is available afterwards. EMME desktop provides a full matrix directory, but it is not directly searchable. However if we open the worksheet located here: Worksheets/Tables/EMME Standard - GENERAL/General/Matrices/Matrix directory - Full matrices (mf) Clicking on the icon that looks like a database table opens the worksheet as a datatable. A datatable is effectively a sqlite database table. Click the save as button Removing spaces in the name simplifies later querying. The matrix directory will be saved at the project level in the RTM\\data_tables.db sqlite database, which we will query in the next section.","title":"Exporting full matrix directory"},{"location":"data_outputs/#searching-for-matrices","text":"Now that we have the full matrix directory saved at the project level in the RTM\\data_tables.db sqlite database, we can use SQL to search for matrices that are of interest. Although there are a number of ways to access the database we recommend a sqlite viewer such as SQLite Studio Suppose we are interested in commute trips and want to find home-based work production attraction (PA) matrices. We can open the database in the sqlite viewer and execute the following query SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE '%HBW%' or UPPER([Name]) LIKE '%WORK%' or UPPER([Description]) LIKE '%HBW%' or UPPER([Description]) LIKE '%WORK%' Which yields the following results We get 164 records, but can see the matrices we want highlighted in the red box. Changing the WHERE clause returns only the results we want: SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE 'HBWP-A%' Now we get only 9 results, 3 income categories by 3 auto ownership levels Changing the WHERE clause as follows let's us see the PA tables for all home-based purposes SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE 'HB%P-A%' Note that we get 55 records. There are 7 home-based purposes, but home-based university does not have income and auto-ownership distinction so 6 x 9 + 1 = 55. Finally, we can add an 'N' to the WHERE clause to see the non-home-based purposes. SELECT Matrix ,[Name] ,[Description] FROM Fullmatrixdirectory WHERE 1=1 and UPPER([Name]) LIKE 'NH%P-A%' And here we get 2 records because there are 2 non-home-based purposes and they do not have income and auto-ownership distinction. Similar methods can be used to find other data of interest","title":"Searching for matrices"},{"location":"data_outputs/#using-the-rtm-and-trip-summaries-databases","text":"In addition to the matrices generated by EMME and stored in the databank during full model run, the RTM saves various input and output data related to the model run in two sqlite databases. The rtm.db database primarily store input data, intermediate values, or high-level model results useful for diagnosing model issues. You can view all the tables within this database with SQLite Studio . The trip_summaries.db database contains high-level results reported by GY (large multi-municipal groups for Metro Vancouver) and disaggregate-level results by network link and transit lines.","title":"Using the rtm and trip summaries databases"},{"location":"data_outputs/#database-queries","text":"SQLite databases are packaged into a single database file, so they are easy to create and share. Like other databases, users can run simple SQL queries to get quick answers about the data. To start writing queries, open query editor in SQLite Studio, then select the correct database to run the query from:","title":"Database queries"},{"location":"data_outputs/#query-example-traffic-zone-with-highest-employment-density","text":"If you are trying to get the max value from a table, use the max(colname) builtin function. SELECT max(empdens) FROM densities Results of the query would look something like this: max(empdens) 2821.884521484375 If you are trying to get the max value for a certain set of rows, for instance, a certain range from taz, add WHERE statement SELECT max(empdens) FROM densities WHERE TAZ1700>=10000 and TAZ1700<20000 Sample results of query: max(empdens) 222.649658203125 Notice how traffic zones in the 10000 series have lower maximum employment density than the entire region.","title":"Query example: Traffic zone with highest employment density"},{"location":"data_outputs/#query-example-total-vehicle-kilometers-vkt-by-peak-period-and-origin-gy","text":"SQL is great at creating pivot tables from large dataset. The sum function computes total of values, and the group by statement define the variable which was used to aggregates the table values. SELECT gy_i, peak, sum(vkt) FROM autoVktGy GROUP BY gy_i, peak Sample results of query: gy_i peak sum(vkt) 1 am 106675.75589299179 1 md 65820.09947714038 1 pm 97089.90104259636 2 am 229276.9490799977 2 md 134214.06760361523 2 pm 212243.29537474972 3 am 71116.98403721345 3 md 84972.73319700224 ... ... ... Learn more about SQL .","title":"Query example: Total vehicle-kilometers (VKT) by peak period and origin GY"},{"location":"dev_getting_started/","text":"Getting Started \u00b6 Setup \u00b6 We recommend Visual Studio Code for RTM development. It comes with capability to debug code, perform git commands, linting, code formatting, and more. Before starting your development environment set up, make sure you have all of the requirements specified in the User Guide's Requirements . Once all the requirements are fulfilled, install Visual Studio Code , then set up your environment with Formatter and Linter. Formatter and Linter \u00b6 Step 1: install pylint \u00b6 Please visit https://marketplace.visualstudio.com/items?itemName=ms-python.pylint , and open the extension page through Visual Studio Code. Typically, pylint will automatically install PyLance - Python language server, if not, please install manually via this link: https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance Step 2: configure formatting setting for VS Code \u00b6 Please visit https://marketplace.visualstudio.com/items?itemName=ms-python.pylint , and open the extension page through Visual Studio Code. Optional: advanced settings \u00b6 You may customize behavior of VS Code using settings.json . For windows OS, it's typically located in %APPDATA%\\Code\\User\\settings.json This is our recommended settings.json values, you may add more settings by referencing documentation for VS Code and its extensions: { \"python.languageServer\": \"Pylance\", \"pylint.lintOnChange\": true, \"pylint.args\": [ \"--ignored-modules=inro,inro.emme.desktop,openmatrix\", \"--ignored-classes=inro,inro.emme.desktop,openmatrix\", \"--extension-pkg-whitelist=inro,inro.emme.desktop,openmatrix\", \"--disable=C,E1101\", \"--max-line-length=120\" ], \"[python]\": { \"editor.defaultFormatter\": \"ms-python.black-formatter\", \"editor.formatOnSave\": true } } You may specify and save specific changes for your set up using setting files for VS Code: * For location of VS Code settings files, see VS Code Documentation on File Locations . * For references to VS Code settings, see VS Code Documentation on Settings Reference . * You may add other editor behavior such as \"editor.formatOnSave\": true, as per your reference.","title":"Getting Started"},{"location":"dev_getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"dev_getting_started/#setup","text":"We recommend Visual Studio Code for RTM development. It comes with capability to debug code, perform git commands, linting, code formatting, and more. Before starting your development environment set up, make sure you have all of the requirements specified in the User Guide's Requirements . Once all the requirements are fulfilled, install Visual Studio Code , then set up your environment with Formatter and Linter.","title":"Setup"},{"location":"dev_getting_started/#formatter-and-linter","text":"","title":"Formatter and Linter"},{"location":"dev_getting_started/#step-1-install-pylint","text":"Please visit https://marketplace.visualstudio.com/items?itemName=ms-python.pylint , and open the extension page through Visual Studio Code. Typically, pylint will automatically install PyLance - Python language server, if not, please install manually via this link: https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance","title":"Step 1: install pylint"},{"location":"dev_getting_started/#step-2-configure-formatting-setting-for-vs-code","text":"Please visit https://marketplace.visualstudio.com/items?itemName=ms-python.pylint , and open the extension page through Visual Studio Code.","title":"Step 2: configure formatting setting for VS Code"},{"location":"dev_getting_started/#optional-advanced-settings","text":"You may customize behavior of VS Code using settings.json . For windows OS, it's typically located in %APPDATA%\\Code\\User\\settings.json This is our recommended settings.json values, you may add more settings by referencing documentation for VS Code and its extensions: { \"python.languageServer\": \"Pylance\", \"pylint.lintOnChange\": true, \"pylint.args\": [ \"--ignored-modules=inro,inro.emme.desktop,openmatrix\", \"--ignored-classes=inro,inro.emme.desktop,openmatrix\", \"--extension-pkg-whitelist=inro,inro.emme.desktop,openmatrix\", \"--disable=C,E1101\", \"--max-line-length=120\" ], \"[python]\": { \"editor.defaultFormatter\": \"ms-python.black-formatter\", \"editor.formatOnSave\": true } } You may specify and save specific changes for your set up using setting files for VS Code: * For location of VS Code settings files, see VS Code Documentation on File Locations . * For references to VS Code settings, see VS Code Documentation on Settings Reference . * You may add other editor behavior such as \"editor.formatOnSave\": true, as per your reference.","title":"Optional: advanced settings"},{"location":"matrix_list/","text":"Matrix Name Description mf70 extSovAm External Demand SOV AM mf71 extHovAm External Demand HOV AM mf75 extSovMd External Demand SOV MD mf76 extHovMd External Demand HOV MD mf80 extSovPm External Demand SOV PM mf81 extHovPm External Demand HOV PM mf85 MD_Demadj MD Incr Calibration Matrix mf90 bikeskim Weighted Average IJ bike score mf91 distAON All or nothing initial distance skim mf92 Bridge_pen_AM Bridge_pen_AM mf93 Bridge_pen_PM Bridge_pen_PM mf95 fare_zones Fare Zones Travelled mf200 SOV_pertrp_VOT_1_Am SOV per-trips VOT 1 AM mf201 SOV_pertrp_VOT_2_Am SOV per-trips VOT 2 AM mf202 SOV_pertrp_VOT_3_Am SOV per-trips VOT 3 AM mf203 SOV_pertrp_VOT_4_Am SOV per-trips VOT 4 AM mf206 HOV_pertrp_VOT_1_Am HOV per-trips VOT 1 AM mf207 HOV_pertrp_VOT_2_Am HOV per-trips VOT 2 AM mf208 HOV_pertrp_VOT_3_Am HOV per-trips VOT 3 AM mf209 HOV_pertrp_VOT_4_Am HOV per-trips VOT 4 AM mf215 Wk_pertrp_Am Walk per-trips AM mf216 Bk_pertrp_Am Bike per-trips AM mf230 SOV_pertrp_VOT_1_Md SOV per-trips VOT 1 MD mf231 SOV_pertrp_VOT_2_Md SOV per-trips VOT 2 MD mf232 SOV_pertrp_VOT_3_Md SOV per-trips VOT 3 MD mf233 SOV_pertrp_VOT_4_Md SOV per-trips VOT 4 MD mf236 HOV_pertrp_VOT_1_Md HOV per-trips VOT 1 MD mf237 HOV_pertrp_VOT_2_Md HOV per-trips VOT 2 MD mf238 HOV_pertrp_VOT_3_Md HOV per-trips VOT 3 MD mf239 HOV_pertrp_VOT_4_Md HOV per-trips VOT 4 MD mf245 Wk_pertrp_Md Walk per-trips MD mf246 Bk_pertrp_Md Bike per-trips MD mf260 SOV_pertrp_VOT_1_Pm SOV per-trips VOT 1 PM mf261 SOV_pertrp_VOT_2_Pm SOV per-trips VOT 2 PM mf262 SOV_pertrp_VOT_3_Pm SOV per-trips VOT 3 PM mf263 SOV_pertrp_VOT_4_Pm SOV per-trips VOT 4 PM mf266 HOV_pertrp_VOT_1_Pm HOV per-trips VOT 1 PM mf267 HOV_pertrp_VOT_2_Pm HOV per-trips VOT 2 PM mf268 HOV_pertrp_VOT_3_Pm HOV per-trips VOT 3 PM mf269 HOV_pertrp_VOT_4_Pm HOV per-trips VOT 4 PM mf275 Wk_pertrp_Pm Walk per-trips PM mf276 Bk_pertrp_Pm Bike per-trips PM mf300 SOV_drvtrp_VOT_1_Am SOV drv-trips VOT 1 AM mf301 SOV_drvtrp_VOT_2_Am SOV drv-trips VOT 2 AM mf302 SOV_drvtrp_VOT_3_Am SOV drv-trips VOT 3 AM mf303 SOV_drvtrp_VOT_4_Am SOV drv-trips VOT 4 AM mf306 HOV_drvtrp_VOT_1_Am HOV drv-trips VOT 1 AM mf307 HOV_drvtrp_VOT_2_Am HOV drv-trips VOT 2 AM mf308 HOV_drvtrp_VOT_3_Am HOV drv-trips VOT 3 AM mf309 HOV_drvtrp_VOT_4_Am HOV drv-trips VOT 4 AM mf312 lgvPceAm light trucks PCE AM mf313 hgvPceAm heavy trucks PCE AM mf314 busAm Bus Person Trips AM mf315 railAm Rail Person Trips AM mf316 WCEAm WCE Person Trips AM mf320 SOV_drvtrp_VOT_1_Md SOV drv-trips VOT 1 MD mf321 SOV_drvtrp_VOT_2_Md SOV drv-trips VOT 2 MD mf322 SOV_drvtrp_VOT_3_Md SOV drv-trips VOT 3 MD mf323 SOV_drvtrp_VOT_4_Md SOV drv-trips VOT 4 MD mf326 HOV_drvtrp_VOT_1_Md HOV drv-trips VOT 1 MD mf327 HOV_drvtrp_VOT_2_Md HOV drv-trips VOT 2 MD mf328 HOV_drvtrp_VOT_3_Md HOV drv-trips VOT 3 MD mf329 HOV_drvtrp_VOT_4_Md HOV drv-trips VOT 4 MD mf332 lgvPceMd light trucks PCE MD mf333 hgvPceMd heavy trucks PCE MD mf334 busMd Bus Person Trips MD mf335 railMd Rail Person Trips MD mf336 WCEMd WCE Person Trips MD mf340 SOV_drvtrp_VOT_1_Pm SOV drv-trips VOT 1 PM mf341 SOV_drvtrp_VOT_2_Pm SOV drv-trips VOT 2 PM mf342 SOV_drvtrp_VOT_3_Pm SOV drv-trips VOT 3 PM mf343 SOV_drvtrp_VOT_4_Pm SOV drv-trips VOT 4 PM mf346 HOV_drvtrp_VOT_1_Pm HOV drv-trips VOT 1 PM mf347 HOV_drvtrp_VOT_2_Pm HOV drv-trips VOT 2 PM mf348 HOV_drvtrp_VOT_3_Pm HOV drv-trips VOT 3 PM mf349 HOV_drvtrp_VOT_4_Pm HOV drv-trips VOT 4 PM mf352 lgvPcePm light trucks PCE PM mf353 hgvPcePm heavy trucks PCE PM mf354 busPm Bus Person Trips PM mf355 railPm Rail Person Trips PM mf356 WCEPm WCE Person Trips PM mf3000 HbWSOVI1PerTrips HbW SOV Low Income Per-Trips mf3001 HbWSOVI2PerTrips HbW SOV Med Income Per-Trips mf3002 HbWSOVI3PerTrips HbW SOV High Income Per-Trips mf3005 HbWHV2I1PerTrips HbW HV2 Low Income Per-Trips mf3006 HbWHV2I2PerTrips HbW HV2 Med Income Per-Trips mf3007 HbWHV2I3PerTrips HbW HV2 High Income Per-Trips mf3010 HbWHV3+I1PerTrips HbW HV3+ Low Income Per-Trips mf3011 HbWHV3+I2PerTrips HbW HV3+ Med Income Per-Trips mf3012 HbWHV3+I3PerTrips HbW HV3+ High Income Per-Trips mf3015 HbWBusPerTrips HbW Bus Per-Trips mf3020 HbWRailPerTrips HbW Rail Per-Trips mf3025 HbWWCEPerTrips HbW WCE Per-Trips mf3030 HbWWalkPerTrips HbW Walk Per-Trips mf3035 HbWBikePerTrips HbW Bike Per-Trips mf3050 HbWP-AI1A0 HbW P-A Trips I1 A0 mf3051 HbWP-AI1A1 HbW P-A Trips I1 A1 mf3052 HbWP-AI1A2 HbW P-A Trips I1 A2 mf3053 HbWP-AI2A0 HbW P-A Trips I1 A0 mf3054 HbWP-AI2A1 HbW P-A Trips I1 A1 mf3055 HbWP-AI2A2 HbW P-A Trips I1 A2 mf3056 HbWP-AI3A0 HbW P-A Trips I1 A0 mf3057 HbWP-AI3A1 HbW P-A Trips I1 A1 mf3058 HbWP-AI3A2 HbW P-A Trips I1 A2 mf3100 HbUSOVPerTrips HbU SOV Per-Trips mf3105 HbUHOVPerTrips HbU HOV Per-Trips mf3115 HbUBusPerTrips HbU Bus Per-Trips mf3120 HbURailPerTrips HbU Rail Per-Trips mf3130 HbUWalkPerTrips HbU Walk Per-Trips mf3135 HbUBikePerTrips HbU Bike Per-Trips mf3150 HbUP-A HbU P-A Trips mf3205 HbScHOVI1PerTrips HbSc HV2+ Low Income Per-Trips mf3206 HbScHOVI2PerTrips HbSc HV2+ Med Income Per-Trips mf3207 HbScHOVI3PerTrips HbSc HV2+ High Income Per-Trips mf3215 HbScBusPerTrips HbSc Bus Per-Trips mf3220 HbScRailPerTrips HbSc Rail Per-Trips mf3230 HbScWalkPerTrips HbSc Walk Per-Trips mf3235 HbScBikePerTrips HbSc Bike Per-Trips mf3250 HbScP-AI1A0 HbSc P-A Trips I1 A0 mf3251 HbScP-AI1A1 HbSc P-A Trips I1 A1 mf3252 HbScP-AI1A2 HbSc P-A Trips I1 A2 mf3253 HbScP-AI2A0 HbSc P-A Trips I1 A0 mf3254 HbScP-AI2A1 HbSc P-A Trips I1 A1 mf3255 HbScP-AI2A2 HbSc P-A Trips I1 A2 mf3256 HbScP-AI3A0 HbSc P-A Trips I1 A0 mf3257 HbScP-AI3A1 HbSc P-A Trips I1 A1 mf3258 HbScP-AI3A2 HbSc P-A Trips I1 A2 mf3300 HbShSOVI1PerTrips HbSh SOV Low Income Per-Trips mf3301 HbShSOVI2PerTrips HbSh SOV Med Income Per-Trips mf3302 HbShSOVI3PerTrips HbSh SOV High Income Per-Trips mf3305 HbShHOVI1PerTrips HbSh HOV Low Income Per-Trips mf3306 HbShHOVI2PerTrips HbSh HOV Med Income Per-Trips mf3307 HbShHOVI3PerTrips HbSh HOV High Income Per-Trips mf3315 HbShBusPerTrips HbSh Bus Per-Trips mf3320 HbShRailPerTrips HbSh Rail Per-Trips mf3330 HbShWalkPerTrips HbSh Walk Per-Trips mf3335 HbShBikePerTrips HbSh Bike Per-Trips mf3350 HbShP-AI1A0 HbSh P-A Trips I1 A0 mf3351 HbShP-AI1A1 HbSh P-A Trips I1 A1 mf3352 HbShP-AI1A2 HbSh P-A Trips I1 A2 mf3353 HbShP-AI2A0 HbSh P-A Trips I1 A0 mf3354 HbShP-AI2A1 HbSh P-A Trips I1 A1 mf3355 HbShP-AI2A2 HbSh P-A Trips I1 A2 mf3356 HbShP-AI3A0 HbSh P-A Trips I1 A0 mf3357 HbShP-AI3A1 HbSh P-A Trips I1 A1 mf3358 HbShP-AI3A2 HbSh P-A Trips I1 A2 mf3400 HbPbSOVI1PerTrips HbPb SOV Low Income Per-Trips mf3401 HbPbSOVI2PerTrips HbPb SOV Med Income Per-Trips mf3402 HbPbSOVI3PerTrips HbPb SOV High Income Per-Trips mf3405 HbPbHOVI1PerTrips HbPb HOV Low Income Per-Trips mf3406 HbPbHOVI2PerTrips HbPb HOV Med Income Per-Trips mf3407 HbPbHOVI3PerTrips HbPb HOV High Income Per-Trips mf3415 HbPbBusPerTrips HbPb Bus Per-Trips mf3420 HbPbRailPerTrips HbPb Rail Per-Trips mf3430 HbPbWalkPerTrips HbPb Walk Per-Trips mf3435 HbPbBikePerTrips HbPb Bike Per-Trips mf3450 HbPbP-AI1A0 HbPb P-A Trips I1 A0 mf3451 HbPbP-AI1A1 HbPb P-A Trips I1 A1 mf3452 HbPbP-AI1A2 HbPb P-A Trips I1 A2 mf3453 HbPbP-AI2A0 HbPb P-A Trips I1 A0 mf3454 HbPbP-AI2A1 HbPb P-A Trips I1 A1 mf3455 HbPbP-AI2A2 HbPb P-A Trips I1 A2 mf3456 HbPbP-AI3A0 HbPb P-A Trips I1 A0 mf3457 HbPbP-AI3A1 HbPb P-A Trips I1 A1 mf3458 HbPbP-AI3A2 HbPb P-A Trips I1 A2 mf3500 HbSoSOVI1PerTrips HbSo SOV Low Income Per-Trips mf3501 HbSoSOVI2PerTrips HbSo SOV Med Income Per-Trips mf3502 HbSoSOVI3PerTrips HbSo SOV High Income Per-Trips mf3505 HbSoHOVI1PerTrips HbSo HOV Low Income Per-Trips mf3506 HbSoHOVI2PerTrips HbSo HOV Med Income Per-Trips mf3507 HbSoHOVI3PerTrips HbSo HOV High Income Per-Trips mf3515 HbSoBusPerTrips HbSo Bus Per-Trips mf3520 HbSoRailPerTrips HbSo Rail Per-Trips mf3530 HbSoWalkPerTrips HbSo Walk Per-Trips mf3535 HbSoBikePerTrips HbSo Bike Per-Trips mf3550 HbSoP-AI1A0 HbSo P-A Trips I1 A0 mf3551 HbSoP-AI1A1 HbSo P-A Trips I1 A1 mf3552 HbSoP-AI1A2 HbSo P-A Trips I1 A2 mf3553 HbSoP-AI2A0 HbSo P-A Trips I1 A0 mf3554 HbSoP-AI2A1 HbSo P-A Trips I1 A1 mf3555 HbSoP-AI2A2 HbSo P-A Trips I1 A2 mf3556 HbSoP-AI3A0 HbSo P-A Trips I1 A0 mf3557 HbSoP-AI3A1 HbSo P-A Trips I1 A1 mf3558 HbSoP-AI3A2 HbSo P-A Trips I1 A2 mf3600 HbEsSOVI1PerTrips HbEs SOV Low Income Per-Trips mf3601 HbEsSOVI2PerTrips HbEs SOV Med Income Per-Trips mf3602 HbEsSOVI3PerTrips HbEs SOV High Income Per-Trips mf3605 HbEsHOVI1PerTrips HbEs HOV Low Income Per-Trips mf3606 HbEsHOVI2PerTrips HbEs HOV Med Income Per-Trips mf3607 HbEsHOVI3PerTrips HbEs HOV High Income Per-Trips mf3615 HbEsBusPerTrips HbEs Bus Per-Trips mf3620 HbEsRailPerTrips HbEs Rail Per-Trips mf3630 HbEsWalkPerTrips HbEs Walk Per-Trips mf3635 HbEsBikePerTrips HbEs Bike Per-Trips mf3650 HbEsP-AI1A0 HbEs P-A Trips I1 A0 mf3651 HbEsP-AI1A1 HbEs P-A Trips I1 A1 mf3652 HbEsP-AI1A2 HbEs P-A Trips I1 A2 mf3653 HbEsP-AI2A0 HbEs P-A Trips I1 A0 mf3654 HbEsP-AI2A1 HbEs P-A Trips I1 A1 mf3655 HbEsP-AI2A2 HbEs P-A Trips I1 A2 mf3656 HbEsP-AI3A0 HbEs P-A Trips I1 A0 mf3657 HbEsP-AI3A1 HbEs P-A Trips I1 A1 mf3658 HbEsP-AI3A2 HbEs P-A Trips I1 A2 mf3700 NHbWSOVPerTrips NHbW SOV Per-Trips mf3705 NHbWHOVPerTrips NHbW HOV Per-Trips mf3715 NHbWBusPerTrips NHbW Bus Per-Trips mf3720 NHbWRailPerTrips NHbW Rail Per-Trips mf3730 NHbWWalkPerTrips NHbW Walk Per-Trips mf3735 NHbWBikePerTrips NHbW Bike Per-Trips mf3750 NHbWP-A NHbW P-A Trips mf3800 NHbOSOVPerTrips NHbO SOV Per-Trips mf3805 NHbOHOVPerTrips NHbO HOV Per-Trips mf3815 NHbOBusPerTrips NHbO Bus Per-Trips mf3820 NHbORailPerTrips NHbO Rail Per-Trips mf3830 NHbOWalkPerTrips NHbO Walk Per-Trips mf3835 NHbOBikePerTrips NHbO Bike Per-Trips mf3850 NHbOP-A NHbO P-A Trips mf5000 AmSovOpCstVOT1 Am Sov VOT1 Op Cost mf5001 AmSovTimeVOT1 Am Sov VOT1 Time mf5003 AmSovOpCstVOT2 Am Sov VOT2 Op Cost mf5004 AmSovTimeVOT2 Am Sov VOT2 Time mf5006 AmSovOpCstVOT3 Am Sov VOT3 Op Cost mf5007 AmSovTimeVOT3 Am Sov VOT3 Time mf5009 AmSovOpCstVOT4 Am Sov VOT4 Op Cost mf5010 AmSovTimeVOT4 Am Sov VOT4 Time mf5012 AmHovOpCstVOT1 Am Hov VOT1 Op Cost mf5013 AmHovTimeVOT1 Am Hov VOT1 Time mf5015 AmHovOpCstVOT2 Am Hov VOT2 Op Cost mf5016 AmHovTimeVOT2 Am Hov VOT2 Time mf5018 AmHovOpCstVOT3 Am Hov VOT3 Op Cost mf5019 AmHovTimeVOT3 Am Hov VOT3 Time mf5021 AmHovOpCstVOT4 Am Hov VOT4 Op Cost mf5022 AmHovTimeVOT4 Am Hov VOT4 Time mf5024 AmLgvOpCst Am LGV Op Cost mf5025 AmLgvTime Am LGV Time mf5027 AmHgvOpCst Am HGV Op Cost mf5028 AmHgvTime Am HGV Time mf5030 MdSovOpCstVOT1 Md Sov VOT1 Op Cost mf5031 MdSovTimeVOT1 Md Sov VOT1 Time mf5033 MdSovOpCstVOT2 Md Sov VOT2 Op Cost mf5034 MdSovTimeVOT2 Md Sov VOT2 Time mf5036 MdSovOpCstVOT3 Md Sov VOT3 Op Cost mf5037 MdSovTimeVOT3 Md Sov VOT3 Time mf5039 MdSovOpCstVOT4 Md Sov VOT4 Op Cost mf5040 MdSovTimeVOT4 Md Sov VOT4 Time mf5042 MdHovOpCstVOT1 Md Hov VOT1 Op Cost mf5043 MdHovTimeVOT1 Md Hov VOT1 Time mf5045 MdHovOpCstVOT2 Md Hov VOT2 Op Cost mf5046 MdHovTimeVOT2 Md Hov VOT2 Time mf5048 MdHovOpCstVOT3 Md Hov VOT3 Op Cost mf5049 MdHovTimeVOT3 Md Hov VOT3 Time mf5051 MdHovOpCstVOT4 Md Hov VOT4 Op Cost mf5052 MdHovTimeVOT4 Md Hov VOT4 Time mf5054 MdLgvOpCst Md LGV Op Cost mf5055 MdLgvTime Md LGV Time mf5057 MdHgvOpCst Md HGV Op Cost mf5058 MdHgvTime Md HGV Time mf5060 PmSovOpCstVOT1 Pm Sov VOT1 Op Cost mf5061 PmSovTimeVOT1 Pm Sov VOT1 Time mf5063 PmSovOpCstVOT2 Pm Sov VOT2 Op Cost mf5064 PmSovTimeVOT2 Pm Sov VOT2 Time mf5066 PmSovOpCstVOT3 Pm Sov VOT3 Op Cost mf5067 PmSovTimeVOT3 Pm Sov VOT3 Time mf5069 PmSovOpCstVOT4 Pm Sov VOT4 Op Cost mf5070 PmSovTimeVOT4 Pm Sov VOT4 Time mf5072 PmHovOpCstVOT1 Pm Hov VOT1 Op Cost mf5073 PmHovTimeVOT1 Pm Hov VOT1 Time mf5075 PmHovOpCstVOT2 Pm Hov VOT2 Op Cost mf5076 PmHovTimeVOT2 Pm Hov VOT2 Time mf5078 PmHovOpCstVOT3 Pm Hov VOT3 Op Cost mf5079 PmHovTimeVOT3 Pm Hov VOT3 Time mf5081 PmHovOpCstVOT4 Pm Hov VOT4 Op Cost mf5082 PmHovTimeVOT4 Pm Hov VOT4 Time mf5084 PmLgvOpCst Pm LGV Op Cost mf5085 PmLgvTime Pm LGV Time mf5087 PmHgvOpCst Pm HGV Op Cost mf5088 PmHgvTime Pm HGV Time mf5100 HbWBlSovCost_I1 HbW Bl Sov Cost_Low_Income mf5101 HbWBlSovTime_I1 HbW Bl Sov Time_Low_Income mf5102 HbWBlSovToll_I1 HbW Bl Sov Toll_Low_Income mf5103 HbWBlSovCost_I2 HbW Bl Sov Cost_Med_Income mf5104 HbWBlSovTime_I2 HbW Bl Sov Time_Med_Income mf5105 HbWBlSovToll_I2 HbW Bl Sov Toll_Med_Income mf5106 HbWBlSovCost_I3 HbW Bl Sov Cost_High_Income mf5107 HbWBlSovTime_I3 HbW Bl Sov Time_High_Income mf5108 HbWBlSovToll_I3 HbW Bl Sov Toll_High_Income mf5110 HbWBlHovCost_I1 HbW Bl Hov Cost_Low_Income mf5111 HbWBlHovTime_I1 HbW Bl Hov Time_Low_Income mf5112 HbWBlHovToll_I1 HbW Bl Hov Toll_Low_Income mf5113 HbWBlHovCost_I2 HbW Bl Hov Cost_Med_Income mf5114 HbWBlHovTime_I2 HbW Bl Hov Time_Med_Income mf5115 HbWBlHovToll_I2 HbW Bl Hov Toll_Med_Income mf5116 HbWBlHovCost_I3 HbW Bl Hov Cost_High_Income mf5117 HbWBlHovTime_I3 HbW Bl Hov Time_High_Income mf5118 HbWBlHovToll_I3 HbW Bl Hov Toll_High_Income mf5120 HbShBlSovCost_I1 HbSh Bl Sov Cost_Low_Income mf5121 HbShBlSovTime_I1 HbSh Bl Sov Time_Low_Income mf5122 HbShBlSovToll_I1 HbSh Bl Sov Toll_Low_Income mf5123 HbShBlSovCost_I2 HbSh Bl Sov Cost_Med_Income mf5124 HbShBlSovTime_I2 HbSh Bl Sov Time_Med_Income mf5125 HbShBlSovToll_I2 HbSh Bl Sov Toll_Med_Income mf5126 HbShBlSovCost_I3 HbSh Bl Sov Cost_High_Income mf5127 HbShBlSovTime_I3 HbSh Bl Sov Time_High_Income mf5128 HbShBlSovToll_I3 HbSh Bl Sov Toll_High_Income mf5130 HbShBlHovCost_I1 HbSh Bl Hov Cost_Low_Income mf5131 HbShBlHovTime_I1 HbSh Bl Hov Time_Low_Income mf5132 HbShBlHovToll_I1 HbSh Bl Hov Toll_Low_Income mf5133 HbShBlHovCost_I2 HbSh Bl Hov Cost_Med_Income mf5134 HbShBlHovTime_I2 HbSh Bl Hov Time_Med_Income mf5135 HbShBlHovToll_I2 HbSh Bl Hov Toll_Med_Income mf5136 HbShBlHovCost_I3 HbSh Bl Hov Cost_High_Income mf5137 HbShBlHovTime_I3 HbSh Bl Hov Time_High_Income mf5138 HbShBlHovToll_I3 HbSh Bl Hov Toll_High_Income mf5139 HbShBl_BPen HbShopping Bridge Penalty mf5140 HbPbBlSovCost_I1 HbPb Bl Sov Cost_Low_Income mf5141 HbPbBlSovTime_I1 HbPb Bl Sov Time_Low_Income mf5142 HbPbBlSovToll_I1 HbPb Bl Sov Toll_Low_Income mf5143 HbPbBlSovCost_I2 HbPb Bl Sov Cost_Med_Income mf5144 HbPbBlSovTime_I2 HbPb Bl Sov Time_Med_Income mf5145 HbPbBlSovToll_I2 HbPb Bl Sov Toll_Med_Income mf5146 HbPbBlSovCost_I3 HbPb Bl Sov Cost_High_Income mf5147 HbPbBlSovTime_I3 HbPb Bl Sov Time_High_Income mf5148 HbPbBlSovToll_I3 HbPb Bl Sov Toll_High_Income mf5150 HbPbBlHovCost_I1 HbPb Bl Hov Cost_Low_Income mf5151 HbPbBlHovTime_I1 HbPb Bl Hov Time_Low_Income mf5152 HbPbBlHovToll_I1 HbPb Bl Hov Toll_Low_Income mf5153 HbPbBlHovCost_I2 HbPb Bl Hov Cost_Med_Income mf5154 HbPbBlHovTime_I2 HbPb Bl Hov Time_Med_Income mf5155 HbPbBlHovToll_I2 HbPb Bl Hov Toll_Med_Income mf5156 HbPbBlHovCost_I3 HbPb Bl Hov Cost_High_Income mf5157 HbPbBlHovTime_I3 HbPb Bl Hov Time_High_Income mf5158 HbPbBlHovToll_I3 HbPb Bl Hov Toll_High_Income mf5159 HbPbBl_BPen HbPerBus Bridge Penalty mf5160 HbSoBlSovCost_I1 HbSo Bl Sov Cost_Low_Income mf5161 HbSoBlSovTime_I1 HbSo Bl Sov Time_Low_Income mf5162 HbSoBlSovToll_I1 HbSo Bl Sov Toll_Low_Income mf5163 HbSoBlSovCost_I2 HbSo Bl Sov Cost_Med_Income mf5164 HbSoBlSovTime_I2 HbSo Bl Sov Time_Med_Income mf5165 HbSoBlSovToll_I2 HbSo Bl Sov Toll_Med_Income mf5166 HbSoBlSovCost_I3 HbSo Bl Sov Cost_High_Income mf5167 HbSoBlSovTime_I3 HbSo Bl Sov Time_High_Income mf5168 HbSoBlSovToll_I3 HbSo Bl Sov Toll_High_Income mf5170 HbSoBlHovCost_I1 HbSo Bl Hov Cost_Low_Income mf5171 HbSoBlHovTime_I1 HbSo Bl Hov Time_Low_Income mf5172 HbSoBlHovToll_I1 HbSo Bl Hov Toll_Low_Income mf5173 HbSoBlHovCost_I2 HbSo Bl Hov Cost_Med_Income mf5174 HbSoBlHovTime_I2 HbSo Bl Hov Time_Med_Income mf5175 HbSoBlHovToll_I2 HbSo Bl Hov Toll_Med_Income mf5176 HbSoBlHovCost_I3 HbSo Bl Hov Cost_High_Income mf5177 HbSoBlHovTime_I3 HbSo Bl Hov Time_High_Income mf5178 HbSoBlHovToll_I3 HbSo Bl Hov Toll_High_Income mf5179 HbSoBl_BPen HbSocial Bridge Penalty mf5200 HbUBlSovCost HbU Bl Sov Cost mf5201 HbUBlSovTime HbU Bl Sov Time mf5202 HbUBlSovToll HbU Bl Sov Toll mf5205 HbUBlHovCost HbU Bl Hov Cost mf5206 HbUBlHovTime HbU Bl Hov Time mf5207 HbUBlHovToll HbU Bl Hov Toll mf5210 HbScBlSovCost HbSc Bl Sov Cost mf5211 HbScBlSovTime HbSc Bl Sov Time mf5212 HbScBlSovToll HbSc Bl Sov Toll mf5215 HbScBlHovCost HbSc Bl Hov Cost mf5216 HbScBlHovTime HbSc Bl Hov Time mf5217 HbScBlHovToll HbSc Bl Hov Toll mf5219 HbScBl_BPen HbSchool Bridge Penalty mf5220 HbEsBlSovCost HbEs Bl Sov Cost mf5221 HbEsBlSovTime HbEs Bl Sov Time mf5222 HbEsBlSovToll HbEs Bl Sov Toll mf5225 HbEsBlHovCost HbEs Bl Hov Cost mf5226 HbEsBlHovTime HbEs Bl Hov Time mf5227 HbEsBlHovToll HbEs Bl Hov Toll mf5229 HbEsBl_BPen HbEscort Bridge Penalty mf5230 NHbWBlSovCost NHbW Bl Sov Cost mf5231 NHbWBlSovTime NHbW Bl Sov Time mf5232 NHbWBlSovToll NHbW Bl Sov Toll mf5235 NHbWBlHovCost NHbW Bl Hov Cost mf5236 NHbWBlHovTime NHbW Bl Hov Time mf5237 NHbWBlHovToll NHbW Bl Hov Toll mf5240 NHbOBlSovCost NHbO Bl Sov Cost mf5241 NHbOBlSovTime NHbO Bl Sov Time mf5242 NHbOBlSovToll NHbO Bl Sov Toll mf5245 NHbOBlHovCost NHbO Bl Hov Cost mf5246 NHbOBlHovTime NHbO Bl Hov Time mf5247 NHbOBlHovToll NHbO Bl Hov Toll mf5249 NHbOBl_BPen NHbO Bridge Penalty mf5300 AmBusIvtt Am Bus InVehicle Time mf5301 AmBusWait Am Bus Waiting Time mf5302 AmBusAux Am Bus Auxillary Time mf5303 AmBusBoard Am Bus Boardings mf5304 AmBusFare Am Bus Fare mf5305 AmBusIvttBRT Am Bus BRT InVehicle Time mf5310 MdBusIvtt Md Bus InVehicle Time mf5311 MdBusWait Md Bus Waiting Time mf5312 MdBusAux Md Bus Auxillary Time mf5313 MdBusBoard Md Bus Boardings mf5314 MdBusFare Md Bus Fare mf5315 MdBusIvttBRT Md Bus BRT InVehicle Time mf5320 PmBusIvtt Pm Bus InVehicle Time mf5321 PmBusWait Pm Bus Waiting Time mf5322 PmBusAux Pm Bus Auxillary Time mf5323 PmBusBoard Pm Bus Boardings mf5324 PmBusFare Pm Bus Fare mf5325 PmBusIvttBRT Pm Bus BRT InVehicle Time mf5400 HbWBlBusIvtt HbW Bl Bus InVehicle Time mf5401 HbWBlBusWait HbW Bl Bus Waiting Time mf5402 HbWBlBusAux HbW Bl Bus Auxillary Time mf5403 HbWBlBusBoard HbW Bl Bus Boardings mf5404 HbWBlBusFare HbW Bl Bus Fare mf5405 HbWBlBusIvttBRT HbW Bl Bus BRT InVehicle Time mf5410 HbUBlBusIvtt HbU Bl Bus InVehicle Time mf5411 HbUBlBusWait HbU Bl Bus Waiting Time mf5412 HbUBlBusAux HbU Bl Bus Auxillary Time mf5413 HbUBlBusBoard HbU Bl Bus Boardings mf5414 HbUBlBusFare HbU Bl Bus Fare mf5415 HbUBlBusIvttBRT HbU Bl Bus BRT InVehicle Time mf5420 HbScBlBusIvtt HbSc Bl Bus InVehicle Time mf5421 HbScBlBusWait HbSc Bl Bus Waiting Time mf5422 HbScBlBusAux HbSc Bl Bus Auxillary Time mf5423 HbScBlBusBoard HbSc Bl Bus Boardings mf5424 HbScBlBusFare HbSc Bl Bus Fare mf5425 HbScBlBusIvttBRT HbSc Bl Bus BRT InVehicle Time mf5430 HbShBlBusIvtt HbSh Bl Bus InVehicle Time mf5431 HbShBlBusWait HbSh Bl Bus Waiting Time mf5432 HbShBlBusAux HbSh Bl Bus Auxillary Time mf5433 HbShBlBusBoard HbSh Bl Bus Boardings mf5434 HbShBlBusFare HbSh Bl Bus Fare mf5435 HbShBlBusIvttBRT HbSh Bl Bus BRT InVehicle Time mf5440 HbPbBlBusIvtt HbPb Bl Bus InVehicle Time mf5441 HbPbBlBusWait HbPb Bl Bus Waiting Time mf5442 HbPbBlBusAux HbPb Bl Bus Auxillary Time mf5443 HbPbBlBusBoard HbPb Bl Bus Boardings mf5444 HbPbBlBusFare HbPb Bl Bus Fare mf5445 HbPbBlBusIvttBRT HbPb Bl Bus BRT InVehicle Time mf5450 HbSoBlBusIvtt HbSo Bl Bus InVehicle Time mf5451 HbSoBlBusWait HbSo Bl Bus Waiting Time mf5452 HbSoBlBusAux HbSo Bl Bus Auxillary Time mf5453 HbSoBlBusBoard HbSo Bl Bus Boardings mf5454 HbSoBlBusFare HbSo Bl Bus Fare mf5455 HbSoBlBusIvttBRT HbSo Bl Bus BRT InVehicle Time mf5460 HbEsBlBusIvtt HbEs Bl Bus InVehicle Time mf5461 HbEsBlBusWait HbEs Bl Bus Waiting Time mf5462 HbEsBlBusAux HbEs Bl Bus Auxillary Time mf5463 HbEsBlBusBoard HbEs Bl Bus Boardings mf5464 HbEsBlBusFare HbEs Bl Bus Fare mf5465 HbEsBlBusIvttBRT HbEs Bl Bus BRT InVehicle Time mf5470 NHbWBlBusIvtt NHbW Bl Bus InVehicle Time mf5471 NHbWBlBusWait NHbW Bl Bus Waiting Time mf5472 NHbWBlBusAux NHbW Bl Bus Auxillary Time mf5473 NHbWBlBusBoard NHbW Bl Bus Boardings mf5474 NHbWBlBusFare NHbW Bl Bus Fare mf5475 NHbWBlBusIvttBRT NHbW Bl Bus BRT InVehicle Time mf5480 NHbOBlBusIvtt NHbO Bl Bus InVehicle Time mf5481 NHbOBlBusWait NHbO Bl Bus Waiting Time mf5482 NHbOBlBusAux NHbO Bl Bus Auxillary Time mf5483 NHbOBlBusBoard NHbO Bl Bus Boardings mf5484 NHbOBlBusFare NHbO Bl Bus Fare mf5485 NHbOBlBusIvttBRT NHbO Bl Bus BRT InVehicle Time mf5500 AmRailIvtt Am Rail Invehicle Time mf5501 AmRailIvttBus Am Rail Invehicle Time on Bus mf5502 AmRailWait Am Rail Waiting Time mf5503 AmRailAux Am Rail Auxilliary Time mf5504 AmRailBoard Am Rail Boardings mf5505 AmRailFare Am Rail Fare mf5506 AmRailIvttBRT Am Rail Invehicle Time on BRT mf5507 AmRailIvttLRT Am Rail Invehicle Time on LRT mf5510 MdRailIvtt Md Rail Invehicle Time mf5511 MdRailIvttBus Md Rail Invehicle Time on Bus mf5512 MdRailWait Md Rail Waiting Time mf5513 MdRailAux Md Rail Auxilliary Time mf5514 MdRailBoard Md Rail Boardings mf5515 MdRailFare Md Rail Fare mf5516 MdRailIvttBRT Md Rail Invehicle Time on BRT mf5517 MdRailIvttLRT Md Rail Invehicle Time on LRT mf5520 PmRailIvtt Pm Rail Invehicle Time mf5521 PmRailIvttBus Pm Rail Invehicle Time on Bus mf5522 PmRailWait Pm Rail Waiting Time mf5523 PmRailAux Pm Rail Auxilliary Time mf5524 PmRailBoard Pm Rail Boardings mf5525 PmRailFare Pm Rail Fare mf5526 PmRailIvttBRT Pm Rail Invehicle Time on BRT mf5527 PmRailIvttLRT Pm Rail Invehicle Time on LRT mf5600 HbWBlRailIvtt HbW Bl Rail Invehicle Time mf5601 HbWBlRailIvttBus HbW Bl Rail Invehicle Time on Bus mf5602 HbWBlRailWait HbW Bl Rail Waiting Time mf5603 HbWBlRailAux HbW Bl Rail Auxilliary Time mf5604 HbWBlRailBoard HbW Bl Rail Boardings mf5605 HbWBlRailFare HbW Bl Rail Fare mf5606 HbWBlRailIvttBRT HbW Bl Rail Invehicle Time on BRT mf5607 HbWBlRailIvttLRT HbW Bl Rail Invehicle Time on LRT mf5610 HbUBlRailIvtt HbU Bl Rail Invehicle Time mf5611 HbUBlRailIvttBus HbU Bl Rail Invehicle Time on Bus mf5612 HbUBlRailWait HbU Bl Rail Waiting Time mf5613 HbUBlRailAux HbU Bl Rail Auxilliary Time mf5614 HbUBlRailBoard HbU Bl Rail Boardings mf5615 HbUBlRailFare HbU Bl Rail Fare mf5616 HbUBlRailIvttBRT HbU Bl Rail Invehicle Time on BRT mf5617 HbUBlRailIvttLRT HbU Bl Rail Invehicle Time on LRT mf5620 HbScBlRailIvtt HbSc Bl Rail Invehicle Time mf5621 HbScBlRailIvttBus HbSc Bl Rail Invehicle Time on Bus mf5622 HbScBlRailWait HbSc Bl Rail Waiting Time mf5623 HbScBlRailAux HbSc Bl Rail Auxilliary Time mf5624 HbScBlRailBoard HbSc Bl Rail Boardings mf5625 HbScBlRailFare HbSc Bl Rail Fare mf5626 HbScBlRailIvttBRT HbSc Bl Rail Invehicle Time on BRT mf5627 HbScBlRailIvttLRT HbSc Bl Rail Invehicle Time on LRT mf5630 HbShBlRailIvtt HbSh Bl Rail Invehicle Time mf5631 HbShBlRailIvttBus HbSh Bl Rail Invehicle Time on Bus mf5632 HbShBlRailWait HbSh Bl Rail Waiting Time mf5633 HbShBlRailAux HbSh Bl Rail Auxilliary Time mf5634 HbShBlRailBoard HbSh Bl Rail Boardings mf5635 HbShBlRailFare HbSh Bl Rail Fare mf5636 HbShBlRailIvttBRT HbSh Bl Rail Invehicle Time on BRT mf5637 HbShBlRailIvttLRT HbSh Bl Rail Invehicle Time on LRT mf5640 HbPbBlRailIvtt HbPb Bl Rail Invehicle Time mf5641 HbPbBlRailIvttBus HbPb Bl Rail Invehicle Time on Bus mf5642 HbPbBlRailWait HbPb Bl Rail Waiting Time mf5643 HbPbBlRailAux HbPb Bl Rail Auxilliary Time mf5644 HbPbBlRailBoard HbPb Bl Rail Boardings mf5645 HbPbBlRailFare HbPb Bl Rail Fare mf5646 HbPbBlRailIvttBRT HbPb Bl Rail Invehicle Time on BRT mf5647 HbPbBlRailIvttLRT HbPb Bl Rail Invehicle Time on LRT mf5650 HbSoBlRailIvtt HbSo Bl Rail Invehicle Time mf5651 HbSoBlRailIvttBus HbSo Bl Rail Invehicle Time on Bus mf5652 HbSoBlRailWait HbSo Bl Rail Waiting Time mf5653 HbSoBlRailAux HbSo Bl Rail Auxilliary Time mf5654 HbSoBlRailBoard HbSo Bl Rail Boardings mf5655 HbSoBlRailFare HbSo Bl Rail Fare mf5656 HbSoBlRailIvttBRT HbSo Bl Rail Invehicle Time on BRT mf5657 HbSoBlRailIvttLRT HbSo Bl Rail Invehicle Time on LRT mf5660 HbEsBlRailIvtt HbEs Bl Rail Invehicle Time mf5661 HbEsBlRailIvttBus HbEs Bl Rail Invehicle Time on Bus mf5662 HbEsBlRailWait HbEs Bl Rail Waiting Time mf5663 HbEsBlRailAux HbEs Bl Rail Auxilliary Time mf5664 HbEsBlRailBoard HbEs Bl Rail Boardings mf5665 HbEsBlRailFare HbEs Bl Rail Fare mf5666 HbEsBlRailIvttBRT HbEs Bl Rail Invehicle Time on BRT mf5667 HbEsBlRailIvttLRT HbEs Bl Rail Invehicle Time on LRT mf5670 NHbWBlRailIvtt NHbW Bl Rail Invehicle Time mf5671 NHbWBlRailIvttBus NHbW Bl Rail Invehicle Time on Bus mf5672 NHbWBlRailWait NHbW Bl Rail Waiting Time mf5673 NHbWBlRailAux NHbW Bl Rail Auxilliary Time mf5674 NHbWBlRailBoard NHbW Bl Rail Boardings mf5675 NHbWBlRailFare NHbW Bl Rail Fare mf5676 NHbWBlRailIvttBRT NHbW Bl Rail Invehicle Time on BRT mf5677 NHbWBlRailIvttLRT NHbW Bl Rail Invehicle Time on LRT mf5680 NHbOBlRailIvtt NHbO Bl Rail Invehicle Time mf5681 NHbOBlRailIvttBus NHbO Bl Rail Invehicle Time on Bus mf5682 NHbOBlRailWait NHbO Bl Rail Waiting Time mf5683 NHbOBlRailAux NHbO Bl Rail Auxilliary Time mf5684 NHbOBlRailBoard NHbO Bl Rail Boardings mf5685 NHbOBlRailFare NHbO Bl Rail Fare mf5686 NHbOBlRailIvttBRT NHbO Bl Rail Invehicle Time on BRT mf5687 NHbOBlRailIvttLRT NHbO Bl Rail Invehicle Time on LRT mf5700 AmWceIvtt Am Rail Invehicle Time mf5701 AmWceIvttRail Am Rail Invehicle Time on Rail mf5702 AmWceIvttBus Am Rail Invehicle Time on Bus mf5703 AmWceWait Am Rail Waiting Time mf5704 AmWceAux Am Rail Auxilliary Time mf5705 AmWceBoard Am Rail Boardings mf5706 AmWceFare Am Rail Fare mf5710 MdWceIvtt Md Rail Invehicle Time mf5711 MdWceIvttRail Md Rail Invehicle Time on Rail mf5712 MdWceIvttBus Md Rail Invehicle Time on Bus mf5713 MdWceWait Md Rail Waiting Time mf5714 MdWceAux Md Rail Auxilliary Time mf5715 MdWceBoard Md Rail Boardings mf5716 MdWceFare Md Rail Fare mf5720 PmWceIvtt Pm Rail Invehicle Time mf5721 PmWceIvttRail Pm Rail Invehicle Time on Rail mf5722 PmWceIvttBus Pm Rail Invehicle Time on Bus mf5723 PmWceWait Pm Rail Waiting Time mf5724 PmWceAux Pm Rail Auxilliary Time mf5725 PmWceBoard Pm Rail Boardings mf5726 PmWceFare Pm Rail Fare mf5800 HbWBlWceIvtt HbW Bl WCE Invehicle Time mf5801 HbWBlWceIvttRail HbW Bl WCE Invehicle Time on WCE mf5802 HbWBlWceIvttBus HbW Bl WCE Invehicle Time on Bus mf5803 HbWBlWceWait HbW Bl WCE Waiting Time mf5804 HbWBlWceAux HbW Bl WCE Auxilliary Time mf5805 HbWBlWceBoard HbW Bl WCE Boardings mf5806 HbWBlWceFare HbW Bl WCE Fare mf5901 AMbusTT AM bus total time mf5902 AMRailTT AM Rail total time mf5903 AMWCETT AM WCE total time mf6000 buspr-lotChceWkAMPA Bus Best PnR Lot - Bus mf6001 railpr-lotChceWkAMPA Rail Best PnR Lot - Rail mf6002 wcepr-lotChceWkAMPA Rail Best PnR Lot -WCE mf6003 pr-gtAutoWkAMPA PnR Generalized Cost Auto Leg mf6005 buspr-GctranWkAMPA PnR Generalized Cost Transit Leg - Bus mf6006 buspr-minGCWkAMPA PnR Combined Skim Result - Bus mf6015 railpr-GctranWkAMPA Rail PnR Generalized Cost Transit Leg - Rail mf6016 railpr-minGCWkAMPA Rail PnR Combined Skim Result - Rail mf6030 wcepr-GctranWkAMPA WCE PnR Generalized Cost Transit Leg mf6031 wcepr-minGCWkAMPA WCE PnR Combined Skim Result mf6048 pr-gtAutoWkMDPA PnR Generalized Cost Auto Leg mf6050 buspr-GctranWkMDPA PnR Generalized Cost Transit Leg - Bus mf6051 buspr-minGCWkMDPA PnR Combined Skim Result - Bus mf6060 railpr-GctranWkMDPA Rail PnR Generalized Cost Transit Leg - Rail mf6061 railpr-minGCWkMDPA Rail PnR Combined Skim Result - Rail mf6075 wcepr-GctranWkMDPA WCE PnR Generalized Cost Transit Leg mf6076 wcepr-minGCWkMDPA WCE PnR Combined Skim Result mf6088 pr-gtAutoWkPMPA PnR Generalized Cost Auto Leg mf6090 buspr-GctranWkPMPA PnR Generalized Cost Transit Leg - Bus mf6091 buspr-minGCWkPMPA PnR Combined Skim Result - Bus mf6100 railpr-GctranWkPMPA Rail PnR Generalized Cost Transit Leg - Rail mf6101 railpr-minGCWkPMPA Rail PnR Combined Skim Result - Rail mf6115 wcepr-GctranWkPMPA WCE PnR Generalized Cost Transit Leg mf6116 wcepr-minGCWkPMPA WCE PnR Combined Skim Result mf6130 buspr-lotChceNWkAMPA Bus Best PnR Lot - Bus mf6131 railpr-lotChceNWkAMPA Rail Best PnR Lot - Rail mf6132 wcepr-lotChceNWkAMPA Rail Best PnR Lot -WCE mf6133 pr-gtAutoNWkAMPA PnR Generalized Cost Auto Leg mf6135 buspr-GctranNWkAMPA PnR Generalized Cost Transit Leg - Bus mf6136 buspr-minGCNWkAMPA PnR Combined Skim Result - Bus mf6145 railpr-GctranNWkAMPA Rail PnR Generalized Cost Transit Leg - Rail mf6146 railpr-minGCNWkAMPA Rail PnR Combined Skim Result - Rail mf6160 wcepr-GctranNWkAMPA WCE PnR Generalized Cost Transit Leg mf6161 wcepr-minGCNWkAMPA WCE PnR Combined Skim Result mf6173 pr-gtAutoNWkMDPA PnR Generalized Cost Auto Leg mf6175 buspr-GctranNWkMDPA PnR Generalized Cost Transit Leg - Bus mf6176 buspr-minGCNWkMDPA PnR Combined Skim Result - Bus mf6185 railpr-GctranNWkMDPA Rail PnR Generalized Cost Transit Leg - Rail mf6186 railpr-minGCNWkMDPA Rail PnR Combined Skim Result - Rail mf6200 wcepr-GctranNWkMDPA WCE PnR Generalized Cost Transit Leg mf6201 wcepr-minGCNWkMDPA WCE PnR Combined Skim Result mf6213 pr-gtAutoNWkPMPA PnR Generalized Cost Auto Leg mf6215 buspr-GctranNWkPMPA PnR Generalized Cost Transit Leg - Bus mf6216 buspr-minGCNWkPMPA PnR Combined Skim Result - Bus mf6225 railpr-GctranNWkPMPA Rail PnR Generalized Cost Transit Leg - Rail mf6226 railpr-minGCNWkPMPA Rail PnR Combined Skim Result - Rail mf6240 wcepr-GctranNWkPMPA WCE PnR Generalized Cost Transit Leg mf6241 wcepr-minGCNWkPMPA WCE PnR Combined Skim Result mf6303 pr-gtAutoWkAMAP PnR Generalized Cost Auto Leg mf6305 buspr-GctranWkAMAP PnR Generalized Cost Transit Leg - Bus mf6306 buspr-minGCWkAMAP PnR Combined Skim Result - Bus mf6315 railpr-GctranWkAMAP Rail PnR Generalized Cost Transit Leg - Rail mf6316 railpr-minGCWkAMAP Rail PnR Combined Skim Result - Rail mf6330 wcepr-GctranWkAMAP WCE PnR Generalized Cost Transit Leg mf6331 wcepr-minGCWkAMAP WCE PnR Combined Skim Result mf6348 pr-gtAutoWkMDAP PnR Generalized Cost Auto Leg mf6350 buspr-GctranWkMDAP PnR Generalized Cost Transit Leg - Bus mf6351 buspr-minGCWkMDAP PnR Combined Skim Result - Bus mf6360 railpr-GctranWkMDAP Rail PnR Generalized Cost Transit Leg - Rail mf6361 railpr-minGCWkMDAP Rail PnR Combined Skim Result - Rail mf6375 wcepr-GctranWkMDAP WCE PnR Generalized Cost Transit Leg mf6376 wcepr-minGCWkMDAP WCE PnR Combined Skim Result mf6388 pr-gtAutoWkPMAP PnR Generalized Cost Auto Leg mf6390 buspr-GctranWkPMAP PnR Generalized Cost Transit Leg - Bus mf6391 buspr-minGCWkPMAP PnR Combined Skim Result - Bus mf6400 railpr-GctranWkPMAP Rail PnR Generalized Cost Transit Leg - Rail mf6401 railpr-minGCWkPMAP Rail PnR Combined Skim Result - Rail mf6415 wcepr-GctranWkPMAP WCE PnR Generalized Cost Transit Leg mf6416 wcepr-minGCWkPMAP WCE PnR Combined Skim Result mf6433 pr-gtAutoNWkAMAP PnR Generalized Cost Auto Leg mf6435 buspr-GctranNWkAMAP PnR Generalized Cost Transit Leg - Bus mf6436 buspr-minGCNWkAMAP PnR Combined Skim Result - Bus mf6445 railpr-GctranNWkAMAP Rail PnR Generalized Cost Transit Leg - Rail mf6446 railpr-minGCNWkAMAP Rail PnR Combined Skim Result - Rail mf6460 wcepr-GctranNWkAMAP WCE PnR Generalized Cost Transit Leg mf6461 wcepr-minGCNWkAMAP WCE PnR Combined Skim Result mf6473 pr-gtAutoNWkMDAP PnR Generalized Cost Auto Leg mf6475 buspr-GctranNWkMDAP PnR Generalized Cost Transit Leg - Bus mf6476 buspr-minGCNWkMDAP PnR Combined Skim Result - Bus mf6485 railpr-GctranNWkMDAP Rail PnR Generalized Cost Transit Leg - Rail mf6486 railpr-minGCNWkMDAP Rail PnR Combined Skim Result - Rail mf6500 wcepr-GctranNWkMDAP WCE PnR Generalized Cost Transit Leg mf6501 wcepr-minGCNWkMDAP WCE PnR Combined Skim Result mf6513 pr-gtAutoNWkPMAP PnR Generalized Cost Auto Leg mf6515 buspr-GctranNWkPMAP PnR Generalized Cost Transit Leg - Bus mf6516 buspr-minGCNWkPMAP PnR Combined Skim Result - Bus mf6525 railpr-GctranNWkPMAP Rail PnR Generalized Cost Transit Leg - Rail mf6526 railpr-minGCNWkPMAP Rail PnR Combined Skim Result - Rail mf6540 wcepr-GctranNWkPMAP WCE PnR Generalized Cost Transit Leg mf6541 wcepr-minGCNWkPMAP WCE PnR Combined Skim Result mf6800 HbWBlBAuPRCost HbW Bl Bus-Auto PR Cost mf6801 HbWBlBAuPRTime HbW Bl Bus-Auto PR Time mf6802 HbWBlBAuPRToll HbW Bl Bus-Auto PR Toll mf6803 HbWBAuPrkCst HbW Bus-Auto PR Parking Cost mf6804 HbWBAuTrmTim HbW Bus-Auto PR Terminal Time mf6810 HbWBlRAuPRCost HbW Bl Rail-Auto PR Cost mf6811 HbWBlRAuPRTime HbW Bl Rail-Auto PR Time mf6812 HbWBlRAuPRToll HbW Bl Rail-Auto PR Toll mf6813 HbWRAuPrkCst HbW Rail-Auto PR Parking Cost mf6814 HbWRAuTrmTim HbW Rail-Auto PR Terminal Time mf6820 HbWBlWAuPRCost HbW Bl WCE-Auto PR Cost mf6821 HbWBlWAuPRTime HbW Bl WCE-Auto PR Time mf6822 HbWBlWAuPRToll HbW Bl WCE-Auto PR Toll mf6823 HbWWAuPrkCst HbW WCE-Auto PR Parking Cost mf6824 HbWWAuTrmTim HbW WCE-Auto PR Terminal Time mf6900 HbWBlBAuBusIvtt HbW Bl Bus-Auto PR InVehicle Time mf6901 HbWBlBAuBusWait HbW Bl Bus-Auto PR Bus Waiting Time mf6902 HbWBlBAuBusAux HbW Bl Bus-Auto PR Bus Auxillary Time mf6903 HbWBlBAuBusBoard HbW Bl Bus-Auto PR Bus Boardings mf6904 HbWBlBAuBusFare HbW Bl Bus-Auto PR Bus Fare mf6910 HbWBlRAuRailIvtt HbW Bl Rail-Auto Rail Invehicle Time mf6911 HbWBlRAuRailIvttBus HbW Bl Rail Invehicle Time on Bus mf6912 HbWBlRAuRailWait HbW Bl Rail-Auto Rail Waiting Time mf6913 HbWBlRAuRailAux HbW Bl Rail-Auto Rail Auxilliary Time mf6914 HbWBlRAuRailBoard HbW Bl Rail-Auto Rail Boardings mf6915 HbWBlRAuRailFare HbW Bl Rail-Auto Rail Fare mf6920 HbWBlWAuWceIvtt HbW Bl WCE-Auto WCE Invehicle Time mf6921 HbWBlWAuWceIvttRail HbW Bl WCE-Auto WCE Invehicle Time on Rail mf6922 HbWBlWAuWceIvttBus HbW Bl WCE-Auto WCE Invehicle Time on Bus mf6923 HbWBlWAuWceWait HbW Bl WCE-Auto WCE Waiting Time mf6924 HbWBlWAuWceAux HbW Bl WCE-Auto WCE Auxilliary Time mf6925 HbWBlWAuWceBoards HbW Bl WCE-Auto WCE Boardings mf6926 HbWBlWAuWceFare HbW Bl WCE-Auto WCE Fare mf8000 24CBLg Cross Border Light Trucks 24 Hrs mf8001 AMCBLg Cross Border Light Trucks AM mf8002 MDCBLg Cross Border Light Trucks MD mf8003 PMCBLg Cross Border Light Trucks PM mf8010 24CBHv Cross Border Heavy Trucks 24 Hrs mf8011 AMCBHv Cross Border Heavy Trucks AM mf8012 MDCBHv Cross Border Heavy Trucks MD mf8013 PMCBHv Cross Border Heavy Trucks PM mf8020 IRLg99 IR Light Truck 1999 Survey Rat mf8021 IRHv99 IR Heavy Truck 1999 Survey Rat mf8022 IRLg24 IR LgTruck Daily Trips mf8023 IRHv24 IR HvTruck Daily Trips mf8024 IRLgAM IR LgTruck AM Trips mf8025 IRHvAM IR HvTruck AM Trips mf8026 IRLgMD IR LgTruck MD Trips mf8027 IRHvMD IR HvTruck MD Trips mf8028 IRLgPM IR LgTruck PM Trips mf8029 IRHvPM IR HvTruck PM Trips mf8030 PMV24 PMV24Hrs mf8031 PMVAM PMVAM mf8032 PMVMD PMVMD mf8033 PMVPM PMVPM mf8040 APHv24 AP HvTrucks Daily Trips mf8041 APHvAM AP HvTrucks AM Trips mf8042 APHvMD AP HvTrucks MD Trips mf8043 APHvPM AP HvTrucks PM Trips mf8050 RGLgAp Rg LgTruck Peak Period AM mf8051 RGHvAp Rg HvTruck Peak Period AM mf8052 RGLgMp Rg LgTruck Peak Period MD mf8053 RGHvMp Rg HvTruck Peak Period MD mf8054 RGLgPp Rg LgTruck Peak Period PM mf8055 RGHvPp Rg HvTruck Peak Period PM mf8060 RGLgFc Rg Lg Truck Impedance mf8061 RGHvFc Rg Hv Truck Impedance mf8062 RGLg24 Rg LgTruck Daily Trips mf8063 RGHv24 Rg HvTruck Daily Trips mf8064 RGLgAM Rg LgTruck AM Trips mf8065 RGLgMD Rg LgTruck MD Trips mf8066 RGLgPM Rg LgTruck PM Trips mf8067 RGHvAM Rg HvTruck AM Trips mf8068 RGHvMD Rg HvTruck MD Trips mf8069 RGHvPM Rg HvTruck PM Trips mf8080 LGVAM LGV Truck Demand AM mf8081 HGVAM HGV Truck Demand AM mf8082 LGVMD LGV Truck Demand MD mf8083 HGVMD HGV Truck Demand MD mf8084 LGVPM LGV Truck Demand PM mf8085 HGVPM HGV Truck Demand PM mf8100 SkimAmSovTimeVOT1 Skim AM SOV VOT1 Time mf8101 SkimAmSovTimeVOT2 Skim AM SOV VOT2 Time mf8102 SkimAmSovTimeVOT3 Skim AM SOV VOT3 Time mf8103 SkimAmSovTimeVOT4 Skim AM SOV VOT4 Time mf8104 SkimAmHovTimeVOT1 Skim AM HOV VOT1 Time mf8105 SkimAmHovTimeVOT2 Skim AM HOV VOT2 Time mf8106 SkimAmHovTimeVOT3 Skim AM HOV VOT3 Time mf8107 SkimAmHovTimeVOT4 Skim AM HOV VOT4 Time mf8108 SkimAmLgvTime Skim AM LGV Time mf8109 SkimAmHgvTime Skim AM HGV Time mf8110 SkimAmSovDistVOT1 Skim AM SOV VOT1 Dist mf8111 SkimAmSovDistVOT2 Skim AM SOV VOT2 Dist mf8112 SkimAmSovDistVOT3 Skim AM SOV VOT3 Dist mf8113 SkimAmSovDistVOT4 Skim AM SOV VOT4 Dist mf8114 SkimAmHovDistVOT1 Skim AM HOV VOT1 Dist mf8115 SkimAmHovDistVOT2 Skim AM HOV VOT2 Dist mf8116 SkimAmHovDistVOT3 Skim AM HOV VOT3 Dist mf8117 SkimAmHovDistVOT4 Skim AM HOV VOT4 Dist mf8118 SkimAmLgvDist Skim AM LGV Dist mf8119 SkimAmHgvDist Skim AM HGV Dist mf8120 SkimAmSovTollVOT1 Skim AM SOV VOT1 Toll mf8121 SkimAmSovTollVOT2 Skim AM SOV VOT2 Toll mf8122 SkimAmSovTollVOT3 Skim AM SOV VOT3 Toll mf8123 SkimAmSovTollVOT4 Skim AM SOV VOT4 Toll mf8124 SkimAmHovTollVOT1 Skim AM HOV VOT1 Toll mf8125 SkimAmHovTollVOT2 Skim AM HOV VOT2 Toll mf8126 SkimAmHovTollVOT3 Skim AM HOV VOT3 Toll mf8127 SkimAmHovTollVOT4 Skim AM HOV VOT4 Toll mf8128 SkimAmLgvToll Skim AM LGV Toll mf8129 SkimAmHgvToll Skim AM HGV Toll mf8130 SkimMdSovTimeVOT1 Skim MD SOV VOT1 Time mf8131 SkimMdSovTimeVOT2 Skim MD SOV VOT2 Time mf8132 SkimMdSovTimeVOT3 Skim MD SOV VOT3 Time mf8133 SkimMdSovTimeVOT4 Skim MD SOV VOT4 Time mf8134 SkimMdHovTimeVOT1 Skim MD HOV VOT1 Time mf8135 SkimMdHovTimeVOT2 Skim MD HOV VOT2 Time mf8136 SkimMdHovTimeVOT3 Skim MD HOV VOT3 Time mf8137 SkimMdHovTimeVOT4 Skim MD HOV VOT4 Time mf8138 SkimMdLgvTime Skim MD LGV Time mf8139 SkimMdHgvTime Skim MD HGV Time mf8140 SkimMdSovDistVOT1 Skim MD SOV VOT1 Dist mf8141 SkimMdSovDistVOT2 Skim MD SOV VOT2 Dist mf8142 SkimMdSovDistVOT3 Skim MD SOV VOT3 Dist mf8143 SkimMdSovDistVOT4 Skim MD SOV VOT4 Dist mf8144 SkimMdHovDistVOT1 Skim MD HOV VOT1 Dist mf8145 SkimMdHovDistVOT2 Skim MD HOV VOT2 Dist mf8146 SkimMdHovDistVOT3 Skim MD HOV VOT3 Dist mf8147 SkimMdHovDistVOT4 Skim MD HOV VOT4 Dist mf8148 SkimMdLgvDist Skim MD LGV Dist mf8149 SkimMdHgvDist Skim MD HGV Dist mf8150 SkimMdSovTollVOT1 Skim MD SOV VOT1 Toll mf8151 SkimMdSovTollVOT2 Skim MD SOV VOT2 Toll mf8152 SkimMdSovTollVOT3 Skim MD SOV VOT3 Toll mf8153 SkimMdSovTollVOT4 Skim MD SOV VOT4 Toll mf8154 SkimMdHovTollVOT1 Skim MD HOV VOT1 Toll mf8155 SkimMdHovTollVOT2 Skim MD HOV VOT2 Toll mf8156 SkimMdHovTollVOT3 Skim MD HOV VOT3 Toll mf8157 SkimMdHovTollVOT4 Skim MD HOV VOT4 Toll mf8158 SkimMdLgvToll Skim MD LGV Toll mf8159 SkimMdHgvToll Skim MD HGV Toll mf8160 SkimPmSovTimeVOT1 Skim PM SOV VOT1 Time mf8161 SkimPmSovTimeVOT2 Skim PM SOV VOT2 Time mf8162 SkimPmSovTimeVOT3 Skim PM SOV VOT3 Time mf8163 SkimPmSovTimeVOT4 Skim PM SOV VOT4 Time mf8164 SkimPmHovTimeVOT1 Skim PM HOV VOT1 Time mf8165 SkimPmHovTimeVOT2 Skim PM HOV VOT2 Time mf8166 SkimPmHovTimeVOT3 Skim PM HOV VOT3 Time mf8167 SkimPmHovTimeVOT4 Skim PM HOV VOT4 Time mf8168 SkimPmLgvTime Skim PM LGV Time mf8169 SkimPmHgvTime Skim PM HGV Time mf8170 SkimPmSovDistVOT1 Skim PM SOV VOT1 Dist mf8171 SkimPmSovDistVOT2 Skim PM SOV VOT2 Dist mf8172 SkimPmSovDistVOT3 Skim PM SOV VOT3 Dist mf8173 SkimPmSovDistVOT4 Skim PM SOV VOT4 Dist mf8174 SkimPmHovDistVOT1 Skim PM HOV VOT1 Dist mf8175 SkimPmHovDistVOT2 Skim PM HOV VOT2 Dist mf8176 SkimPmHovDistVOT3 Skim PM HOV VOT3 Dist mf8177 SkimPmHovDistVOT4 Skim PM HOV VOT4 Dist mf8178 SkimPmLgvDist Skim PM LGV Dist mf8179 SkimPmHgvDist Skim PM HGV Dist mf8180 SkimPmSovTollVOT1 Skim PM SOV VOT1 Toll mf8181 SkimPmSovTollVOT2 Skim PM SOV VOT2 Toll mf8182 SkimPmSovTollVOT3 Skim PM SOV VOT3 Toll mf8183 SkimPmSovTollVOT4 Skim PM SOV VOT4 Toll mf8184 SkimPmHovTollVOT1 Skim PM HOV VOT1 Toll mf8185 SkimPmHovTollVOT2 Skim PM HOV VOT2 Toll mf8186 SkimPmHovTollVOT3 Skim PM HOV VOT3 Toll mf8187 SkimPmHovTollVOT4 Skim PM HOV VOT4 Toll mf8188 SkimPmLgvToll Skim PM LGV Toll mf8189 SkimPmHgvToll Skim PM HGV Toll mf9000 HbWLSI1A0 HbW LogSum I1 A0 mf9001 HbWLSI1A1 HbW LogSum I1 A1 mf9002 HbWLSI1A2 HbW LogSum I1 A2 mf9003 HbWLSI2A0 HbW LogSum I1 A0 mf9004 HbWLSI2A1 HbW LogSum I1 A1 mf9005 HbWLSI2A2 HbW LogSum I1 A2 mf9006 HbWLSI3A0 HbW LogSum I1 A0 mf9007 HbWLSI3A1 HbW LogSum I1 A1 mf9008 HbWLSI3A2 HbW LogSum I1 A2 mf9010 HbULS HbU LogSum mf9020 HbScLSA0 HbSc LogSum A0 mf9021 HbScLSA1 HbSc LogSum A1 mf9022 HbScLSA2 HbSc LogSum A2 mf9030 HbShLSI1A0 LogSum HbSh I1 A0 mf9031 HbShLSI1A1 LogSum HbSh I1 A1 mf9032 HbShLSI1A2 LogSum HbSh I1 A2 mf9033 HbShLSI2A0 LogSum HbSh I2 A0 mf9034 HbShLSI2A1 LogSum HbSh I2 A1 mf9035 HbShLSI2A2 LogSum HbSh I2 A2 mf9036 HbShLSI3A0 LogSum HbSh I3 A0 mf9037 HbShLSI3A1 LogSum HbSh I3 A1 mf9038 HbShLSI3A2 LogSum HbSh I3 A2 mf9040 HbPbLSI1A0 HbPb LogSum I1 A0 mf9041 HbPbLSI1A1 HbPb LogSum I1 A1 mf9042 HbPbLSI1A2 HbPb LogSum I1 A2 mf9043 HbPbLSI2A0 HbPb LogSum I1 A0 mf9044 HbPbLSI2A1 HbPb LogSum I1 A1 mf9045 HbPbLSI2A2 HbPb LogSum I1 A2 mf9046 HbPbLSI3A0 HbPb LogSum I1 A0 mf9047 HbPbLSI3A1 HbPb LogSum I1 A1 mf9048 HbPbLSI3A2 HbPb LogSum I1 A2 mf9050 HbSoLSI1A0 LogSum HbSo I1 A0 mf9051 HbSoLSI1A1 LogSum HbSo I1 A1 mf9052 HbSoLSI1A2 LogSum HbSo I1 A2 mf9053 HbSoLSI2A0 LogSum HbSo I2 A0 mf9054 HbSoLSI2A1 LogSum HbSo I2 A1 mf9055 HbSoLSI2A2 LogSum HbSo I2 A2 mf9056 HbSoLSI3A0 LogSum HbSo I3 A0 mf9057 HbSoLSI3A1 LogSum HbSo I3 A1 mf9058 HbSoLSI3A2 LogSum HbSo I3 A2 mf9060 HbEsLSA0 HbEs LogSum A0 mf9061 HbEsLSA1 HbEs LogSum A1 mf9062 HbEsLSA2 HbEs LogSum A2 mf9070 NHbWLS NHbW LogSum mf9080 NHbOLS NHbO LogSum mf9100 P-AFrictionFact1 Trip Distribution Friction Factor 1 mf9101 P-AFrictionFact2 Trip Distribution Friction Factor 2 mf9102 P-AFrictionFact3 Trip Distribution Friction Factor 3 mf9103 P-AFrictionFact4 Trip Distribution Friction Factor 4 mf9104 P-AFrictionFact5 Trip Distribution Friction Factor 5 mf9105 P-AFrictionFact6 Trip Distribution Friction Factor 6 mf9106 P-AFrictionFact7 Trip Distribution Friction Factor 7 mf9107 P-AFrictionFact8 Trip Distribution Friction Factor 8 mf9108 P-AFrictionFact9 Trip Distribution Friction Factor 9 mf9200 Kij_hbw Kij_hbw mf9201 Kij_hbu Kij_hbu mf9202 Kij_hbsch Kij_hbsch mf9203 Kij_hbshop Kij_hbshop mf9204 Kij_hbpb Kij_hbpb mf9205 Kij_hbsoc Kij_hbsoc mf9206 Kij_hbesc Kij_hbesc mf9207 Kij_nhbw Kij_nhbw mf9208 Kij_nhbo Kij_nhbo mf9300 HbWLSAUI1A0 HbW LogSum Auto I1 A0 mf9301 HbWLSAUI1A1 HbW LogSum Auto I1 A1 mf9302 HbWLSAUI1A2 HbW LogSum Auto I1 A2 mf9303 HbWLSAUI2A0 HbW LogSum Auto I1 A0 mf9304 HbWLSAUI2A1 HbW LogSum Auto I1 A1 mf9305 HbWLSAUI2A2 HbW LogSum Auto I1 A2 mf9306 HbWLSAUI3A0 HbW LogSum Auto I1 A0 mf9307 HbWLSAUI3A1 HbW LogSum Auto I1 A1 mf9308 HbWLSAUI3A2 HbW LogSum Auto I1 A2 mf9310 HbULSAU HbU LogSum Auto mf9320 HbScLSAUA0 HbSc LogSum Auto A0 mf9321 HbScLSAUA1 HbSc LogSum Auto A1 mf9322 HbScLSAUA2 HbSc LogSum Auto A2 mf9330 HbShLSAUI1A0 LogSum HbSh Auto I1 A0 mf9331 HbShLSAUI1A1 LogSum HbSh Auto I1 A1 mf9332 HbShLSAUI1A2 LogSum HbSh Auto I1 A2 mf9333 HbShLSAUI2A0 LogSum HbSh Auto I2 A0 mf9334 HbShLSAUI2A1 LogSum HbSh Auto I2 A1 mf9335 HbShLSAUI2A2 LogSum HbSh Auto I2 A2 mf9336 HbShLSAUI3A0 LogSum HbSh Auto I3 A0 mf9337 HbShLSAUI3A1 LogSum HbSh Auto I3 A1 mf9338 HbShLSAUI3A2 LogSum HbSh Auto I3 A2 mf9340 HbPbLSAUI1A0 HbPb LogSum Auto I1 A0 mf9341 HbPbLSAUI1A1 HbPb LogSum Auto I1 A1 mf9342 HbPbLSAUI1A2 HbPb LogSum Auto I1 A2 mf9343 HbPbLSAUI2A0 HbPb LogSum Auto I1 A0 mf9344 HbPbLSAUI2A1 HbPb LogSum Auto I1 A1 mf9345 HbPbLSAUI2A2 HbPb LogSum Auto I1 A2 mf9346 HbPbLSAUI3A0 HbPb LogSum Auto I1 A0 mf9347 HbPbLSAUI3A1 HbPb LogSum Auto I1 A1 mf9348 HbPbLSAUI3A2 HbPb LogSum Auto I1 A2 mf9350 HbSoLSAUI1A0 LogSum HbSo Auto I1 A0 mf9351 HbSoLSAUI1A1 LogSum HbSo Auto I1 A1 mf9352 HbSoLSAUI1A2 LogSum HbSo Auto I1 A2 mf9353 HbSoLSAUI2A0 LogSum HbSo Auto I2 A0 mf9354 HbSoLSAUI2A1 LogSum HbSo Auto I2 A1 mf9355 HbSoLSAUI2A2 LogSum HbSo Auto I2 A2 mf9356 HbSoLSAUI3A0 LogSum HbSo Auto I3 A0 mf9357 HbSoLSAUI3A1 LogSum HbSo Auto I3 A1 mf9358 HbSoLSAUI3A2 LogSum HbSo Auto I3 A2 mf9360 HbEsLSAUA0 HbEs LogSum Auto A0 mf9361 HbEsLSAUA1 HbEs LogSum Auto A1 mf9362 HbEsLSAUA2 HbEs LogSum Auto A2 mf9370 NHbWLSAU NHbW LogSum Auto mf9380 NHbOLSAU NHbO LogSum Auto mf9400 HbWLSTRI1A0 HbW LogSum Transit I1 A0 mf9401 HbWLSTRI1A1 HbW LogSum Transit I1 A1 mf9402 HbWLSTRI1A2 HbW LogSum Transit I1 A2 mf9403 HbWLSTRI2A0 HbW LogSum Transit I1 A0 mf9404 HbWLSTRI2A1 HbW LogSum Transit I1 A1 mf9405 HbWLSTRI2A2 HbW LogSum Transit I1 A2 mf9406 HbWLSTRI3A0 HbW LogSum Transit I1 A0 mf9407 HbWLSTRI3A1 HbW LogSum Transit I1 A1 mf9408 HbWLSTRI3A2 HbW LogSum Transit I1 A2 mf9410 HbULSTR HbU LogSum Transit mf9420 HbScLSTRA0 HbSc LogSum Transit A0 mf9421 HbScLSTRA1 HbSc LogSum Transit A1 mf9422 HbScLSTRA2 HbSc LogSum Transit A2 mf9430 HbShLSTRI1A0 LogSum HbSh Transit I1 A0 mf9431 HbShLSTRI1A1 LogSum HbSh Transit I1 A1 mf9432 HbShLSTRI1A2 LogSum HbSh Transit I1 A2 mf9433 HbShLSTRI2A0 LogSum HbSh Transit I2 A0 mf9434 HbShLSTRI2A1 LogSum HbSh Transit I2 A1 mf9435 HbShLSTRI2A2 LogSum HbSh Transit I2 A2 mf9436 HbShLSTRI3A0 LogSum HbSh Transit I3 A0 mf9437 HbShLSTRI3A1 LogSum HbSh Transit I3 A1 mf9438 HbShLSTRI3A2 LogSum HbSh Transit I3 A2 mf9440 HbPbLSTRI1A0 HbPb LogSum Transit I1 A0 mf9441 HbPbLSTRI1A1 HbPb LogSum Transit I1 A1 mf9442 HbPbLSTRI1A2 HbPb LogSum Transit I1 A2 mf9443 HbPbLSTRI2A0 HbPb LogSum Transit I1 A0 mf9444 HbPbLSTRI2A1 HbPb LogSum Transit I1 A1 mf9445 HbPbLSTRI2A2 HbPb LogSum Transit I1 A2 mf9446 HbPbLSTRI3A0 HbPb LogSum Transit I1 A0 mf9447 HbPbLSTRI3A1 HbPb LogSum Transit I1 A1 mf9448 HbPbLSTRI3A2 HbPb LogSum Transit I1 A2 mf9450 HbSoLSTRI1A0 LogSum HbSo Transit I1 A0 mf9451 HbSoLSTRI1A1 LogSum HbSo Transit I1 A1 mf9452 HbSoLSTRI1A2 LogSum HbSo Transit I1 A2 mf9453 HbSoLSTRI2A0 LogSum HbSo Transit I2 A0 mf9454 HbSoLSTRI2A1 LogSum HbSo Transit I2 A1 mf9455 HbSoLSTRI2A2 LogSum HbSo Transit I2 A2 mf9456 HbSoLSTRI3A0 LogSum HbSo Transit I3 A0 mf9457 HbSoLSTRI3A1 LogSum HbSo Transit I3 A1 mf9458 HbSoLSTRI3A2 LogSum HbSo Transit I3 A2 mf9460 HbEsLSTRA0 HbEs LogSum Transit A0 mf9461 HbEsLSTRA1 HbEs LogSum Transit A1 mf9462 HbEsLSTRA2 HbEs LogSum Transit A2 mf9470 NHbWLSTR NHbW LogSum Transit mf9480 NHbOLSTR NHbO LogSum Transit mf9500 HbWLSACI1A0 HbW LogSum Active I1 A0 mf9501 HbWLSACI1A1 HbW LogSum Active I1 A1 mf9502 HbWLSACI1A2 HbW LogSum Active I1 A2 mf9503 HbWLSACI2A0 HbW LogSum Active I1 A0 mf9504 HbWLSACI2A1 HbW LogSum Active I1 A1 mf9505 HbWLSACI2A2 HbW LogSum Active I1 A2 mf9506 HbWLSACI3A0 HbW LogSum Active I1 A0 mf9507 HbWLSACI3A1 HbW LogSum Active I1 A1 mf9508 HbWLSACI3A2 HbW LogSum Active I1 A2 mf9510 HbULSAC HbU LogSum Active mf9520 HbScLSACA0 HbSc LogSum Active A0 mf9521 HbScLSACA1 HbSc LogSum Active A1 mf9522 HbScLSACA2 HbSc LogSum Active A2 mf9530 HbShLSACI1A0 LogSum HbSh Active I1 A0 mf9531 HbShLSACI1A1 LogSum HbSh Active I1 A1 mf9532 HbShLSACI1A2 LogSum HbSh Active I1 A2 mf9533 HbShLSACI2A0 LogSum HbSh Active I2 A0 mf9534 HbShLSACI2A1 LogSum HbSh Active I2 A1 mf9535 HbShLSACI2A2 LogSum HbSh Active I2 A2 mf9536 HbShLSACI3A0 LogSum HbSh Active I3 A0 mf9537 HbShLSACI3A1 LogSum HbSh Active I3 A1 mf9538 HbShLSACI3A2 LogSum HbSh Active I3 A2 mf9540 HbPbLSACI1A0 HbPb LogSum Active I1 A0 mf9541 HbPbLSACI1A1 HbPb LogSum Active I1 A1 mf9542 HbPbLSACI1A2 HbPb LogSum Active I1 A2 mf9543 HbPbLSACI2A0 HbPb LogSum Active I1 A0 mf9544 HbPbLSACI2A1 HbPb LogSum Active I1 A1 mf9545 HbPbLSACI2A2 HbPb LogSum Active I1 A2 mf9546 HbPbLSACI3A0 HbPb LogSum Active I1 A0 mf9547 HbPbLSACI3A1 HbPb LogSum Active I1 A1 mf9548 HbPbLSACI3A2 HbPb LogSum Active I1 A2 mf9550 HbSoLSACI1A0 LogSum HbSo Active I1 A0 mf9551 HbSoLSACI1A1 LogSum HbSo Active I1 A1 mf9552 HbSoLSACI1A2 LogSum HbSo Active I1 A2 mf9553 HbSoLSACI2A0 LogSum HbSo Active I2 A0 mf9554 HbSoLSACI2A1 LogSum HbSo Active I2 A1 mf9555 HbSoLSACI2A2 LogSum HbSo Active I2 A2 mf9556 HbSoLSACI3A0 LogSum HbSo Active I3 A0 mf9557 HbSoLSACI3A1 LogSum HbSo Active I3 A1 mf9558 HbSoLSACI3A2 LogSum HbSo Active I3 A2 mf9560 HbEsLSACA0 HbEs LogSum Active A0 mf9561 HbEsLSACA1 HbEs LogSum Active A1 mf9562 HbEsLSACA2 HbEs LogSum Active A2 mf9570 NHbWLSAC NHbW LogSum Active mf9580 NHbOLSAC NHbO LogSum Active mo10 TotPop POP_Total mo11 Pop0t4 POP_0to4 mo12 Pop5t12 POP_5to12 mo13 Pop13t17 POP_13to17 mo14 Pop18t24 POP_18to24 mo15 Pop25t34 POP_25to34 mo16 Pop35t54 POP_35to54 mo17 Pop55t64 POP_55to64 mo18 Pop65Up POP_65plus mo20 TotEmp EMP_Total mo21 EmpConMfg Construct_Mfg mo22 EmpFire FIRE mo23 EmpTcuWh TCU_Wholesale mo24 EmpRet Retail mo25 EmpBoS Business_OtherServices mo26 EmpAcFoInCu AccomFood_InfoCult mo27 EmpHeEdPuAd Health_Educat_PubAdmin mo30 EnrolElem Elementary_Enrolment mo31 EnrolSec Secondary_Enrolment mo32 EnrolPsFte PostSecFTE mo40 TotHh HHOLDS_Total mo41 Hh1p HHOLDS_1Person mo42 Hh2p HHOLDS_2Person mo43 Hh3p HHOLDS_3Person mo44 Hh4pUp HHOLDS_4plusPerson mo50 areahc taz_a_area_hec mo51 zoneindex Zone numbers mo60 prk2hr parkingcost2hr mo61 prk8hr parkingcost8hr mo69 railStn railstationinzone mo70 cs250 car_share_250m mo71 cs500 car_share_500m mo80 bikesc bikescore mo90 prcost pnr_lot_cost mo91 prcap pnr_lot_capacity mo92 prtrmt pnr_terminal_time mo100 d_cbd cbd dummy mo101 d_tc towncentre dummy mo102 d_uni university dummy mo103 d_ubc ubc dummy mo104 d_sfu sfu dummy mo105 d_air airport dummy mo106 d_yvr yvr dummy mo107 d_ns northshore dummy mo108 d_dtes downtowneastidedummy mo109 d_fry ferrydummy mo110 d_hosp hospitaldummy mo120 ga_ensem ga Ensemble Groups mo121 gb_ensem gb Ensemble Groups mo122 gc_ensem gc Ensemble Groups mo123 gd_ensem gd Ensemble Groups mo124 ge_ensem ge Ensemble Groups mo125 gf_ensem gf Ensemble Groups mo126 gg_ensem gg Ensemble Groups mo127 gh_ensem gh Ensemble Groups mo128 gi_ensem gi Ensemble Groups mo129 gj_ensem gj Ensemble Groups mo130 gk_ensem gk Ensemble Groups mo131 gl_ensem gl Ensemble Groups mo132 gm_ensem gm Ensemble Groups mo133 gn_ensem gn Ensemble Groups mo134 go_ensem go Ensemble Groups mo135 gp_ensem gp Ensemble Groups mo136 gq_ensem gq Ensemble Groups mo137 gr_ensem gr Ensemble Groups mo138 gs_ensem gs Ensemble Groups mo139 gt_ensem gt Ensemble Groups mo140 gu_ensem gu Ensemble Groups mo141 gv_ensem gv Ensemble Groups mo142 gw_ensem gw Ensemble Groups mo143 gx_ensem gx Ensemble Groups mo144 gy_ensem gy Ensemble Groups mo145 gz_ensem One-to-One Zone Ensemble mo200 popdens Population density (per/hec) mo201 empdens Employment density (job/hec) mo202 combinedens Pop + Emp density (per hec) mo205 popdensln Log Population density (per/hec) mo206 empdensln Log Employment density (job/hec) mo207 combinedensln Log Pop + Emp density (per hec) mo210 autoAcc Auto Accessibility mo211 transitAcc Transit Accessibility mo212 distCbd Distance to CBD mo213 distTc Distance to Town Centre mo214 uniAcc University Accessibility mo215 socAcc Social Recreational Accessibility mo220 autoAccLn Log Auto Accessibility mo221 transitAccLn Log Transit Accessibility mo222 distCbdLn Log Distance to CBD mo223 distTcLn Log Distance to Town Centre mo224 uniAccLn Log University Accessibility mo225 socAccLn Log Social Recreational Accessibility mo2000 hbwInc1Au0prd hbwInc1Au0 Productions mo2001 hbwInc2Au0prd hbwInc2Au0 Productions mo2002 hbwInc3Au0prd hbwInc3Au0 Productions mo2003 hbwInc1Au1prd hbwInc1Au1 Productions mo2004 hbwInc2Au1prd hbwInc2Au1 Productions mo2005 hbwInc3Au1prd hbwInc3Au1 Productions mo2006 hbwInc1Au2prd hbwInc1Au2 Productions mo2007 hbwInc2Au2prd hbwInc2Au2 Productions mo2008 hbwInc3Au2prd hbwInc3Au2 Productions mo2010 hbescInc1Au0prd hbescInc1Au0 Productions mo2011 hbescInc2Au0prd hbescInc2Au0 Productions mo2012 hbescInc3Au0prd hbescInc3Au0 Productions mo2013 hbescInc1Au1prd hbescInc1Au1 Productions mo2014 hbescInc2Au1prd hbescInc2Au1 Productions mo2015 hbescInc3Au1prd hbescInc3Au1 Productions mo2016 hbescInc1Au2prd hbescInc1Au2 Productions mo2017 hbescInc2Au2prd hbescInc2Au2 Productions mo2018 hbescInc3Au2prd hbescInc3Au2 Productions mo2020 hbpbInc1Au0prd hbpbInc1Au0 Productions mo2021 hbpbInc2Au0prd hbpbInc2Au0 Productions mo2022 hbpbInc3Au0prd hbpbInc3Au0 Productions mo2023 hbpbInc1Au1prd hbpbInc1Au1 Productions mo2024 hbpbInc2Au1prd hbpbInc2Au1 Productions mo2025 hbpbInc3Au1prd hbpbInc3Au1 Productions mo2026 hbpbInc1Au2prd hbpbInc1Au2 Productions mo2027 hbpbInc2Au2prd hbpbInc2Au2 Productions mo2028 hbpbInc3Au2prd hbpbInc3Au2 Productions mo2030 hbschInc1Au0prd hbschInc1Au0 Productions mo2031 hbschInc2Au0prd hbschInc2Au0 Productions mo2032 hbschInc3Au0prd hbschInc3Au0 Productions mo2033 hbschInc1Au1prd hbschInc1Au1 Productions mo2034 hbschInc2Au1prd hbschInc2Au1 Productions mo2035 hbschInc3Au1prd hbschInc3Au1 Productions mo2036 hbschInc1Au2prd hbschInc1Au2 Productions mo2037 hbschInc2Au2prd hbschInc2Au2 Productions mo2038 hbschInc3Au2prd hbschInc3Au2 Productions mo2040 hbshopInc1Au0prd hbshopInc1Au0 Productions mo2041 hbshopInc2Au0prd hbshopInc2Au0 Productions mo2042 hbshopInc3Au0prd hbshopInc3Au0 Productions mo2043 hbshopInc1Au1prd hbshopInc1Au1 Productions mo2044 hbshopInc2Au1prd hbshopInc2Au1 Productions mo2045 hbshopInc3Au1prd hbshopInc3Au1 Productions mo2046 hbshopInc1Au2prd hbshopInc1Au2 Productions mo2047 hbshopInc2Au2prd hbshopInc2Au2 Productions mo2048 hbshopInc3Au2prd hbshopInc3Au2 Productions mo2050 hbsocInc1Au0prd hbsocInc1Au0 Productions mo2051 hbsocInc2Au0prd hbsocInc2Au0 Productions mo2052 hbsocInc3Au0prd hbsocInc3Au0 Productions mo2053 hbsocInc1Au1prd hbsocInc1Au1 Productions mo2054 hbsocInc2Au1prd hbsocInc2Au1 Productions mo2055 hbsocInc3Au1prd hbsocInc3Au1 Productions mo2056 hbsocInc1Au2prd hbsocInc1Au2 Productions mo2057 hbsocInc2Au2prd hbsocInc2Au2 Productions mo2058 hbsocInc3Au2prd hbsocInc3Au2 Productions mo2060 hbuprd hbu productions mo2070 nhbwprd nhbw productions mo2080 nhboprd nhbo productions mo8020 IRLgPr IR LgTruck Productions mo8021 IRHvPr IR HvTruck Productions mo8022 IRLgAdj IR Lg Adjustment Calc mo8023 IRHvAdj IR Hv Adjustment Calc mo8030 PMVrat PMV activity ratio mo8050 RGLgPr Rg Daily LgTruck Trip Prod mo8051 RGHvPr Rg Daily HvTruck Trip Prod md2000 hbwatr hbw Attractions md2010 hbescatr hbesc Attractions md2020 hbpbatr hbpb Attractions md2030 hbschatr hbsch Attractions md2031 hbschprgx hbsch Production Sum by GX md2032 hbschsc hbsch Attraction by GX md2040 hbshopatr hbshop Attractions md2050 hbsocatr hbsoc Attractions md2060 hbuatr hbu Attractions md2070 nhbwatr nhbw Attractions md2080 nhboatr nhbo Attractions md8020 IRLgAt IR LgTruck Attractions md8021 IRHvAt IR HvTruck Attractions md8022 IRLgAdj IR Lg Adjustment Calc md8023 IRHvAdj IR Hv Adjustment Calc md8050 RGLgAt Rg Daily LgTruck Trip Att md8051 RGHvAt Rg Daily HvTruck Trip Att ms1 CycleNum Current Cycle Number ms2 AmScen AMScenario ms3 MdScen MDScenario ms4 PmScen PMScenario ms10 Year Horizon Year of Run ms12 Processors Number of Processors for Computer Running Model ms20 parkingModel Run Parking Model ms21 tollSkim Run Toll Skim ms30 IterGlobal Global Iterations ms35 IterDist DistributionIterations ms36 RelErrDist Distribution Relative Error ms40 IterAss Assignment Iterations ms41 ConRelGap ConvergenceRelativeGap ms42 ConBestRel ConvergenceBestRelative ms43 ConNorm ConvergenceNormalized ms44 AutoOcc Standard HOV Occupancy ms45 tranCongest Run Congested Transit Assignment ms46 tranCapac Run Capacitated Transit Assignment ms100 autoOpCost Auto Operating Cost ms101 lgvOpCost Light Truck Operating Cost ms102 hgvOpCost Heavy Truck Operating Cost ms110 HOVOccHbw HOV Occupancy HB Work ms111 HOVOccHbu HOV Occupancy HB University ms112 HOVOccHbesc HOV Occupancy HB Escorting ms113 HOVOccHbpb HOV Occupancy HB Pers Bus ms114 HOVOccHBsch HOV Occupancy HB School ms115 HOVOccHBshop HOV Occupancy HB Shop ms116 HOVOccHBsoc HOV Occupancy HB Social ms117 HOVOccNHBw HOV Occupancy NHB Work ms118 HOVOccNHBo HOV Occupancy NHB Other ms122 AutoOccHbesc Auto Occupancy HB Escorting ms128 AutoOccNHBo Auto Occupancy NHB Other ms130 lgvPCE Light Truck Passenger Car Equivalent ms131 hgvPCE Heavy Truck Passenger Car Equivalent ms142 sov_pct_Hbesc SOV proportion of Auto HB Escorting ms148 sov_pct_NHBo SOV proportion of Auto NHB Other ms150 lgvTollFac LGV toll factor ms151 hgvTollFac HGV toll factor ms152 sovTollFac SOV toll factor ms153 hovTollFac HOV toll factor ms154 BRTASCFactor BRT Percent of Rail ASC ms155 LRTASCFactor LRT Percent of Rail ASC ms156 BRTIVTFactor BRT Percent of Rail IVT ms157 LRTIVTFactor LRT Percent of Rail IVT ms160 oneZoneFare One Zone Fare - FS-SV ms161 fareIncrement Fare Increment ms162 wce_bfare_zone1_1 wce_bfare_zone1_1 ms163 wce_bfare_zone1_2 wce_bfare_zone1_2 ms164 wce_bfare_zone1_3 wce_bfare_zone1_3 ms165 wce_bfare_zone3_1 wce_bfare_zone3_1 ms166 wce_bfare_zone3_2 wce_bfare_zone3_2 ms167 wce_bfare_zone3_3 wce_bfare_zone3_3 ms168 wce_bfare_zone4_1 wce_bfare_zone4_1 ms169 wce_bfare_zone4_2 wce_bfare_zone4_2 ms170 wce_bfare_zone4_3 wce_bfare_zone4_3 ms171 wce_bfare_zone5_1 wce_bfare_zone5_1 ms172 wce_bfare_zone5_2 wce_bfare_zone5_2 ms173 wce_bfare_zone5_3 wce_bfare_zone5_3 ms174 wce_bfare_zone13_1 wce_bfare_zone13_1 ms175 wce_bfare_zone13_2 wce_bfare_zone13_2 ms176 wce_bfare_zone13_3 wce_bfare_zone13_3 ms177 wce_bfare_zone34_1 wce_bfare_zone34_1 ms178 wce_bfare_zone34_2 wce_bfare_zone34_2 ms179 wce_bfare_zone34_3 wce_bfare_zone34_3 ms180 wce_bfare_zone45_1 wce_bfare_zone34_1 ms181 wce_bfare_zone45_2 wce_bfare_zone34_2 ms182 wce_bfare_zone45_3 wce_bfare_zone34_3 ms190 wce_fare_1z wce_fare_1z ms191 wce_fare_2z wce_fare_2z ms192 wce_fare_3z wce_fare_3z ms193 wce_fare_4z wce_fare_4z ms194 wce_fare_5z wce_fare_5z ms200 AutoVOT1 AutoVOT1 ms201 AutoVOT2 AutoVOT2 ms202 AutoVOT3 AutoVOT3 ms203 AutoVOT4 AutoVOT4 ms204 AutoVOT5 AutoVOT5 ms212 VotBus VOT Work ms213 VotRail VOT Work ms214 VotWce VOT Work ms218 VotLgv VOT lgv ms219 VotHgv VOT hgv ms300 busIVTprcpWk bus in-vehicle time perception work ms301 busWAITprcpWk bus wait time perception work ms302 busWALKprcpWk bus walk time perception work ms303 busBOARDSprcpWk bus boarding perception work ms310 railIVTprcpWk rail in-vehicle time perception work ms311 railWAITprcpWk rail wait time perception work ms312 railWALKprcpWk rail walk time perception work ms313 railBOARDSprcpWk rail boarding perception work ms320 wceIVTprcpWk wce in-vehicle time perception work ms321 wceWAITprcpWk wce wait time perception work ms322 wceWALKprcpWk wce walk time perception work ms323 wceBOARDSprcpWk wce boarding perception work ms330 busIVTprcpNwk bus in-vehicle time perception nonwork ms331 busWAITprcpNwk bus wait time perception nonwork ms332 busWALKprcpNwk bus walk time perception nonwork ms333 busTRANSprcpNwk bus transfer perception nonwork ms334 busBOARDSprcpNwk bus boarding perception nonwork ms340 railIVTprcpNwk rail in-vehicle time perception nonwork ms341 railWAITprcpNwk rail wait time perception nonwork ms342 railWALKprcpNwk rail walk time perception nonwork ms343 railTRANSprcpNwk rail transfer perception nonwork ms344 railBOARDSprcpNwk rail boarding perception nonwork ms350 wceIVTprcpNwk wce in-vehicle time perception nonwork ms351 wceWAITprcpNwk wce wait time perception nonwork ms352 wceWALKprcpNwk wce walk time perception nonwork ms353 wceTRANSprcpNwk wce transfer perception nonwork ms354 wceBOARDSprcpNwk wce boarding perception nonwork ms360 pr_auto_time_prcp Park and Ride Drive time perception factor ms365 busIVTpr_cal_railGT BusIVT percep RailGT Cal ms366 transfer_cal_railGT transfer percep RailGT Cal ms400 HbWBl_AM_P-A HbW Blend AM P-A Factor ms401 HbWBl_MD_P-A HbW Blend MD P-A Factor ms402 HbWBl_PM_P-A HbW Blend PM P-A Factor ms403 HbWBl_AM_A-P HbW Blend AM A-P Factor ms404 HbWBl_MD_A-P HbW Blend MD A-P Factor ms405 HbWBl_PM_A-P HbW Blend PM A-P Factor ms406 HbWBl_AM_WCE_P-A HbW Blend AM WCE P-A Factor ms407 HbWBl_PM_WCE_P-A HbW Blend PM WCE P-A Factor ms408 HbWBl_PM_WCE_A-P HbW Blend PM WCE A-P Factor ms410 HbUBl_AM_P-A HbU Blend AM P-A Factor ms411 HbUBl_MD_P-A HbU Blend MD P-A Factor ms412 HbUBl_PM_P-A HbU Blend PM P-A Factor ms413 HbUBl_AM_A-P HbU Blend AM A-P Factor ms414 HbUBl_MD_A-P HbU Blend MD A-P Factor ms415 HbUBl_PM_A-P HbU Blend PM A-P Factor ms420 HbScBl_AM_P-A HbSc Blend AM P-A Factor ms421 HbScBl_MD_P-A HbSc Blend MD P-A Factor ms422 HbScBl_PM_P-A HbSc Blend PM P-A Factor ms423 HbScBl_AM_A-P HbSc Blend AM A-P Factor ms424 HbScBl_MD_A-P HbSc Blend MD A-P Factor ms425 HbScBl_PM_A-P HbSc Blend PM A-P Factor ms430 HbShBl_AM_P-A HbSh Blend AM P-A Factor ms431 HbShBl_MD_P-A HbSh Blend MD P-A Factor ms432 HbShBl_PM_P-A HbSh Blend PM P-A Factor ms433 HbShBl_AM_A-P HbSh Blend AM A-P Factor ms434 HbShBl_MD_A-P HbSh Blend MD A-P Factor ms435 HbShBl_PM_A-P HbSh Blend PM A-P Factor ms440 HbPbBl_AM_P-A HbPb Blend AM P-A Factor ms441 HbPbBl_MD_P-A HbPb Blend MD P-A Factor ms442 HbPbBl_PM_P-A HbPb Blend PM P-A Factor ms443 HbPbBl_AM_A-P HbPb Blend AM A-P Factor ms444 HbPbBl_MD_A-P HbPb Blend MD A-P Factor ms445 HbPbBl_PM_A-P HbPb Blend PM A-P Factor ms450 HbSoBl_AM_P-A HbSo Blend AM P-A Factor ms451 HbSoBl_MD_P-A HbSo Blend MD P-A Factor ms452 HbSoBl_PM_P-A HbSo Blend PM P-A Factor ms453 HbSoBl_AM_A-P HbSo Blend AM A-P Factor ms454 HbSoBl_MD_A-P HbSo Blend MD A-P Factor ms455 HbSoBl_PM_A-P HbSo Blend PM A-P Factor ms460 HbEsBl_AM_P-A HbEs Blend AM P-A Factor ms461 HbEsBl_MD_P-A HbEs Blend MD P-A Factor ms462 HbEsBl_PM_P-A HbEs Blend PM P-A Factor ms463 HbEsBl_AM_A-P HbEs Blend AM A-P Factor ms464 HbEsBl_MD_A-P HbEs Blend MD A-P Factor ms465 HbEsBl_PM_A-P HbEs Blend PM A-P Factor ms470 NHbWBl_AM_P-A NHbW Blend AM P-A Factor ms471 NHbWBl_MD_P-A NHbW Blend MD P-A Factor ms472 NHbWBl_PM_P-A NHbW Blend PM P-A Factor ms480 NHbOBl_AM_P-A NHbO Blend AM P-A Factor ms481 NHbOBl_MD_P-A NHbO Blend MD P-A Factor ms482 NHbOBl_PM_P-A NHbO Blend PM P-A Factor ms490 Zero Zero ms500 TrFr_HbWBl_AM_P-A TrFr HbW Blend AM P-A Factor ms501 TrFr_HbWBl_MD_P-A TrFr HbW Blend MD P-A Factor ms502 TrFr_HbWBl_PM_P-A TrFr HbW Blend PM P-A Factor ms503 TrFr_HbWBl_OP_P-A TrFr HbW Blend OP P-A Factor ms504 TrFr_HbWBl_AM_A-P TrFr HbW Blend AM A-P Factor ms505 TrFr_HbWBl_MD_A-P TrFr HbW Blend MD A-P Factor ms506 TrFr_HbWBl_PM_A-P TrFr HbW Blend PM A-P Factor ms507 TrFr_HbWBl_OP_A-P TrFr HbW Blend OP A-P Factor ms510 TrFr_HbUBl_AM_P-A TrFr HbU Blend AM P-A Factor ms511 TrFr_HbUBl_MD_P-A TrFr HbU Blend MD P-A Factor ms512 TrFr_HbUBl_PM_P-A TrFr HbU Blend PM P-A Factor ms513 TrFr_HbUBl_OP_P-A TrFr HbU Blend OP P-A Factor ms514 TrFr_HbUBl_AM_A-P TrFr HbU Blend AM A-P Factor ms515 TrFr_HbUBl_MD_A-P TrFr HbU Blend MD A-P Factor ms516 TrFr_HbUBl_PM_A-P TrFr HbU Blend PM A-P Factor ms517 TrFr_HbUBl_OP_A-P TrFr HbU Blend OP A-P Factor ms520 TrFr_HbScBl_AM_P-A TrFr HbSc Blend AM P-A Factor ms521 TrFr_HbScBl_MD_P-A TrFr HbSc Blend MD P-A Factor ms522 TrFr_HbScBl_PM_P-A TrFr HbSc Blend PM P-A Factor ms523 TrFr_HbScBl_OP_P-A TrFr HbSc Blend OP P-A Factor ms524 TrFr_HbScBl_AM_A-P TrFr HbSc Blend AM A-P Factor ms525 TrFr_HbScBl_MD_A-P TrFr HbSc Blend MD A-P Factor ms526 TrFr_HbScBl_PM_A-P TrFr HbSc Blend PM A-P Factor ms527 TrFr_HbScBl_OP_A-P TrFr HbSc Blend OP A-P Factor ms530 TrFr_HbShBl_AM_P-A TrFr HbSh Blend AM P-A Factor ms531 TrFr_HbShBl_MD_P-A TrFr HbSh Blend MD P-A Factor ms532 TrFr_HbShBl_PM_P-A TrFr HbSh Blend PM P-A Factor ms533 TrFr_HbShBl_OP_P-A TrFr HbSh Blend OP P-A Factor ms534 TrFr_HbShBl_AM_A-P TrFr HbSh Blend AM A-P Factor ms535 TrFr_HbShBl_MD_A-P TrFr HbSh Blend MD A-P Factor ms536 TrFr_HbShBl_PM_A-P TrFr HbSh Blend PM A-P Factor ms537 TrFr_HbShBl_OP_A-P TrFr HbSh Blend OP A-P Factor ms540 TrFr_HbPbBl_AM_P-A TrFr HbPb Blend AM P-A Factor ms541 TrFr_HbPbBl_MD_P-A TrFr HbPb Blend MD P-A Factor ms542 TrFr_HbPbBl_PM_P-A TrFr HbPb Blend PM P-A Factor ms543 TrFr_HbPbBl_OP_P-A TrFr HbPb Blend OP P-A Factor ms544 TrFr_HbPbBl_AM_A-P TrFr HbPb Blend AM A-P Factor ms545 TrFr_HbPbBl_MD_A-P TrFr HbPb Blend MD A-P Factor ms546 TrFr_HbPbBl_PM_A-P TrFr HbPb Blend PM A-P Factor ms547 TrFr_HbPbBl_OP_A-P TrFr HbPb Blend OP A-P Factor ms550 TrFr_HbSoBl_AM_P-A TrFr HbSo Blend AM P-A Factor ms551 TrFr_HbSoBl_MD_P-A TrFr HbSo Blend MD P-A Factor ms552 TrFr_HbSoBl_PM_P-A TrFr HbSo Blend PM P-A Factor ms553 TrFr_HbSoBl_OP_P-A TrFr HbSo Blend OP P-A Factor ms554 TrFr_HbSoBl_AM_A-P TrFr HbSo Blend AM A-P Factor ms555 TrFr_HbSoBl_MD_A-P TrFr HbSo Blend MD A-P Factor ms556 TrFr_HbSoBl_PM_A-P TrFr HbSo Blend PM A-P Factor ms557 TrFr_HbSoBl_OP_A-P TrFr HbSo Blend OP A-P Factor ms560 TrFr_HbEsBl_AM_P-A TrFr HbEs Blend AM P-A Factor ms561 TrFr_HbEsBl_MD_P-A TrFr HbEs Blend MD P-A Factor ms562 TrFr_HbEsBl_PM_P-A TrFr HbEs Blend PM P-A Factor ms563 TrFr_HbEsBl_OP_P-A TrFr HbEs Blend OP P-A Factor ms564 TrFr_HbEsBl_AM_A-P TrFr HbEs Blend AM A-P Factor ms565 TrFr_HbEsBl_MD_A-P TrFr HbEs Blend MD A-P Factor ms566 TrFr_HbEsBl_PM_A-P TrFr HbEs Blend PM A-P Factor ms567 TrFr_HbEsBl_OP_A-P TrFr HbEs Blend OP A-P Factor ms570 TrFr_NHbWBl_AM_P-A TrFr NHbW Blend AM P-A Factor ms571 TrFr_NHbWBl_MD_P-A TrFr NHbW Blend MD P-A Factor ms572 TrFr_NHbWBl_PM_P-A TrFr NHbW Blend PM P-A Factor ms573 TrFr_NHbWBl_OP_P-A TrFr NHbW Blend OP P-A Factor ms580 TrFr_NHbOBl_AM_P-A TrFr NHbO Blend AM P-A Factor ms581 TrFr_NHbOBl_MD_P-A TrFr NHbO Blend MD P-A Factor ms582 TrFr_NHbOBl_PM_P-A TrFr NHbO Blend PM P-A Factor ms583 TrFr_NHbOBl_OP_P-A TrFr NHbO Blend OP P-A Factor ms8030 gg27nonret non-retail employment in gg27 ms8050 RGphAM Rg Truck Peak Hour Factor AM ms8051 RGphMD Rg Truck Peak Hour Factor MD ms8052 RGphPM Rg Truck Peak Hour Factor PM","title":"Matrix list"},{"location":"naming_conventions/","text":"Naming Conventions \u00b6 Matrix Names \u00b6 Matrix names stored either as intermediate or final data will have the following naming conventions: Data Type (dtype) DLT \u2013 Daily trips (Logsum) LSM \u2013 Logsum utility (Logsum) AMT \u2013 AM trips (Logsum) MDT \u2013 Midday trips (Logsum) PMT \u2013 PM trips (Logsum) AMM \u2013 AM time (ROH) MDM \u2013 Midday time (ROH) PMM \u2013 PM constant (ROH) AMC \u2013 AM constant (ROH) MDC \u2013 Midday constant (ROH) PMC \u2013 PM constant (ROH) Purpose HWRK \u2013 Home-based work HUNI \u2013 Home-based university HSCH \u2013 Home-based school HSHP \u2013 Home-based shopping HPBS \u2013 Home-based personal business HSOC \u2013 Home-based social HESC \u2013 Home-based escort NWRK \u2013 Non-home-based work NOTH \u2013 Non-home-based other Income I1 \u2013 low I2 \u2013 medium I3 \u2013 high I9 \u2013 all Auto Ownership A0 \u2013 no car A1 \u2013 1 car A2 \u2013 2 cars A9 \u2013 all Mode SOV \u2013 single occupancy vehicle HOV \u2013 high occupancy vehicle BUS \u2013 bus RAL \u2013 rail WCE \u2013 West Coast Express LGV \u2013 light truck HGV \u2013 heavy truck This is an example for daily trip for home-based work, low income and 1-car household :","title":"Naming Conventions"},{"location":"naming_conventions/#naming-conventions","text":"","title":"Naming Conventions"},{"location":"naming_conventions/#matrix-names","text":"Matrix names stored either as intermediate or final data will have the following naming conventions: Data Type (dtype) DLT \u2013 Daily trips (Logsum) LSM \u2013 Logsum utility (Logsum) AMT \u2013 AM trips (Logsum) MDT \u2013 Midday trips (Logsum) PMT \u2013 PM trips (Logsum) AMM \u2013 AM time (ROH) MDM \u2013 Midday time (ROH) PMM \u2013 PM constant (ROH) AMC \u2013 AM constant (ROH) MDC \u2013 Midday constant (ROH) PMC \u2013 PM constant (ROH) Purpose HWRK \u2013 Home-based work HUNI \u2013 Home-based university HSCH \u2013 Home-based school HSHP \u2013 Home-based shopping HPBS \u2013 Home-based personal business HSOC \u2013 Home-based social HESC \u2013 Home-based escort NWRK \u2013 Non-home-based work NOTH \u2013 Non-home-based other Income I1 \u2013 low I2 \u2013 medium I3 \u2013 high I9 \u2013 all Auto Ownership A0 \u2013 no car A1 \u2013 1 car A2 \u2013 2 cars A9 \u2013 all Mode SOV \u2013 single occupancy vehicle HOV \u2013 high occupancy vehicle BUS \u2013 bus RAL \u2013 rail WCE \u2013 West Coast Express LGV \u2013 light truck HGV \u2013 heavy truck This is an example for daily trip for home-based work, low income and 1-car household :","title":"Matrix Names"},{"location":"reports/","text":"Reports \u00b6 RTM Development Report \u00b6 The RTM Phase 3 Development Report contains details on the modelling methodology, model structure and parameters, and model validation. Please access the report using the following link: RTM_Development_Report.pdf","title":"Reports"},{"location":"reports/#reports","text":"","title":"Reports"},{"location":"reports/#rtm-development-report","text":"The RTM Phase 3 Development Report contains details on the modelling methodology, model structure and parameters, and model validation. Please access the report using the following link: RTM_Development_Report.pdf","title":"RTM Development Report"},{"location":"scenario_comparison/","text":"Scenario Comparison Tool \u00b6 How it works \u00b6 The Scenario Comparison EMME toolbox is an utility that allows planners/modellers to quickly compare metrics across two different EMME scenarios. This toolbox requires two RTM runs to be completed using the latest version of RTM. For each run of the RTM, the model will create two databases for each scenario the model was run for: rtm.db , and trip_summaries.db . The two databases house data relating to transportation metrics. The tool will connect to the pair of databases for each scenario, pull relevant data, join the data for each scenario, and then output the data for each metric/aggregation/disaggregation as a csv. The code reads from the configuration file ScenarioComparison.json . The config file will give a series of tables from the two databases for the script to pull from. For each table, the disagg field will give a list of columns to disaggregate by for the measure, what type of aggregation/group by action to perform (ex average or summing), and what columns to sum/average for (ex volumes or trips). In certain cases, the config file will also tell the script if it needs to rename certain columns in the database output, if multiple levels of aggregation are needed, or if custom SQL queries are needed (stored in the queries folder). If multiple levels of aggregation is needed, the additional column of aggregation is specified under \"filter_col\" . The script will aggregate over each value specified under \"filter\" , and the values under the column specified in the \"col\" field. The script uses one generic function to for each of the outputs. To add an additional output, or to remove/modify the output, the config file would need to be modified. Measures are calculated in two ways. If \"groupby\":true , then the script will group the database output by the col specificed using the aggregation specified in \"how\" . The resulting value is the aggregated results of the column specified in the \"value\" field. If \"groupby\":false , then the script will aggregate all values specified in \"col\" according to the aggregation method specified in \"how\" . It will name the output of the aggregation according to the \"value\" . The disaggregation column will be named according to the name set under \"alias\" , and the disaggregation values are the names of the columns specified under \"col\" . Nearly all measures are aggregated using sum . Fare measures (average fare by gy origin and average fares by period) are the only measures that aggregate using the mean. Most measures specified in the script/json are some form of a demand indicator, therefore the most best aggregation method is to sum the trips/crossings/boardings by disaggregation column. The following image lists the tables that each of the outputs pulls data from. It also lists all the functions, and what outputs are created by each function. Interface \u00b6 Once the tool has been imported to EMME, the tool needs 3 inputs to run. Scenario 1 Directory and Scenario 2 Directory refers to the directories that house the rtm.db and trip_summaries.db for each of the two scenarios. Output directory refers to the directory where the outputs will be saved to. The tool will create a folder named ScenarioComparison where all the output csvs will be saved to. Output format \u00b6 The outputs are saved in a single csv, where the name of the file is in the following format. summary_comparison-1_{Scenario 1 ID}_{Scenario 1 Name}-2_{Scenario 2 ID}_{Scenario 2 Name} The tables are in the following format. metric value_1_{Scenario 1 ID}_{Scenario 1 Name} value_2_{Scenario 2 ID}_{Scenario 2 Name} abs_change rel_change fare_matrix_gy-fare-gy_i-7 4 6 2 0.5 The name of each metric follows the following format. {Database table}-{Measure}-{Disaggregation Column}-{Disaggregation Value} For example, for fare_matrix_gy-fare-gy_i-7 * Data is pulled from the fare_matrix_gy table * The values in the table are the average fare * The data is disaggregated by gy origin zone * This specific metric shows the average fare for origins from gy zone 7 for both Scenario 1 and 2","title":"Scenario Comparison Tool"},{"location":"scenario_comparison/#scenario-comparison-tool","text":"","title":"Scenario Comparison Tool"},{"location":"scenario_comparison/#how-it-works","text":"The Scenario Comparison EMME toolbox is an utility that allows planners/modellers to quickly compare metrics across two different EMME scenarios. This toolbox requires two RTM runs to be completed using the latest version of RTM. For each run of the RTM, the model will create two databases for each scenario the model was run for: rtm.db , and trip_summaries.db . The two databases house data relating to transportation metrics. The tool will connect to the pair of databases for each scenario, pull relevant data, join the data for each scenario, and then output the data for each metric/aggregation/disaggregation as a csv. The code reads from the configuration file ScenarioComparison.json . The config file will give a series of tables from the two databases for the script to pull from. For each table, the disagg field will give a list of columns to disaggregate by for the measure, what type of aggregation/group by action to perform (ex average or summing), and what columns to sum/average for (ex volumes or trips). In certain cases, the config file will also tell the script if it needs to rename certain columns in the database output, if multiple levels of aggregation are needed, or if custom SQL queries are needed (stored in the queries folder). If multiple levels of aggregation is needed, the additional column of aggregation is specified under \"filter_col\" . The script will aggregate over each value specified under \"filter\" , and the values under the column specified in the \"col\" field. The script uses one generic function to for each of the outputs. To add an additional output, or to remove/modify the output, the config file would need to be modified. Measures are calculated in two ways. If \"groupby\":true , then the script will group the database output by the col specificed using the aggregation specified in \"how\" . The resulting value is the aggregated results of the column specified in the \"value\" field. If \"groupby\":false , then the script will aggregate all values specified in \"col\" according to the aggregation method specified in \"how\" . It will name the output of the aggregation according to the \"value\" . The disaggregation column will be named according to the name set under \"alias\" , and the disaggregation values are the names of the columns specified under \"col\" . Nearly all measures are aggregated using sum . Fare measures (average fare by gy origin and average fares by period) are the only measures that aggregate using the mean. Most measures specified in the script/json are some form of a demand indicator, therefore the most best aggregation method is to sum the trips/crossings/boardings by disaggregation column. The following image lists the tables that each of the outputs pulls data from. It also lists all the functions, and what outputs are created by each function.","title":"How it works"},{"location":"scenario_comparison/#interface","text":"Once the tool has been imported to EMME, the tool needs 3 inputs to run. Scenario 1 Directory and Scenario 2 Directory refers to the directories that house the rtm.db and trip_summaries.db for each of the two scenarios. Output directory refers to the directory where the outputs will be saved to. The tool will create a folder named ScenarioComparison where all the output csvs will be saved to.","title":"Interface"},{"location":"scenario_comparison/#output-format","text":"The outputs are saved in a single csv, where the name of the file is in the following format. summary_comparison-1_{Scenario 1 ID}_{Scenario 1 Name}-2_{Scenario 2 ID}_{Scenario 2 Name} The tables are in the following format. metric value_1_{Scenario 1 ID}_{Scenario 1 Name} value_2_{Scenario 2 ID}_{Scenario 2 Name} abs_change rel_change fare_matrix_gy-fare-gy_i-7 4 6 2 0.5 The name of each metric follows the following format. {Database table}-{Measure}-{Disaggregation Column}-{Disaggregation Value} For example, for fare_matrix_gy-fare-gy_i-7 * Data is pulled from the fare_matrix_gy table * The values in the table are the average fare * The data is disaggregated by gy origin zone * This specific metric shows the average fare for origins from gy zone 7 for both Scenario 1 and 2","title":"Output format"},{"location":"workflow/","text":"Model Workflow \u00b6 In this section, we will show you the best practices when working with the RTM from setting up your environment to extracting model results. Requirements \u00b6 RTM Users \u00b6 We encourage all users to be familiar with OpenPaths EMME . If you are new to EMME, we recommend you to review Tutorials for OpenPaths EMME . * Licensed installation of OpenPaths EMME 25.00.00 for RTM 3.7 Note A valid OpenPaths EMME license is required to run the RTM. RTM Developers \u00b6 If you are a developer for RTM. You should be familiar with RTM's development stack: python , SQL and git * Installation of git * Installation of git lfs * Installation of SQLite Studio * Installation of Visual Studio Code (optional) Previous RTM version requirements \u00b6 The RTM is developed and tested against specific versions of the EMME software. If you are using previous RTM versions, here are the EMME Desktop versions you would need: for RTM-3.3, install EMME-v4.4.2 for RTM-3.4, install EMME-v4.4.4.2 for RTM-3.5, install EMME-v4.5.0.2 for RTM-3.6, install EMME-v4.5.0.2 for RTM-3.7, install OpenPaths EMME 25.00.00 System requirements \u00b6 The minimimum system requirement for running the RTM is: 2-core processor 3.0 GHz 16 GB memory 15 GB Storage (for 1 scenario runs, 12 GB is required for each additional scenario) The recommended system requirement is: 8-core processor with 4.0 GHz 32 GB memory or more 50 GB Storage (for 3 scenario runs with extra space for advanced analysis) Obtaining Model files \u00b6 Use Latest RTM version \u00b6 RTM users can obtain a copy of the latest RTM model release through their TransLink contact. We periodically provide new releases to everyone on our model user list. Git Clone (for Developers only) \u00b6 Git is only required if you are a developer making contribution to the source code of RTM. To get the latest version of the rtm, clone it from the RTM github repository. The following bash commands will clone the repository from the master branch, and fetch the latest commit with large file storage support. Then, it will checkout the current branch into a new branch named master_working_01 . We recommend that you rename the new branch based on the project you are working on: git clone https://github.com/TransLinkForecasting/rtm.git --branch master --single-branch cd rtm/ git fetch --all git lfs install git lfs pull git checkout -b 'master_working_01' To clone a specific official release, replace master with a version tag such as RTM3.3 . If this is the first time you use git, you might need to set your name and email for git commits, see Git Setup . Update File References \u00b6 EMME toolbox stores absolute paths and they need to be updated when the model has been copied or moved to a new directory. Please also run the following after you clone the model: cd 'RTM/Scripts/' bash 'relocate_tools.sh' Alternatively, you can run the batch file on Windows: cd 'RTM/Scripts/' start 'relocate_tools.bat' Folder Structure \u00b6 The RTM is delivered as a collection of input files in the BaseNetworks/ folder with a collection of python scripts in Scripts/ folder. The EMME project file RTM.emp references all required folders and is loaded with a Minimal Base Databank stored in Template/ folder. Everything stored within the RTM folder is considered project-level, and run specific inputs are stored in the BaseNetworks/ folder. Before running the model, familiarize yourself with the model folder structure. RTM/ \u251c\u2500\u2500 BaseNetworks/ \u2502 \u2514\u2500\u2500 Inputs/ \u2502 \u251c\u2500\u2500 Documentation/ \u251c\u2500\u2500 Logbook/ \u251c\u2500\u2500 Media/ \u251c\u2500\u2500 Scripts/ \u251c\u2500\u2500 Template/ \u251c\u2500\u2500 Views/ \u251c\u2500\u2500 Worksheets/ \u2502 \u2514\u2500\u2500 RTM.emp Warning Do not delete the Minimal Base Databank in EMME or the Template/ folder. This is an empty placeholder databank that is packaged with the RTM to enable the initialization of databank through EMME Modeler or EMME Notebook. Base Networks Folder \u00b6 The base network folder contains files required to build a databank from scratch. Each set of files in the main BaseNetworks/ supply input data for each scenario year (for example: 2017, 2035, 2050), the set of files can be mainly classified as the following: Network batchin files: base_network_*.txt , link_shape_*.txt , etc Calibration factors (K-factors): dist_factors_gy.csv.gz External demand, bike scores: externals_bikescore_*.csv.gz Starter Skims: start_skims.csv.gz Fare zones: fare_zones_travelled.csv.gz Definition of ensembles: taz1700_ensembles.csv Definition of modes: modes.in Definition of transit vehicles: tvehicles.in Demographics and geographics of Metro Vancouver: taz1700_demographics_*.csv , taz1700_geographics_*.csv Geographic dummies: taz1700_dummies.csv Park and ride assumptions: taz1700_pnr.csv Time slicing: time_slicing.csv , time_slicing_gb.csv Transit bias calibrations: transit_adj.csv Truck model batch files: TruckBatchFiles/* RTM/ \u251c\u2500\u2500 BaseNetworks/ \u2502 \u251c\u2500\u2500 base_network_*.txt \u2502 \u251c\u2500\u2500 link_shape_*.txt \u2502 \u251c\u2500\u2500 turns_*.txt \u2502 \u251c\u2500\u2500 transit_lines_*.txt \u2502 \u251c\u2500\u2500 extra_nodes_*.txt \u2502 \u251c\u2500\u2500 extra_links_*.txt \u2502 \u251c\u2500\u2500 extra_turns_*.txt \u2502 \u251c\u2500\u2500 extra_transit_lines_*.txt \u2502 \u251c\u2500\u2500 dist_factors_gy.csv.gz \u2502 \u251c\u2500\u2500 externals_bikescore_*.csv.gz \u2502 \u251c\u2500\u2500 fare_zones_travelled.csv.gz \u2502 \u251c\u2500\u2500 start_skims.csv.gz \u2502 \u251c\u2500\u2500 taz1700_ensembles.csv \u2502 \u251c\u2500\u2500 modes.in \u2502 \u251c\u2500\u2500 tvehicles.in \u2502 \u2502 \u2502 \u2514\u2500\u2500 Inputs/ \u2502 \u251c\u2500\u2500 taz1700_demographics_*.csv \u2502 \u251c\u2500\u2500 taz1700_geographics_*.csv \u2502 \u251c\u2500\u2500 taz1700_dummies.csv \u2502 \u251c\u2500\u2500 taz1700_pnr.csv \u2502 \u251c\u2500\u2500 time_slicing.csv \u2502 \u251c\u2500\u2500 time_slicing_gb.csv \u2502 \u251c\u2500\u2500 transit_adj.csv \u2502 \u2514\u2500\u2500 TruckBatchFiles/ \u2502 \u251c\u2500\u2500 *AsiaPacificv1.txt \u2502 \u251c\u2500\u2500 *CrossBorderv1.txt \u2502 \u251c\u2500\u2500 IRBatchIn.txt \u2502 \u251c\u2500\u2500 PMVActivity.txt \u2502 \u2514\u2500\u2500 RGBatchIn.txt \u2514\u2500\u2500 ... Scripts Folder \u00b6 This folder contains python scripts used to run the model: Toolbox relocate tool: relocate_tools.bat , relocate_tools.sh , toolbox_modify.py Model Scripts toolbox: Phase3Scripts/ Model utility toolbox: util/ Model tools, scripts and notebooks of model initialization and model runs: Notebooks/ Analytics toolbox: Phase3Analytics/ Tools toolbox: Phase3Tools/ RTM/ \u251c\u2500\u2500 Scripts/ \u2502 \u251c\u2500\u2500 relocate_tools.bat \u2502 \u251c\u2500\u2500 relocate_tools.sh \u2502 \u251c\u2500\u2500 toolbox_modify.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 Notebooks/ \u2502 \u251c\u2500\u2500 Phase3Scripts/ \u2502 \u251c\u2500\u2500 util/ \u2502 \u251c\u2500\u2500 Phase3Analytics/ \u2502 \u2514\u2500\u2500 Phase3Tools/ \u2514\u2500\u2500 ... Note The Scripts control the model behavior of the RTM. You are free to go through the script to familiarize with the implementation of the model. For more information regarding the modeling structure of the RTM, modeling decisions, data sources, estimation of sub models, please contact us . Other Folders \u00b6 Media folder Contains shapefiles, ArcGIS maps. Output data are typically stored here. Documentation folder contains printable user manuals. Logbook folder contains EMME logbook information generated by EMME Modeler tools. Template folder provides an empty databank that allow Modeler to initialize new RTM databanks. View folder contains standard views provided in the EMME environment. Worksheets folder contains standardized worksheets in the EMME environment. RTM/ \u251c\u2500\u2500 Media/ \u251c\u2500\u2500 Documentation/ \u251c\u2500\u2500 Logbook/ \u251c\u2500\u2500 Template/ \u251c\u2500\u2500 Views/ \u251c\u2500\u2500 Worksheets/ \u2514\u2500\u2500 ... Initialize Databank \u00b6 You can initialize a single databank using the EMME Modeler or initialize multiple databanks using EMME Notebook, depending on your need. To start the EMME, open the file. This will launch EMME, open the Minimal Base Databank's Placeholder Scenario when prompted. With EMME Modeler \u00b6 To open Modeler, click on the EMME Modeler icon . Once the Modeler opens, find Translink RTM Phase 3 Model toolbox and open the Initialize Emmebank tool: Enter a folder name and a name for the databank. Try to be descriptive. For example, if this is a databank for editing or debugging network only, name it network_edit_something , if this is a final copy of a new business as usual run for year 2050, name it 00_BAU_2050 . Once you enter the names, click > Run . Once the run is completed, the Tool complete message will appear. With EMME Notebook \u00b6 If you are comfortable with python and Jupyter Notebook, we strongly recommend that you use EMME Notebook to initialize databanks. This improves the reproducibility of your model run. This is generally not needed for network scenario or testing, but for final copy of a run, it is highly recommend. The init_many.ipynb notebook contains a template for initialize many databanks. To open EMME Notebook, click on the EMME Notebook icon . The EMME Notebook should open in your browser, if it didn't open automatically, look for http://localhost:xxxx/ in the EMME - Notebook command prompt window. The default is usually http://localhost:8888/ . Once opened, you should see a list of folders in the Scripts/ folder. Go to Tools/ folder and make a duplicate of init_many.ipynb , name it to something more descriptive for your use, like init_many_00_BAU_2050.ipynb , then open it. Modify params as needed, run the entire script to initialize the databank. Build Scenario \u00b6 There are a wide range of changes you can make to the EMME base model for your study. We won't be able to cover them all in detail here, but here are some examples of model changes: demographics new roads or bridges new transit lines new transit stops transit fare tolls mobility pricing park and ride pricing etc... Network editing \u00b6 The EMME Network Editor provides users the ability to create and modify nodes, links, turns, transit lines, and transit segments. We will use a BRT line example to illustrate how to add modes and vehicles for the RTM, and edit networks in EMME. Custom Inputs \u00b6 There are a number of custom inputs that the RTM uses at model initialization and run time to modify model behavior: At initiation Transit vehicles and modes At runtime Required files with model data demographic and geographic Optional files to manipulate model data Scalars: custom_scalars.csv Network: custom_network.txt Transit Lines: custom_tline.txt Transit Segment: custom_tseg.txt The optional custom_<datatype> series files can be placed in the RTM/<your run directory>/inputs folder of the RTM instance. These files are imported after the main model setup ( create scenarios and data import ) and can be used to change any model setting held in a scalar matrix or execute an arbitrary set of network link, transit line, or transit segment calculations. Files with a .csv extension indicate comma delimited data while files with a .txt extension indicate tab delimited data. Warning Do not leave a trailing new line or carriage return at the end of the files custom_<datatype>.txt files. In addition, there can be only one file of each name in the inputs folder. Custom Scalars \u00b6 This file allow the user to change data held in scalar matrices in the scripting without changing the scripting. This in turn allows the user to run multiple runs with the same scripts and different scalar matrix data. The custom_scalars.csv file has headers and requires the following information in comma delimited format. Matrix ID Matrix Name Matrix Description Value The EMME Matrix ID number The EMME Matrix Name The EMME Matrix Description The value to change Below is an example of changing the auto operating cost using the custom scalars file. Custom Network \u00b6 This file allows the user to execute an arbitrary set of network link calculations prior to running the model. The custom_network.txt file should not have headers and requires the following information in tab delimited format Period Result Expression Selection Aggregation AM, MD, PM, or ALL Where the calculation should be stored (for instance @capacity) The network calculation to be executed Which link(s) to execute the calculation on (optional - can be left blank to update all links) Results aggregation (optional and generally left blank) Below is an example of removing one lane and reducing capacity on links 111902-111904 and 111904-111902, and setting the signal delay to 0 for all links that are not VDF 14 for all three assignment periods. Note that there is no carriage return on the last line. Custom Transit Lines \u00b6 This file allows the user to execute an arbitrary set of network transit line calculations prior to running the model. The custom_tline.txt file should not have headers and requires the following information in tab delimited format Period Result Expression Selection Aggregation AM, MD, PM, or ALL Where the calculation should be stored (for instance hdw) The network calculation to be executed Which line(s) to execute the calculation on (optional - can be left blank to update all lines) Results aggregation (optional and generally left blank) Below is an example of reducing the headway on all lines except those in the Fraser Valley during the AM and PM peak hours, and setting the signal delay to 0 for all links that are not VDF 14 for all three assignment periods. Note that there is no carriage return on the last line. Custom Transit Segments \u00b6 This file allows the user to execute an arbitrary set of network transit segment calculations prior to running the model. The custom_tseg.txt file should not have headers and requires the following information in tab delimited format Period Result Expression Selection Aggregation AM, MD, PM, or ALL Where the calculation should be stored (for instance ttf) The network calculation to be executed Which line(s) to execute the calculation on (optional - can be left blank to update all lines) Results aggregation (optional and generally left blank) Below is an example of changing the transit time function (ttf) on links 111902-111904 and 111904-111902 to ttf2. Note that there is no carriage return on the last line. Example: 2050 BRT Line \u00b6 Similar to most model application exercises, in our example, there are two main components of changes we are making to the 2050 base model: network editing and model custom inputs. Network editing - adding a new transit line Custom inputs Add congestion pricing to the network with custom_network.txt input file Use a custom demographic file for corridor intensification Step 1: add new modes \u00b6 Before we create a new transit line, we need to make sure the mode and the vehicle of is available, add the new mode to modes.in file in the base network folder: Then check the tvehicles.in file in the base network folder: After new modes or vehicles are added, it won't be available in the already initialized databank and scenario. You should re-initialize a working copy of the databank. Note Choosing the BRT and LRT vehicle in tvehicles.in with modes.in implements the unobserved mode discussed in the October 2019 User Group Meeting. Presentation available here . Step 2: create new scenarios \u00b6 Duplicate the base scenario input files, and rename them with a new scenario number. For example, if you are making changes on top of scenario \"5000\", name it \"5001\", \"5002\". Load new scenario manually \u00b6 To load new scenario into your current data bank, open Modeler and Import Network Tool: Enter the scenario number and a title for it, click run to load the scenario. It now should be available in the list of scenario and you can set it as your active scenario to perform your network changes. Optional: Always load new scenario on databank initialization \u00b6 If you want the newly created scenario to be imported every time you initialize a new emmebank, modify InitEmmebank.py . Once you set up the scenarios, add these new scenario numbers into the InitEmmebank.py script. This will allow the initialize emmebank modeler tool to load the new scenarios. Now, you can initialize a new databank and your new scenario will be loaded by default. You can give it a descriptive and abbreviated name such as 01_Lg_BRT_CP_LU_2050 . Step 3: perform network editing \u00b6 Open EMME network editor , add notes, links, and transit line as needed. Make sure you save a copy of the build file before you save and exit the scenario. Build files can be created from any changes you made while in the Network Editor. The build file is a great way to save and replicate model network changes. If you have a build file, you can stage it and run the changes. Copy your build file to Network_builds/ folder. Open \"Network History and Builds\" prompt from EMME Network Editor . Click the folder icon to \"Add network builds\", then click \"Stage\". Preview the build, then run the build: Step 4: set up custom inputs \u00b6 For the BRT example, we will use custom_network.txt and a simple 8-zone shapefile to add congestion pricing attribute to the network in batch. We will also load a custom demographic file with 20% population intensification along the study corridor. Custom inputs often require link tagging defined by the intersection of input polygons and network links in EMME. This can be done in Modeler or Notebook using the Geographic Tagging Tool: After link tagging, the custom input file such as custom_network.txt needs to be placed in the Input/ folder within the emme databank folder, such as the folder db_01_Lg_BRT_CP_LU_2050 or 01_Lg_BRT_CP_LU_2050 . (Do not place the file in the BaseNetworks folder!) Read more about Custom Networks . Note Read more on link tagging tool in: RTM\\Documentation\\ToolDoc_GeographicTagging.pdf . The custom demographic file such as taz1700_demographics_2050-20.csv needs to be selected as the demographic input when running the model. Export Scenario \u00b6 While in most cases your current databank for the alternative scenario is ready to run, it is important to export your new scenario network and organize all input files into a sub folder within the Scenario_Inputs folder. This allows you to share your new scenario with other users without needing to copy the entire databank with results. This also allow you to quickly reinitialize the scenario to run on a newer version of the RTM. Export Network \u00b6 To export your scenario's network, open EMME Modeler: Then select your new scenario to export: Organize Scenario Inputs \u00b6 We highly recommend that you store every input required to rebuild your new scenario and replicate your model run within a sub folder of the Scenario_Inputs folder. For our example, this would be the folder structure for all the changes we have made in our BRT example: RTM/ \u251c\u2500\u2500 Scenario_Inputs/ \u2502 \u2514\u2500\u2500 01_2050-20-hw8-cp_Lg_BRT-g/ \u2502 \u251c\u2500\u2500 custom_network.txt \u2502 \u251c\u2500\u2500 modes.in \u2502 \u251c\u2500\u2500 tvehicles.in \u2502 \u2502 \u2502 \u251c\u2500\u2500 BaseNetworks/ \u2502 \u2502 \u251c\u2500\u2500 Inputs/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 taz1700_demographics_2050-20.csv \u2502 \u2502 \u251c\u2500\u2500 base_network_5001.txt \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 Scripts/Phase3Scripts/ \u2502 \u2502 \u251c\u2500\u2500 Tools/ \u2502 \u2502 \u2502 \u2502\u2500\u2500 init_many_example_lgrt.ipynb \u2502 \u2502 \u2502 \u2514\u2500\u2500 run_many_example_lgrt.ipynb \u2502 \u2502 \u2514\u2500\u2500 Phase3Scripts/ \u2502 \u2502 \u2514\u2500\u2500 InitEmmebank.py \u2502 \u2514\u2500\u2500 Media/ \u2502 \u2514\u2500\u2500 RTM_8ZonePolygon.shp \u2514\u2500\u2500 ... Run Model \u00b6 You can run the model using a single databank using the EMME Modeler or run multiple databanks using the EMME Notebook, depending on your need. EMME Modeler \u00b6 Once you have set up all the input files in the databank, such as customized demographics, custom input files, etc, you are ready to do a full model run using the EMME Modeler. To open Modeler, click on the EMME Modeler icon . Find Translink RTM Phase 3 Model toolbox and open the Run RTM3 tool: Change the Full Model Run settings as needed. Make sure to enter the correct model horizon year, scenario, demographic and geographic inputs. Click Run . Each model run can take 2 to 4 hours depending on the custom settings and scenario. Warning Do not interrupt the model run. The RTM model is not designed to be run in separate stages or run multiple times. If you stopped a model run before it is done, you should start over and run the model again. EMME Notebook \u00b6 The run_many.ipynb notebook contains a template for running many model runs. It handles multiple model runs and runs with complex set up. It is ideal for our BRT example where we needed to perform link tagging and use custom network input file for congestion pricing. Note sometimes additional coding is required to call toolboxes that are relevant to the particular model run. To open EMME Notebook, click on the EMME Notebook icon . The EMME Notebook should open in your browser, if it didn't open automatically, look for http://localhost:xxxx/ in the EMME - Notebook command prompt window. The default is usually http://localhost:8888/ . Once opened, you should see a list of folders in the Scripts/ folder. Go to Tools/ folder and make a duplicate of run_many.ipynb , name it to something more descriptive for your use, like run_many_00_BAU_2050.ipynb or run_many_example_lgrt.ipynb , then open it. Modify params as needed. Make sure you carefully review the script associated with the model run. You may need to add new code to this section depending on the requirements of your model run. Run the entire script to perform the full model run with custom settings. Commit Changes (for Developers only) \u00b6 If this is your first time using git after an installation, see Git Setup . Git is only required if you are a developer making contribution to the source code of RTM. While working with the RTM on git, here are some ground rules: Main project branches ( master , example_brt_proj ) are always protected. Push a temp branch then do pull request to merge your changes Only stage and commit model inputs and settings, never databanks Any scenario-specific changes for scripts should be placed into Scenario_Inputs for commit Organize a Scenario_Inputs folder when you finish a scenario Use active verb for commit messages, like \"add scenario inputs for brt examples.\" Warning Do not commit the databank associated with the model run. We recommend you to place the databank folders in your version of the .gitignore . When staging and committing model changes, think about what other users or developers may need to run your scenario from scratch. Commit inputs and set up, not the results. Push Changes to GitHub \u00b6 If you are a collaborator, you may push your changes to the rtm repository on GitHub. You will not be able to push directly to most of the branches already on the rtm repository as they are protected, so always push your working branch and merge to any main branch with a pull request. An admin will review your changes when you submit a pull request. git push origin master_your-project Learn more about git and read more about GitHub pull request .","title":"Model Workflow"},{"location":"workflow/#model-workflow","text":"In this section, we will show you the best practices when working with the RTM from setting up your environment to extracting model results.","title":"Model Workflow"},{"location":"workflow/#requirements","text":"","title":"Requirements"},{"location":"workflow/#rtm-users","text":"We encourage all users to be familiar with OpenPaths EMME . If you are new to EMME, we recommend you to review Tutorials for OpenPaths EMME . * Licensed installation of OpenPaths EMME 25.00.00 for RTM 3.7 Note A valid OpenPaths EMME license is required to run the RTM.","title":"RTM Users"},{"location":"workflow/#rtm-developers","text":"If you are a developer for RTM. You should be familiar with RTM's development stack: python , SQL and git * Installation of git * Installation of git lfs * Installation of SQLite Studio * Installation of Visual Studio Code (optional)","title":"RTM Developers"},{"location":"workflow/#previous-rtm-version-requirements","text":"The RTM is developed and tested against specific versions of the EMME software. If you are using previous RTM versions, here are the EMME Desktop versions you would need: for RTM-3.3, install EMME-v4.4.2 for RTM-3.4, install EMME-v4.4.4.2 for RTM-3.5, install EMME-v4.5.0.2 for RTM-3.6, install EMME-v4.5.0.2 for RTM-3.7, install OpenPaths EMME 25.00.00","title":"Previous RTM version requirements"},{"location":"workflow/#system-requirements","text":"The minimimum system requirement for running the RTM is: 2-core processor 3.0 GHz 16 GB memory 15 GB Storage (for 1 scenario runs, 12 GB is required for each additional scenario) The recommended system requirement is: 8-core processor with 4.0 GHz 32 GB memory or more 50 GB Storage (for 3 scenario runs with extra space for advanced analysis)","title":"System requirements"},{"location":"workflow/#obtaining-model-files","text":"","title":"Obtaining Model files"},{"location":"workflow/#use-latest-rtm-version","text":"RTM users can obtain a copy of the latest RTM model release through their TransLink contact. We periodically provide new releases to everyone on our model user list.","title":"Use Latest RTM version"},{"location":"workflow/#git-clone-for-developers-only","text":"Git is only required if you are a developer making contribution to the source code of RTM. To get the latest version of the rtm, clone it from the RTM github repository. The following bash commands will clone the repository from the master branch, and fetch the latest commit with large file storage support. Then, it will checkout the current branch into a new branch named master_working_01 . We recommend that you rename the new branch based on the project you are working on: git clone https://github.com/TransLinkForecasting/rtm.git --branch master --single-branch cd rtm/ git fetch --all git lfs install git lfs pull git checkout -b 'master_working_01' To clone a specific official release, replace master with a version tag such as RTM3.3 . If this is the first time you use git, you might need to set your name and email for git commits, see Git Setup .","title":"Git Clone (for Developers only)"},{"location":"workflow/#update-file-references","text":"EMME toolbox stores absolute paths and they need to be updated when the model has been copied or moved to a new directory. Please also run the following after you clone the model: cd 'RTM/Scripts/' bash 'relocate_tools.sh' Alternatively, you can run the batch file on Windows: cd 'RTM/Scripts/' start 'relocate_tools.bat'","title":"Update File References"},{"location":"workflow/#folder-structure","text":"The RTM is delivered as a collection of input files in the BaseNetworks/ folder with a collection of python scripts in Scripts/ folder. The EMME project file RTM.emp references all required folders and is loaded with a Minimal Base Databank stored in Template/ folder. Everything stored within the RTM folder is considered project-level, and run specific inputs are stored in the BaseNetworks/ folder. Before running the model, familiarize yourself with the model folder structure. RTM/ \u251c\u2500\u2500 BaseNetworks/ \u2502 \u2514\u2500\u2500 Inputs/ \u2502 \u251c\u2500\u2500 Documentation/ \u251c\u2500\u2500 Logbook/ \u251c\u2500\u2500 Media/ \u251c\u2500\u2500 Scripts/ \u251c\u2500\u2500 Template/ \u251c\u2500\u2500 Views/ \u251c\u2500\u2500 Worksheets/ \u2502 \u2514\u2500\u2500 RTM.emp Warning Do not delete the Minimal Base Databank in EMME or the Template/ folder. This is an empty placeholder databank that is packaged with the RTM to enable the initialization of databank through EMME Modeler or EMME Notebook.","title":"Folder Structure"},{"location":"workflow/#base-networks-folder","text":"The base network folder contains files required to build a databank from scratch. Each set of files in the main BaseNetworks/ supply input data for each scenario year (for example: 2017, 2035, 2050), the set of files can be mainly classified as the following: Network batchin files: base_network_*.txt , link_shape_*.txt , etc Calibration factors (K-factors): dist_factors_gy.csv.gz External demand, bike scores: externals_bikescore_*.csv.gz Starter Skims: start_skims.csv.gz Fare zones: fare_zones_travelled.csv.gz Definition of ensembles: taz1700_ensembles.csv Definition of modes: modes.in Definition of transit vehicles: tvehicles.in Demographics and geographics of Metro Vancouver: taz1700_demographics_*.csv , taz1700_geographics_*.csv Geographic dummies: taz1700_dummies.csv Park and ride assumptions: taz1700_pnr.csv Time slicing: time_slicing.csv , time_slicing_gb.csv Transit bias calibrations: transit_adj.csv Truck model batch files: TruckBatchFiles/* RTM/ \u251c\u2500\u2500 BaseNetworks/ \u2502 \u251c\u2500\u2500 base_network_*.txt \u2502 \u251c\u2500\u2500 link_shape_*.txt \u2502 \u251c\u2500\u2500 turns_*.txt \u2502 \u251c\u2500\u2500 transit_lines_*.txt \u2502 \u251c\u2500\u2500 extra_nodes_*.txt \u2502 \u251c\u2500\u2500 extra_links_*.txt \u2502 \u251c\u2500\u2500 extra_turns_*.txt \u2502 \u251c\u2500\u2500 extra_transit_lines_*.txt \u2502 \u251c\u2500\u2500 dist_factors_gy.csv.gz \u2502 \u251c\u2500\u2500 externals_bikescore_*.csv.gz \u2502 \u251c\u2500\u2500 fare_zones_travelled.csv.gz \u2502 \u251c\u2500\u2500 start_skims.csv.gz \u2502 \u251c\u2500\u2500 taz1700_ensembles.csv \u2502 \u251c\u2500\u2500 modes.in \u2502 \u251c\u2500\u2500 tvehicles.in \u2502 \u2502 \u2502 \u2514\u2500\u2500 Inputs/ \u2502 \u251c\u2500\u2500 taz1700_demographics_*.csv \u2502 \u251c\u2500\u2500 taz1700_geographics_*.csv \u2502 \u251c\u2500\u2500 taz1700_dummies.csv \u2502 \u251c\u2500\u2500 taz1700_pnr.csv \u2502 \u251c\u2500\u2500 time_slicing.csv \u2502 \u251c\u2500\u2500 time_slicing_gb.csv \u2502 \u251c\u2500\u2500 transit_adj.csv \u2502 \u2514\u2500\u2500 TruckBatchFiles/ \u2502 \u251c\u2500\u2500 *AsiaPacificv1.txt \u2502 \u251c\u2500\u2500 *CrossBorderv1.txt \u2502 \u251c\u2500\u2500 IRBatchIn.txt \u2502 \u251c\u2500\u2500 PMVActivity.txt \u2502 \u2514\u2500\u2500 RGBatchIn.txt \u2514\u2500\u2500 ...","title":"Base Networks Folder"},{"location":"workflow/#scripts-folder","text":"This folder contains python scripts used to run the model: Toolbox relocate tool: relocate_tools.bat , relocate_tools.sh , toolbox_modify.py Model Scripts toolbox: Phase3Scripts/ Model utility toolbox: util/ Model tools, scripts and notebooks of model initialization and model runs: Notebooks/ Analytics toolbox: Phase3Analytics/ Tools toolbox: Phase3Tools/ RTM/ \u251c\u2500\u2500 Scripts/ \u2502 \u251c\u2500\u2500 relocate_tools.bat \u2502 \u251c\u2500\u2500 relocate_tools.sh \u2502 \u251c\u2500\u2500 toolbox_modify.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 Notebooks/ \u2502 \u251c\u2500\u2500 Phase3Scripts/ \u2502 \u251c\u2500\u2500 util/ \u2502 \u251c\u2500\u2500 Phase3Analytics/ \u2502 \u2514\u2500\u2500 Phase3Tools/ \u2514\u2500\u2500 ... Note The Scripts control the model behavior of the RTM. You are free to go through the script to familiarize with the implementation of the model. For more information regarding the modeling structure of the RTM, modeling decisions, data sources, estimation of sub models, please contact us .","title":"Scripts Folder"},{"location":"workflow/#other-folders","text":"Media folder Contains shapefiles, ArcGIS maps. Output data are typically stored here. Documentation folder contains printable user manuals. Logbook folder contains EMME logbook information generated by EMME Modeler tools. Template folder provides an empty databank that allow Modeler to initialize new RTM databanks. View folder contains standard views provided in the EMME environment. Worksheets folder contains standardized worksheets in the EMME environment. RTM/ \u251c\u2500\u2500 Media/ \u251c\u2500\u2500 Documentation/ \u251c\u2500\u2500 Logbook/ \u251c\u2500\u2500 Template/ \u251c\u2500\u2500 Views/ \u251c\u2500\u2500 Worksheets/ \u2514\u2500\u2500 ...","title":"Other Folders"},{"location":"workflow/#initialize-databank","text":"You can initialize a single databank using the EMME Modeler or initialize multiple databanks using EMME Notebook, depending on your need. To start the EMME, open the file. This will launch EMME, open the Minimal Base Databank's Placeholder Scenario when prompted.","title":"Initialize Databank"},{"location":"workflow/#with-emme-modeler","text":"To open Modeler, click on the EMME Modeler icon . Once the Modeler opens, find Translink RTM Phase 3 Model toolbox and open the Initialize Emmebank tool: Enter a folder name and a name for the databank. Try to be descriptive. For example, if this is a databank for editing or debugging network only, name it network_edit_something , if this is a final copy of a new business as usual run for year 2050, name it 00_BAU_2050 . Once you enter the names, click > Run . Once the run is completed, the Tool complete message will appear.","title":"With EMME Modeler"},{"location":"workflow/#with-emme-notebook","text":"If you are comfortable with python and Jupyter Notebook, we strongly recommend that you use EMME Notebook to initialize databanks. This improves the reproducibility of your model run. This is generally not needed for network scenario or testing, but for final copy of a run, it is highly recommend. The init_many.ipynb notebook contains a template for initialize many databanks. To open EMME Notebook, click on the EMME Notebook icon . The EMME Notebook should open in your browser, if it didn't open automatically, look for http://localhost:xxxx/ in the EMME - Notebook command prompt window. The default is usually http://localhost:8888/ . Once opened, you should see a list of folders in the Scripts/ folder. Go to Tools/ folder and make a duplicate of init_many.ipynb , name it to something more descriptive for your use, like init_many_00_BAU_2050.ipynb , then open it. Modify params as needed, run the entire script to initialize the databank.","title":"With EMME Notebook"},{"location":"workflow/#build-scenario","text":"There are a wide range of changes you can make to the EMME base model for your study. We won't be able to cover them all in detail here, but here are some examples of model changes: demographics new roads or bridges new transit lines new transit stops transit fare tolls mobility pricing park and ride pricing etc...","title":"Build Scenario"},{"location":"workflow/#network-editing","text":"The EMME Network Editor provides users the ability to create and modify nodes, links, turns, transit lines, and transit segments. We will use a BRT line example to illustrate how to add modes and vehicles for the RTM, and edit networks in EMME.","title":"Network editing"},{"location":"workflow/#custom-inputs","text":"There are a number of custom inputs that the RTM uses at model initialization and run time to modify model behavior: At initiation Transit vehicles and modes At runtime Required files with model data demographic and geographic Optional files to manipulate model data Scalars: custom_scalars.csv Network: custom_network.txt Transit Lines: custom_tline.txt Transit Segment: custom_tseg.txt The optional custom_<datatype> series files can be placed in the RTM/<your run directory>/inputs folder of the RTM instance. These files are imported after the main model setup ( create scenarios and data import ) and can be used to change any model setting held in a scalar matrix or execute an arbitrary set of network link, transit line, or transit segment calculations. Files with a .csv extension indicate comma delimited data while files with a .txt extension indicate tab delimited data. Warning Do not leave a trailing new line or carriage return at the end of the files custom_<datatype>.txt files. In addition, there can be only one file of each name in the inputs folder.","title":"Custom Inputs"},{"location":"workflow/#custom-scalars","text":"This file allow the user to change data held in scalar matrices in the scripting without changing the scripting. This in turn allows the user to run multiple runs with the same scripts and different scalar matrix data. The custom_scalars.csv file has headers and requires the following information in comma delimited format. Matrix ID Matrix Name Matrix Description Value The EMME Matrix ID number The EMME Matrix Name The EMME Matrix Description The value to change Below is an example of changing the auto operating cost using the custom scalars file.","title":"Custom Scalars"},{"location":"workflow/#custom-network","text":"This file allows the user to execute an arbitrary set of network link calculations prior to running the model. The custom_network.txt file should not have headers and requires the following information in tab delimited format Period Result Expression Selection Aggregation AM, MD, PM, or ALL Where the calculation should be stored (for instance @capacity) The network calculation to be executed Which link(s) to execute the calculation on (optional - can be left blank to update all links) Results aggregation (optional and generally left blank) Below is an example of removing one lane and reducing capacity on links 111902-111904 and 111904-111902, and setting the signal delay to 0 for all links that are not VDF 14 for all three assignment periods. Note that there is no carriage return on the last line.","title":"Custom Network"},{"location":"workflow/#custom-transit-lines","text":"This file allows the user to execute an arbitrary set of network transit line calculations prior to running the model. The custom_tline.txt file should not have headers and requires the following information in tab delimited format Period Result Expression Selection Aggregation AM, MD, PM, or ALL Where the calculation should be stored (for instance hdw) The network calculation to be executed Which line(s) to execute the calculation on (optional - can be left blank to update all lines) Results aggregation (optional and generally left blank) Below is an example of reducing the headway on all lines except those in the Fraser Valley during the AM and PM peak hours, and setting the signal delay to 0 for all links that are not VDF 14 for all three assignment periods. Note that there is no carriage return on the last line.","title":"Custom Transit Lines"},{"location":"workflow/#custom-transit-segments","text":"This file allows the user to execute an arbitrary set of network transit segment calculations prior to running the model. The custom_tseg.txt file should not have headers and requires the following information in tab delimited format Period Result Expression Selection Aggregation AM, MD, PM, or ALL Where the calculation should be stored (for instance ttf) The network calculation to be executed Which line(s) to execute the calculation on (optional - can be left blank to update all lines) Results aggregation (optional and generally left blank) Below is an example of changing the transit time function (ttf) on links 111902-111904 and 111904-111902 to ttf2. Note that there is no carriage return on the last line.","title":"Custom Transit Segments"},{"location":"workflow/#example-2050-brt-line","text":"Similar to most model application exercises, in our example, there are two main components of changes we are making to the 2050 base model: network editing and model custom inputs. Network editing - adding a new transit line Custom inputs Add congestion pricing to the network with custom_network.txt input file Use a custom demographic file for corridor intensification","title":"Example: 2050 BRT Line"},{"location":"workflow/#step-1-add-new-modes","text":"Before we create a new transit line, we need to make sure the mode and the vehicle of is available, add the new mode to modes.in file in the base network folder: Then check the tvehicles.in file in the base network folder: After new modes or vehicles are added, it won't be available in the already initialized databank and scenario. You should re-initialize a working copy of the databank. Note Choosing the BRT and LRT vehicle in tvehicles.in with modes.in implements the unobserved mode discussed in the October 2019 User Group Meeting. Presentation available here .","title":"Step 1: add new modes"},{"location":"workflow/#step-2-create-new-scenarios","text":"Duplicate the base scenario input files, and rename them with a new scenario number. For example, if you are making changes on top of scenario \"5000\", name it \"5001\", \"5002\".","title":"Step 2: create new scenarios"},{"location":"workflow/#load-new-scenario-manually","text":"To load new scenario into your current data bank, open Modeler and Import Network Tool: Enter the scenario number and a title for it, click run to load the scenario. It now should be available in the list of scenario and you can set it as your active scenario to perform your network changes.","title":"Load new scenario manually"},{"location":"workflow/#optional-always-load-new-scenario-on-databank-initialization","text":"If you want the newly created scenario to be imported every time you initialize a new emmebank, modify InitEmmebank.py . Once you set up the scenarios, add these new scenario numbers into the InitEmmebank.py script. This will allow the initialize emmebank modeler tool to load the new scenarios. Now, you can initialize a new databank and your new scenario will be loaded by default. You can give it a descriptive and abbreviated name such as 01_Lg_BRT_CP_LU_2050 .","title":"Optional: Always load new scenario on databank initialization"},{"location":"workflow/#step-3-perform-network-editing","text":"Open EMME network editor , add notes, links, and transit line as needed. Make sure you save a copy of the build file before you save and exit the scenario. Build files can be created from any changes you made while in the Network Editor. The build file is a great way to save and replicate model network changes. If you have a build file, you can stage it and run the changes. Copy your build file to Network_builds/ folder. Open \"Network History and Builds\" prompt from EMME Network Editor . Click the folder icon to \"Add network builds\", then click \"Stage\". Preview the build, then run the build:","title":"Step 3: perform network editing"},{"location":"workflow/#step-4-set-up-custom-inputs","text":"For the BRT example, we will use custom_network.txt and a simple 8-zone shapefile to add congestion pricing attribute to the network in batch. We will also load a custom demographic file with 20% population intensification along the study corridor. Custom inputs often require link tagging defined by the intersection of input polygons and network links in EMME. This can be done in Modeler or Notebook using the Geographic Tagging Tool: After link tagging, the custom input file such as custom_network.txt needs to be placed in the Input/ folder within the emme databank folder, such as the folder db_01_Lg_BRT_CP_LU_2050 or 01_Lg_BRT_CP_LU_2050 . (Do not place the file in the BaseNetworks folder!) Read more about Custom Networks . Note Read more on link tagging tool in: RTM\\Documentation\\ToolDoc_GeographicTagging.pdf . The custom demographic file such as taz1700_demographics_2050-20.csv needs to be selected as the demographic input when running the model.","title":"Step 4: set up custom inputs"},{"location":"workflow/#export-scenario","text":"While in most cases your current databank for the alternative scenario is ready to run, it is important to export your new scenario network and organize all input files into a sub folder within the Scenario_Inputs folder. This allows you to share your new scenario with other users without needing to copy the entire databank with results. This also allow you to quickly reinitialize the scenario to run on a newer version of the RTM.","title":"Export Scenario"},{"location":"workflow/#export-network","text":"To export your scenario's network, open EMME Modeler: Then select your new scenario to export:","title":"Export Network"},{"location":"workflow/#organize-scenario-inputs","text":"We highly recommend that you store every input required to rebuild your new scenario and replicate your model run within a sub folder of the Scenario_Inputs folder. For our example, this would be the folder structure for all the changes we have made in our BRT example: RTM/ \u251c\u2500\u2500 Scenario_Inputs/ \u2502 \u2514\u2500\u2500 01_2050-20-hw8-cp_Lg_BRT-g/ \u2502 \u251c\u2500\u2500 custom_network.txt \u2502 \u251c\u2500\u2500 modes.in \u2502 \u251c\u2500\u2500 tvehicles.in \u2502 \u2502 \u2502 \u251c\u2500\u2500 BaseNetworks/ \u2502 \u2502 \u251c\u2500\u2500 Inputs/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 taz1700_demographics_2050-20.csv \u2502 \u2502 \u251c\u2500\u2500 base_network_5001.txt \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 Scripts/Phase3Scripts/ \u2502 \u2502 \u251c\u2500\u2500 Tools/ \u2502 \u2502 \u2502 \u2502\u2500\u2500 init_many_example_lgrt.ipynb \u2502 \u2502 \u2502 \u2514\u2500\u2500 run_many_example_lgrt.ipynb \u2502 \u2502 \u2514\u2500\u2500 Phase3Scripts/ \u2502 \u2502 \u2514\u2500\u2500 InitEmmebank.py \u2502 \u2514\u2500\u2500 Media/ \u2502 \u2514\u2500\u2500 RTM_8ZonePolygon.shp \u2514\u2500\u2500 ...","title":"Organize Scenario Inputs"},{"location":"workflow/#run-model","text":"You can run the model using a single databank using the EMME Modeler or run multiple databanks using the EMME Notebook, depending on your need.","title":"Run Model"},{"location":"workflow/#emme-modeler","text":"Once you have set up all the input files in the databank, such as customized demographics, custom input files, etc, you are ready to do a full model run using the EMME Modeler. To open Modeler, click on the EMME Modeler icon . Find Translink RTM Phase 3 Model toolbox and open the Run RTM3 tool: Change the Full Model Run settings as needed. Make sure to enter the correct model horizon year, scenario, demographic and geographic inputs. Click Run . Each model run can take 2 to 4 hours depending on the custom settings and scenario. Warning Do not interrupt the model run. The RTM model is not designed to be run in separate stages or run multiple times. If you stopped a model run before it is done, you should start over and run the model again.","title":"EMME Modeler"},{"location":"workflow/#emme-notebook","text":"The run_many.ipynb notebook contains a template for running many model runs. It handles multiple model runs and runs with complex set up. It is ideal for our BRT example where we needed to perform link tagging and use custom network input file for congestion pricing. Note sometimes additional coding is required to call toolboxes that are relevant to the particular model run. To open EMME Notebook, click on the EMME Notebook icon . The EMME Notebook should open in your browser, if it didn't open automatically, look for http://localhost:xxxx/ in the EMME - Notebook command prompt window. The default is usually http://localhost:8888/ . Once opened, you should see a list of folders in the Scripts/ folder. Go to Tools/ folder and make a duplicate of run_many.ipynb , name it to something more descriptive for your use, like run_many_00_BAU_2050.ipynb or run_many_example_lgrt.ipynb , then open it. Modify params as needed. Make sure you carefully review the script associated with the model run. You may need to add new code to this section depending on the requirements of your model run. Run the entire script to perform the full model run with custom settings.","title":"EMME Notebook"},{"location":"workflow/#commit-changes-for-developers-only","text":"If this is your first time using git after an installation, see Git Setup . Git is only required if you are a developer making contribution to the source code of RTM. While working with the RTM on git, here are some ground rules: Main project branches ( master , example_brt_proj ) are always protected. Push a temp branch then do pull request to merge your changes Only stage and commit model inputs and settings, never databanks Any scenario-specific changes for scripts should be placed into Scenario_Inputs for commit Organize a Scenario_Inputs folder when you finish a scenario Use active verb for commit messages, like \"add scenario inputs for brt examples.\" Warning Do not commit the databank associated with the model run. We recommend you to place the databank folders in your version of the .gitignore . When staging and committing model changes, think about what other users or developers may need to run your scenario from scratch. Commit inputs and set up, not the results.","title":"Commit Changes (for Developers only)"},{"location":"workflow/#push-changes-to-github","text":"If you are a collaborator, you may push your changes to the rtm repository on GitHub. You will not be able to push directly to most of the branches already on the rtm repository as they are protected, so always push your working branch and merge to any main branch with a pull request. An admin will review your changes when you submit a pull request. git push origin master_your-project Learn more about git and read more about GitHub pull request .","title":"Push Changes to GitHub"},{"location":"about/contributing/","text":"Contributing \u00b6 If you would like to stay in touch, please join our mailing list. Name (required) Organization (required) Email (required) Phone Number Comments Thank you for joining our mailing list. If you have trouble with submitting the form, please visit Google Forms .","title":"Contributing"},{"location":"about/contributing/#contributing","text":"If you would like to stay in touch, please join our mailing list. Name (required) Organization (required) Email (required) Phone Number Comments Thank you for joining our mailing list. If you have trouble with submitting the form, please visit Google Forms .","title":"Contributing"},{"location":"about/license/","text":"License \u00b6","title":"License"},{"location":"about/license/#license","text":"","title":"License"},{"location":"about/release-notes/","text":"Release Notes \u00b6","title":"Release Notes"},{"location":"about/release-notes/#release-notes","text":"","title":"Release Notes"}]}